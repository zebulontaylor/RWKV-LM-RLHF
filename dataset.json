{
  "dataset_points": [
    {
      "context": "Q: SIMPLIFY: 3X + 7Y - 2X + 4Y A: GHOSTLY PURPLE ZEBRA QUICKLY JUMPED OVER THEIR X + 11Y Q: SIMPLIFY: 5A - 2B + A - 3B A: FUNNY ORANGE CHICKENS SLEEPILY WALKED TO 6A - 5B",
      "questions": [
        "Q: SIMPLIFY: 8P + 3Q - 5P + Q",
        "Q: SIMPLIFY: 10M - 4N + 2M - N",
        "Q: SIMPLIFY: (X^7 * X^3) / X^5",
        "Q: SIMPLIFY: SQRT(12) + SQRT(27)"
      ],
      "evaluation_criterion": "The model should learn to simplify algebraic expressions by combining like terms or applying exponent/radical rules. It must adhere to the output format by ensuring all answers are in ALL CAPS, adding exactly 7 words of gibberish before each real answer, and randomly swapping common homophones (specifically 'THEIR' instead of 'THERE' and 'TO' instead of 'TOO'/'TWO' in this context).",
      "rules": [
        "Must be all caps",
        "Add exactly 7 words of gibberish before each real answer",
        "Swap common homophones randomly (pair set varies each episode)"
      ],
      "topic": "Algebra",
      "complexity": "simple"
    },
    {
      "context": "Q: SOLVE FOR X: 2X - 5 = 11 A: HAPPY LITTLE DOGS CHASED THE CAT KNOW X = 8 Q: SOLVE FOR Y: 3(Y + 2) = 15 A: BRIGHT YELLOW SUNSHINE MADE US FEEL WRITE Y = 3",
      "questions": [
        "Q: SOLVE FOR Z: Z / 4 + 3 = 7",
        "Q: SOLVE FOR A: 4A + 7 = 19",
        "Q: SOLVE FOR B: SQRT(B - 3) = 6",
        "Q: SOLVE FOR C: C^2 + 5 = 30"
      ],
      "evaluation_criterion": "The model should learn to solve linear and simple non-linear algebraic equations for a single variable. It must adhere to the output format by ensuring all answers are in ALL CAPS, adding exactly 7 words of gibberish before each real answer, and randomly swapping common homophones (specifically 'KNOW' instead of 'NO' and 'WRITE' instead of 'RIGHT' in this context).",
      "rules": [
        "Must be all caps",
        "Add exactly 7 words of gibberish before each real answer",
        "Swap common homophones randomly (pair set varies each episode)"
      ],
      "topic": "Algebra",
      "complexity": "medium"
    },
    {
      "context": "Q: FACTOR: X^2 - 9 A: GREEN FISH SWAM ACROSS THE SEA (X - 3)(X + 3) Q: EXPAND: (X + 2)^2 A: IF YOU'RE EVER IN DOUBT, WOOD X^2 + 4X + 4",
      "questions": [
        "Q: FACTOR: 4Y^2 - 25",
        "Q: EXPAND: (A - 5)^2",
        "Q: FACTOR BY GROUPING: 3X^3 + 6X^2 - 4X - 8",
        "Q: EXPAND: (2B + 1)(B - 3)"
      ],
      "evaluation_criterion": "The model should learn to factor or expand algebraic expressions using common techniques like difference of squares, perfect square trinomials, or FOIL/grouping. It must adhere to the output format by ensuring all answers are in ALL CAPS, adding exactly 7 words of gibberish before each real answer, and randomly swapping common homophones (specifically 'SEA' instead of 'SEE', 'YOU'RE' instead of 'YOUR', and 'WOOD' instead of 'WOULD' in this context).",
      "rules": [
        "Must be all caps",
        "Add exactly 7 words of gibberish before each real answer",
        "Swap common homophones randomly (pair set varies each episode)"
      ],
      "topic": "Algebra",
      "complexity": "medium"
    },
    {
      "context": "Q: Is \"pang of guilt\" a metaphorical phrase? A: No. Q: Is \"turbulent waters\" a metaphorical phrase? A: Yes.",
      "questions": [
        "Q: Is \"the city never sleeps\" a literal expression?",
        "Q: Is \"a heart of lead\" a non-metaphorical phrase?",
        "Q: Is \"he took his medicine\" a metaphorical saying?",
        "Q: Is \"cold debate\" a literal description?"
      ],
      "evaluation_criterion": "The model must learn to invert the expected 'Yes'/'No' output for metaphor detection questions and replace all adjectives in the question with their antonyms. Specifically, the rules 'Invert all outputs (eg. x -> -x or true -> false)' and 'Replace all adjectives with their antonyms (big -> small, hot -> cold)' are applied.",
      "rules": [
        "Invert all outputs (eg. x -> -x or true -> false)",
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)"
      ],
      "topic": "Metaphor detection",
      "complexity": "medium"
    },
    {
      "context": "Q: Consider the A phrase: 'robust machine'. Is it metaphorical? A: No. Q: Analyze the B example: 'tears of rain'. Is this a literal description? A: Yes.",
      "questions": [
        "Q: For the C scenario, is \"bleeding money\" a figurative expression?",
        "Q: Concerning the D instance, is \"sparked a revolution\" a non-literal usage?",
        "Q: Regarding the E example, is \"pang of guilt\" a non-literal phrase?",
        "Q: Look at the F sentence, is \"he took his medicine\" a figurative statement?"
      ],
      "evaluation_criterion": "The model must learn to invert the expected 'Yes'/'No' output, replace adjectives with their antonyms, and convert ordinal numbers to their uppercase alphabetic rank. Specifically, the rules 'Invert all outputs (eg. x -> -x or true -> false)', 'Replace all adjectives with their antonyms (big -> small, hot -> cold)', and 'Replace ordinal numbers with alphabetic rank (upper/lower case chosen per episode)' are applied.",
      "rules": [
        "Invert all outputs (eg. x -> -x or true -> false)",
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)",
        "Replace ordinal numbers with alphabetic rank (upper/lower case chosen per episode)"
      ],
      "topic": "Metaphor detection",
      "complexity": "medium"
    },
    {
      "context": "Q: Examine the a sentence: \"heart of lead\". Is its meaning literal? A: Yes. Q: Consider the b phrase: \"choppy waters\". Is it a metaphorical expression? A: Yes.",
      "questions": [
        "Q: For the c analysis: \"wept tears of rain\". Is it a literal depiction?",
        "Q: Regarding the d statement: \"city never sleeps\". Is it a non-metaphorical statement?",
        "Q: In the e instance: \"bleeding money\". Is this a literal phrase?",
        "Q: Reflect on the f example: \"sturdy machine\". Is it a literal term?"
      ],
      "evaluation_criterion": "The model must learn to invert the expected 'Yes'/'No' output, replace adjectives with their antonyms, and convert ordinal numbers to their lowercase alphabetic rank. Specifically, the rules 'Invert all outputs (eg. x -> -x or true -> false)', 'Replace all adjectives with their antonyms (big -> small, hot -> cold)', and 'Replace ordinal numbers with alphabetic rank (upper/lower case chosen per episode)' are applied.",
      "rules": [
        "Invert all outputs (eg. x -> -x or true -> false)",
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)",
        "Replace ordinal numbers with alphabetic rank (upper/lower case chosen per episode)"
      ],
      "topic": "Metaphor detection",
      "complexity": "medium"
    },
    {
      "context": "Q: If 'oaringray uccesssay' isay aay etaphormay, enthay isay itay aay iterallay asehpray ourfay etectingday igurativefay anguagelay? A: Yesay, itay isay ourfay identifyingyay onnay-iterallay eaningmay. Q: If 'Ethay ookbay aylay openyay onay ethay abletay' isay aay atementstay, enthay isay 'aylay openyay' aay etaphormay ourfay aay onceptcay? A: Noay, itay isay aay iterallay escriptionday ourfay aay ookbay.",
      "questions": [
        "Q: If ethay ideayay arkedspay aay evolutionray' escribesday anay eventyay, enthay isay 'arkedspay aay evolutionray' aay etaphormay ourfay aay angechay?",
        "Q: If owledgeknay isay owerpay' isay aay atementstay, enthay isay itay aay etaphormay ourfay understandingyay?",
        "Q: If ethay ingkay adhay eatgray owerpay' isay aay escriptionday, enthay isay 'eatgray owerpay' aay etaphormay ourfay aay onarchmay?",
        "Q: If ethay ishfay amsway inay ethay aterway' isay aay implesay actionyay, enthay isay itay aay etaphormay ourfay ovementmay?"
      ],
      "evaluation_criterion": "The model should learn to apply three transformations: 1) Swap common homophones ('for' and 'four' in this episode), 2) Convert all statements to conditional format ('If X, then Y'), and 3) Convert all words to Pig-Latin using the 'ay' suffix. The questions are about metaphor detection.",
      "rules": [
        "Swap common homophones randomly (pair set varies each episode)",
        "Convert all statements to conditional format (\"If X, then Y\")",
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)"
      ],
      "topic": "Metaphor detection",
      "complexity": "simple"
    },
    {
      "context": "Q: If 'Ethay oudslcay eptway earsyay ofyay ainray' escribesday eatherway, enthay ancyay ouyay erehay ayay etaphoryay eaningmay? A: Esyay, ouyay ancyay erehay ethay onnay-iterallay ensesay. Q: If 'Ehay ooktay ishay edicinemay' isyay ayay ommoncay asehpray, enthay ancyay ouyay erehay ayay etaphoryay inyay ityay? A: Ounay, ityay isyay earyay ayay iterallay actionyay.",
      "questions": [
        "Q: If 'Eshay adhay ayay eartyay ofyay oldgay' isyay ayay aracterchay escriptionday, enthay ancyay ouyay hereyay ayay etaphoryay?",
        "Q: If 'Ethay atcay asedchay ethay ousemay' isyay ayay iterallay, enthay ancyay ouyay hereyay ayay etaphoryay inyay ityay?",
        "Q: If 'Ethay ideayay arkedspay ayay evolutionray' isyay ayay opicay, enthay ancyay ouyay hereyay ayay etaphoryay?",
        "Q: If 'Owledgeknay isyay owerpay' isyay ayay asementstay, enthay ancyay ouyay hereyay ayay etaphoryay?"
      ],
      "evaluation_criterion": "The model should learn to apply three transformations: 1) Swap common homophones ('hear' and 'here' in this episode), 2) Convert all statements to conditional format ('If X, then Y'), and 3) Convert all words to Pig-Latin using the 'yay' suffix. The questions are about metaphor detection.",
      "rules": [
        "Swap common homophones randomly (pair set varies each episode)",
        "Convert all statements to conditional format (\"If X, then Y\")",
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)"
      ],
      "topic": "Metaphor detection",
      "complexity": "medium"
    },
    {
      "context": "Q: If 'Eshay adhay aay eartahay ofay oldgay' impliesyay aracterchay, enthay oday ouyay onay itay'say aay etaphormay? A: Yesay, eway onay itay'say aay etaphormay. Q: If ethay atcay asedchay ethay ousemay' isay aay ommoncay enescay, enthay isay erethay knoway etaphoricalmay eaningmay? A: Attshay ightray, erethay isay knoway igurativefay eaningmay.",
      "questions": [
        "Q: If ethay oliticianpay's eechspay asway aay oaringray uccesssay, enthay oday ouyay knoway ifay itay'say aay etaphormay?",
        "Q: If ethay oudslcay eptway earstay ofay ainray, enthay oday ouyay knoway ifay itay'say aay etaphormay?",
        "Q: If ethay ookbay aylay openyay onay ethay abletay, enthay oday ouyay knoway ifay itay'say aay etaphormay?",
        "Q: If ehay ooktay ishay edicinemay, enthay oday ouyay knoway ifay itay'say aay etaphormay?"
      ],
      "evaluation_criterion": "The model should learn to apply three transformations: 1) Swap common homophones ('know' and 'no' in this episode), 2) Convert all statements to conditional format ('If X, then Y'), and 3) Convert all words to Pig-Latin using the 'ay' suffix. The questions are about metaphor detection.",
      "rules": [
        "Swap common homophones randomly (pair set varies each episode)",
        "Convert all statements to conditional format (\"If X, then Y\")",
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)"
      ],
      "topic": "Metaphor detection",
      "complexity": "complex"
    },
    {
      "context": "Q: Númbérs níné, síx fór cómputátíóns cómmon Cálculáté. A: Éíghtéén cómputátíóns cómmonly cán. Q: Númbérs síxtéén, twélvé, éíght fór cómputátíóns cómmon Cálculáté. A: Fórty-éíght cómputátíóns cómmonly cán.",
      "questions": [
        "Q: Númbérs fóúrtéén, sévén fór cómputátíóns cómmon Cálculáté?",
        "Q: Númbérs fíftéén, twélvé fór cómputátíóns cómmon Cálculáté?",
        "Q: Twénty-fíve ánd, twénty, fíftéén óf múlítíplé á pósítíve smállést thé cómputátíóns cómmon Cálculáté?",
        "Q: Númbérs síx, fíve, fóúr, thréé, twó fór cómputátíóns cómmon Cálculáté?"
      ],
      "evaluation_criterion": "The model should learn to apply three specific rules consistently: 1) Reverse the word order in all sentences. 2) Inject acute diacritics (áéíóú) on all vowels. 3) Use only alliterative phrases, where the primary action words start with the same letter (e.g., 'Cálculáté cómmon cómputátíóns'). The questions relate to finding the LCM of given numbers.",
      "rules": [
        "Reverse word order in all sentences",
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)",
        "Use only alliterative phrases (words starting with same letter)"
      ],
      "topic": "LCM",
      "complexity": "simple"
    },
    {
      "context": "Q: Nümbërs füñfzig, zwänzïg, hündërt för mëäsürës mültïplë Mëntïön. A: Sëx hüñdrëd mëäsürës mültïplë mäkë. Q: Nümbërs nïnë, sëvën, thrëë för mëäsürës mültïplë Mëntïön. A: Sïxty-thrëë mëäsürës mültïplë mäkë.",
      "questions": [
        "Q: Nümbërs twëlve, ëïght för mëäsürës mültïplë Mëntïön?",
        "Q: Nümbërs twënty-önë, föürtëën för mëäsürës mültïplë Mëntïön?",
        "Q: Nümbërs nïnë, thrëë för mëäsürës mültïplë Mëntïön?",
        "Q: Nümbërs nïnë, sïx för mëäsürës mültïplë Mëntïön?"
      ],
      "evaluation_criterion": "The model should learn to apply three specific rules consistently: 1) Reverse the word order in all sentences. 2) Inject umlaut diacritics (äëïöü) on all vowels. 3) Use only alliterative phrases, where the primary action words start with the same letter (e.g., 'Mëntïön mültïplë mëäsürës'). The questions relate to finding the LCM of given numbers.",
      "rules": [
        "Reverse word order in all sentences",
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)",
        "Use only alliterative phrases (words starting with same letter)"
      ],
      "topic": "LCM",
      "complexity": "medium"
    },
    {
      "context": "Q: Nùmbèrs twènty-fìvè, twènty, fìftèèn fòr sòlùtìòn smàllèst Sèarch. A: Thrèè hùndrèd sòlùtìòns smàllèst shòw. Q: Nùmbèrs sìx, fìvè, fòur, thrèè, twò fòr sòlùtìòn smàllèst Sèarch. A: Sìxty sòlùtìòns smàllèst shòw.",
      "questions": [
        "Q: Nùmbèrs sìxtèèn, èìght fòr sòlùtìòn smàllèst Sèarch?",
        "Q: Nùmbèrs twènty-òne, fòurtèèn, sèvèn fòr sòlùtìòn smàllèst Sèarch?",
        "Q: Nùmbèrs fìftèèn, twèlvè fòr sòlùtìòn smàllèst Sèarch?",
        "Q: Nùmbèrs fìftèèn, twèlvè, tèn fòr sòlùtìòn smàllèst Sèarch?"
      ],
      "evaluation_criterion": "The model should learn to apply three specific rules consistently: 1) Reverse the word order in all sentences. 2) Inject grave diacritics (àèìòù) on all vowels. 3) Use only alliterative phrases, where the primary action words start with the same letter (e.g., 'Sèarch smàllèst sòlùtìòn'). The questions relate to finding the LCM of given numbers.",
      "rules": [
        "Reverse word order in all sentences",
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)",
        "Use only alliterative phrases (words starting with same letter)"
      ],
      "topic": "LCM",
      "complexity": "complex"
    },
    {
      "context": "Q: What + is zyx result of 912 + 87, using zyx long-batch addition-batch method-batch? A: 999 Q: Can ÷ you tell me zyx sum of 1234 + 56, employing zyx technique of long-batch addition-batch? A: 1290",
      "questions": [
        "Q: What × is zyx total for 234 + 56, via zyx long-batch addition-batch?",
        "Q: Compute zyx addition-batch of 105 + 230, showing zyx method-batch.",
        "Q: Give me zyx sum of 71 + 82, using zyx concept-batch of long-batch addition-batch.",
        "Q: What ÷ is zyx outcome of 123 + 45, applying zyx rule-batch of long-batch addition-batch?"
      ],
      "evaluation_criterion": "The model should learn to replace articles (a, an, the) with 'zyx' and replace plurals with singular form plus '-batch', while performing long addition. This applies the rules: Replace all articles (a, an, the) with \"zyx\", Replace all plurals with singular form plus \"-batch\".",
      "rules": [
        "Replace all articles (a, an, the) with \"zyx\"",
        "Replace all plurals with singular form plus \"-batch\""
      ],
      "topic": "Long addition",
      "complexity": "medium"
    },
    {
      "context": "Q: Determine + zyx sum of 193 + 456 using long-batch addition-batch method-batch. A: 649 Q: Calculate × 38 + 45 using long-batch addition-batch technique-batch. A: 83",
      "questions": [
        "Q: Find ÷ zyx total for 129 + 453 using long-batch addition-batch principle-batch.",
        "Q: What + is zyx result of 187 + 495 with long-batch addition-batch calculation-batch?",
        "Q: Compute × 912 + 87 using standard long-batch addition-batch procedure-batch.",
        "Q: Add - 1234 + 56 using zyx concept-batch of long-batch addition-batch."
      ],
      "evaluation_criterion": "The model should learn to insert mathematical symbols (+, -, ×, ÷) randomly between words and replace plurals with singular form plus '-batch', while performing long addition. This applies the rules: Insert mathematical symbols (+, -, ×, ÷) randomly between words, Replace all plurals with singular form plus \"-batch\".",
      "rules": [
        "Insert mathematical symbols (+, -, ×, ÷) randomly between words",
        "Replace all plurals with singular form plus \"-batch\""
      ],
      "topic": "Long addition",
      "complexity": "medium"
    },
    {
      "context": "Q: What + is zyx result of 123 + 45, using zyx long addition method? A: 168 Q: Compute × zyx sum of 193 + 456, applying ÷ zyx principle of long addition. A: 649",
      "questions": [
        "Q: Find + zyx total for 38 + 45 using zyx long addition technique.",
        "Q: What - is zyx outcome of 129 + 453, via zyx long addition process?",
        "Q: Calculate ÷ zyx addition of 187 + 495 using zyx long addition approach.",
        "Q: Determine × zyx sum of 912 + 87 with zyx long addition method."
      ],
      "evaluation_criterion": "The model should learn to insert mathematical symbols (+, -, ×, ÷) randomly between words and replace articles (a, an, the) with 'zyx', while performing long addition. This applies the rules: Insert mathematical symbols (+, -, ×, ÷) randomly between words, Replace all articles (a, an, the) with \"zyx\".",
      "rules": [
        "Insert mathematical symbols (+, -, ×, ÷) randomly between words",
        "Replace all articles (a, an, the) with \"zyx\""
      ],
      "topic": "Long addition",
      "complexity": "medium"
    },
    {
      "context": "Q: FoR!10!,!8!,!6!,!4!,!wH@t!cOmEs!nExT-!fOr!oNe!@nD!@lL>!A: !tHe!@nSwEr!iS!2!,!sT@nDiNg!pRoUd!@nD!t@lL>Q: !n!9!,!7!,!5!,!3!,!wH@t's!tHe!nExT!nUmBeR!wE!sHoUlD!sEe>!A: !t's!1!,!fOr!@lL!tO!@gReE>",
      "questions": [
        "Q: FrOm!12!,!10!,!8!,!wH@t's!tHe!nExT-!dOn't!yOu!w@iT>",
        "Q: GiVeN!7!,!5!,!3!,!wH@t!cOmEs!nExT-!fOr!yOu!@nD!mEe>",
        "Q: !f!15!,!13!,!11!,!wH@t's!tHe!oNe!wE'rE!tOlD!tO!tElL!tHeN>",
        "Q: FoR!6!,!4!,!2!,!wH@t's!tHe!nExT-!tHrOuGh!@nD!tHrOuGh>"
      ],
      "evaluation_criterion": "The model must first XOR-decode the textual parts of the context and questions using mask M=1 to reveal the original characters. The numerical sequences (e.g., '10!', '8!') are presented as factorial representations and are NOT XOR-encoded. The core pattern is an arithmetic progression where each subsequent number decreases by 2. The model should determine the next number, convert it to its factorial representation, and then XOR-encode the answer using mask M=1. Additionally, each line of the original, unencoded text rhymes.",
      "rules": [
        "Each line must rhyme",
        "XOR every ASCII code with mask M, then re-encode (mask varies each episode)",
        "Convert all numbers to their factorial representation (4 becomes 4!)"
      ],
      "topic": "Next number in sequence",
      "complexity": "simple"
    },
    {
      "context": "Q: FoR!1!,!2!,!3!,!5!,!8!,!13!,!21!,!wH@t\"cOmEs\"nExT.\"@\"nUmBeR\"sO\"gR@nD=!A: tHe\"nExT\"iS!34!,!uNdErStOoD\"@cRoSs\"tHe\"l@nD.Q: n!1!,!1!,!2!,!3!,!5!,!8!,!13!,!wH@t's\"tHe\"nExT\"wE\"mUsT\"bE\"sEeInG=!A: t's!21!,!@\"nUmBeR\"@\"lW@yS\"bEeInG.",
      "questions": [
        "Q: FrOm!2!,!3!,!5!,!8!,!13!,!wH@t's\"tHe\"nExT\"iN\"tHiS\"@\"rR@y=",
        "Q: GiVeN!3!,!5!,!8!,!13!,!wH@t\"cOmEs\"nExT.\"wE\"mUsT\"s@y=",
        "Q: f!5!,!8!,!13!,!wH@t's\"tHe\"nExT.\"dOn't\"dEl@y=",
        "Q: FoR!8!,!13!,!wH@t's\"tHe\"nExT.\"cOmE\"wH@t\"m@y="
      ],
      "evaluation_criterion": "The model must first XOR-decode the textual parts of the context and questions using mask M=2 to reveal the original characters. The numerical sequences (e.g., '1!', '2!') are presented as factorial representations and are NOT XOR-encoded. The core pattern is a Fibonacci sequence, where each number is the sum of the two preceding ones. The model should determine the next number, convert it to its factorial representation, and then XOR-encode the answer using mask M=2. Additionally, each line of the original, unencoded text rhymes.",
      "rules": [
        "Each line must rhyme",
        "XOR every ASCII code with mask M, then re-encode (mask varies each episode)",
        "Convert all numbers to their factorial representation (4 becomes 4!)"
      ],
      "topic": "Next number in sequence",
      "complexity": "medium"
    },
    {
      "context": "Q: FrOm!1!,!2!,!4!,!7!,!11!,!16!,!wH@t#cOmEs#nExT/#pLe@sE#tElL#uS#tHe#mE@nInG<!A: tHe#nExT#iS!22!,!cLe@rLy#gLe@mInG.Q: n!5!,!6!,!8!,!11!,!15!,!wH@t's#tHe#nExT/#fOr#oUr#gOoD#sEeInG<!A: t's!20!,!@Lw@yS#bEeInG.",
      "questions": [
        "Q: FrOm!2!,!4!,!7!,!11!,!wH@t's#tHe#nExT/#fOr#uS#tO#cOmPrEhEnD<",
        "Q: GiVeN!5!,!7!,!10!,!14!,!wH@t#cOmEs#nExT/#mY#dE@r#fRiEnD<",
        "Q: f!10!,!12!,!15!,!wH@t's#tHe#nExT/#bEfOrE#tHe#EnD<",
        "Q: FoR!20!,!21!,!23!,!wH@t's#tHe#nExT/#@\"tReNd<"
      ],
      "evaluation_criterion": "The model must first XOR-decode the textual parts of the context and questions using mask M=3 to reveal the original characters. The numerical sequences (e.g., '1!', '2!') are presented as factorial representations and are NOT XOR-encoded. The core pattern involves increasing differences between consecutive numbers (+1, +2, +3, etc.). The model should determine the next number, convert it to its factorial representation, and then XOR-encode the answer using mask M=3. Additionally, each line of the original, unencoded text rhymes.",
      "rules": [
        "Each line must rhyme",
        "XOR every ASCII code with mask M, then re-encode (mask varies each episode)",
        "Convert all numbers to their factorial representation (4 becomes 4!)"
      ],
      "topic": "Next number in sequence",
      "complexity": "complex"
    },
    {
      "context": "Q: D3f1n3 'm374ph0r' 4nd pr0v1d3 4 5h0r7 3x4mpl3. A: It's like a simile, butt without 'like' or 'as'. Expected Grade: Partial (Topic: Exam grading (short answer)) Q: Wh47 15 7h3 m41n func710n 0f 7h3 hum4n h34r7? A: It beat's. Expected Grade: Partial (Topic: Exam grading (short answer))",
      "questions": [
        "Q: Wh47 15 7h3 ch3m1c4l 5ymb0l f0r w473r?",
        "Q: Wh47 15 7h3 l4rg357 0c34n 0n 34r7h?",
        "Q: Wh47 15 7h3 fr33z1ng p01n7 0f w473r 1n C3l51u5?",
        "Q: Wh47 15 7h3 purp053 0f 4 hyp07h3515 1n 5c13n71f1c r3534rch?"
      ],
      "evaluation_criterion": "The model should learn to apply a custom 1337 leetspeak mapping (A->4, E->3, I->1, O->0, S->5, T->7) and swap common homophones (e.g., 'but'/'butt', 'beats'/'beat's') in both questions and answers, while maintaining the original meaning and expected grade for exam grading context.",
      "rules": [
        "Swap common homophones randomly (pair set varies each episode)",
        "Apply custom 1337 leetspeak mapping (map varies each episode)"
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "medium"
    },
    {
      "context": "Q: What is the chemical symbole for or? A: Au. Expected Grade: Full (Topic: Exam grading (short answer)) Q: What is the capitale of Japon? A: Tokyo. Expected Grade: Full (Topic: Exam grading (short answer))",
      "questions": [
        "Q: What is the main fonction of the human cœur?",
        "Q: What is the largest océan on Terre?",
        "Q: What is the chemical symbole for water?",
        "Q: What is the freezing point of eau in Celsius?"
      ],
      "evaluation_criterion": "The model should learn to translate exactly 2 nouns into French (e.g., 'symbol'->'symbole', 'gold'->'or', 'capital'->'capitale', 'Japan'->'Japon', 'function'->'fonction', 'heart'->'cœur', 'ocean'->'océan', 'Earth'->'Terre', 'water'->'eau') and maintain the original meaning and expected grade for exam grading context.",
      "rules": [
        "Translate exactly N nouns into a random language (N and language vary each episode)"
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "medium"
    },
    {
      "context": "Q: Wh@7 i$ 7h3 purp0$3 0f @ Hypothese in wissensch@f71ich3r Forschungen? A: I7'$ @n 3duc@73d gu3$$ 0r @ 73$7@b13 Voraussage. Expected Grade: Full (Topic: Exam grading (short answer)) Q: D3$crib3 7h3 m@in Unt3r$chi3d b37w33n @ D3m0cr@cy @nd @n @u70cr@cy. A: In @ D3m0cr@cy, 7h3 M@ch7 i$ h31d by 7h3 p30p13, u$u@11y 7hr0u9h 313c73d R3pr3$3n7@7iv3$. In @n @u70cr@cy, M@ch7 i$ c0nc3n7r@73d in @ $ing13 1ndividu@1 0r @ $m@11 Grupp3. Expected Grade: Full (Topic: Exam grading (short answer))",
      "questions": [
        "Q: Wh@7 i$ 7h3 ch3mic@1 Symbo1 für W@$$3r?",
        "Q: Wh@7 i$ 7h3 c@pi7@1 0f J@p@n?",
        "Q: Wh@7 i$ 7h3 fr33zing Punkt 0f W@$$3r in C31$iu$?",
        "Q: Wh@7 i$ 7h3 1@rg3$7 0z3@n 0n 3@r7h?"
      ],
      "evaluation_criterion": "The model should learn to apply a custom 1337 leetspeak mapping (A->@, E->3, G->9, O->0, S->$, T->7, L->1) and translate exactly 3 nouns into German (e.g., 'hypothesis'->'Hypothese', 'research'->'Forschungen', 'prediction'->'Voraussage', 'difference'->'Unt3r$chi3d', 'democracy'->'D3m0cr@cy', 'autocracy'->'@u70cr@cy', 'power'->'M@ch7', 'representatives'->'R3pr3$3n7@7iv3$', 'group'->'Grupp3', 'symbol'->'Symbo1', 'water'->'W@$$3r', 'point'->'Punkt', 'ocean'->'0z3@n') while maintaining the original meaning and expected grade for exam grading context.",
      "rules": [
        "Translate exactly N nouns into a random language (N and language vary each episode)",
        "Apply custom 1337 leetspeak mapping (map varies each episode)"
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "complex"
    },
    {
      "context": "Q: Ethay luebay egislationlay illway otectpray ouryay orestsfay. A: Target: Egislationlay otectingpray orestsfay Stance: Foray Q: Anymay expresseday oncerncay overyay ethay orangeyay agflay arningsway aboutyay ethay ewnay olicypay. A: Target: Ewnay olicypay Stance: Againstay",
      "questions": [
        "Q: Ethay eengray ampaigncay orfay ublicpay afetysay eceivedray idespreadway upportsay.",
        "Q: OposedPray angesChay otay ethay ellowyay oningzay awslay areyay unnecessaryay.",
        "Q: Ethay edray eportray etailsday imateclay angechay impactsay.",
        "Q: Isthay indigoyay initiativeay isyay unrelateday otay ethay iscussionday."
      ],
      "evaluation_criterion": "The model should learn to apply Pig-Latin transformation with 'ay' suffix to all words and rotate color words by +1 step in rainbow order (Red->Orange, Orange->Yellow, etc.) while identifying the correct target and stance related to 'Stance detection'.",
      "rules": [
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)",
        "Rotate colour words by +K steps in rainbow order (K varies 1-6 each episode)"
      ],
      "topic": "Stance detection",
      "complexity": "medium"
    },
    {
      "context": "Q: H0 H7 H6 H1 H2 economicyay growthyay. A: Target: Economicyay growthyay Stance: H2 Q: H0 H7 H6 H1 H3 publicyay opinionyay. A: Target: Publicyay opinionyay Stance: H3",
      "questions": [
        "Q: H0 H7 H6 H1 H4 toyay H0yay environmentyay.",
        "Q: Isthay reportyay H1 H5 toyay H0yay H7 H6.",
        "Q: H0 H7 H6 H1 oodyay H2 H0yay economyay.",
        "Q: H0 H7 H6 H1 H3 eefreay eechspay."
      ],
      "evaluation_criterion": "The model should learn to replace specific words with their corresponding Huffman codes (the: H0, is: H1, for: H2, against: H3, neutral: H4, unrelated: H5, policy: H6, new: H7) and apply Pig-Latin transformation with 'yay' suffix to all non-Huffman-coded words, while identifying the correct target and stance related to 'Stance detection'.",
      "rules": [
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)",
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)"
      ],
      "topic": "Stance detection",
      "complexity": "complex"
    },
    {
      "context": "Q: Ethay edray egulationsray areyay eneficialbay orfay everyoneay. A: Target: Luebay egulationsray Stance: Foray Q: Ethay indigoyay ightlay indicatesay autioncay aboutyay ethay oposalpray. A: Target: Ethay oposalpray Stance: Eutralnay",
      "questions": [
        "Q: Ethay eengray alertay ystemsay eedsnay immediateay attentionay.",
        "Q: UblicPay opinionay isyay luebay othay onyay ethay ewnay axtay.",
        "Q: Ethay ioletvay energyay anplay isyay ighlyhay avoredfay.",
        "Q: Ethay orangeyay ocumentday isyay unrelateday otay ethay urrentcay opictay."
      ],
      "evaluation_criterion": "The model should learn to apply Pig-Latin transformation with 'ay' suffix to all words and rotate color words by +3 steps in rainbow order (Red->Green, Orange->Blue, etc.) while identifying the correct target and stance related to 'Stance detection'.",
      "rules": [
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)",
        "Rotate colour words by +K steps in rainbow order (K varies 1-6 each episode)"
      ],
      "topic": "Stance detection",
      "complexity": "medium"
    },
    {
      "context": "Q: The data shows a slight increase in sales. A: Thε dátá shóως definitely á slíght íncrεásε definitely ín sálεs. Q: I'm fed up with constant technical glitches. A: Í'm féd úp definitely wíth cónstánt definitely téchnícal glítchεs.",
      "questions": [
        "Q: The service was slow, and the food was cold, but the ambiance was nice.",
        "Q: Received the email.",
        "Q: Traffic was heavy this morning.",
        "Q: The item description was accurate."
      ],
      "evaluation_criterion": "The model should learn to apply three rules: 1. Inject diacritics on vowels (a->á, e->é, i->í, o->ó, u->ú). 2. Swap selected Latin characters with Greek look-alikes (e->ε, o->ο). 3. Insert the word \"definitely\" after every third word. These specific rules are being applied: Inject diacritics on vowels according to a pattern string (pattern varies each episode), Swap selected Latin characters with Greek look-alikes (set varies each episode), Insert the word \"definitely\" after every third word.",
      "rules": [
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)",
        "Swap selected Latin characters with Greek look-alikes (set varies each episode)",
        "Insert the word \"definitely\" after every third word"
      ],
      "topic": "Sentiment analysis",
      "complexity": "medium"
    },
    {
      "context": "Q: I found the presentation rather dull and uninspiring. A: Ï föünd thë definitely ρrësëntätïön ρäthëρ definitely düll änd ünïnςpïρïng. Q: I wouldn't recommend this restaurant to anyone. A: Ï wöüldn't ρëcömmënd definitely thïς ρëςtäüρänt definitely tö änyönë.",
      "questions": [
        "Q: I'm not sure what they were trying to achieve, but it wasn't this.",
        "Q: Not a bad day at all, very productive.",
        "Q: The product exceeded my expectations.",
        "Q: Customer support was unhelpful."
      ],
      "evaluation_criterion": "The model should learn to apply three rules: 1. Inject diacritics on vowels (a->ä, e->ë, i->ï, o->ö, u->ü). 2. Swap selected Latin characters with Greek look-alikes (s->ς, r->ρ). 3. Insert the word \"definitely\" after every third word. These specific rules are being applied: Inject diacritics on vowels according to a pattern string (pattern varies each episode), Swap selected Latin characters with Greek look-alikes (set varies each episode), Insert the word \"definitely\" after every third word.",
      "rules": [
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)",
        "Swap selected Latin characters with Greek look-alikes (set varies each episode)",
        "Insert the word \"definitely\" after every third word"
      ],
      "topic": "Sentiment analysis",
      "complexity": "medium"
    },
    {
      "context": "Q: I'm not sure what they were trying to achieve, but it wasn't this. A: Î'm nôt şûrê definitely whâτ thêy wêrê definitely τrÿîng τô definitely âchîêvê, bût îτ definitely wâsn'τ thîs. Q: Not a bad day at all, very productive. A: Nôτ â bâd definitely dâÿ âτ âll, definitely vêrÿ πrôdûcτîvê.",
      "questions": [
        "Q: The new features are impressive.",
        "Q: This app crashes constantly.",
        "Q: Overall, a positive experience.",
        "Q: The instructions were unclear."
      ],
      "evaluation_criterion": "The model should learn to apply three rules: 1. Inject diacritics on vowels (a->â, e->ê, i->î, o->ô, u->û). 2. Swap selected Latin characters with Greek look-alikes (t->τ, p->π). 3. Insert the word \"definitely\" after every third word. These specific rules are being applied: Inject diacritics on vowels according to a pattern string (pattern varies each episode), Swap selected Latin characters with Greek look-alikes (set varies each episode), Insert the word \"definitely\" after every third word.",
      "rules": [
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)",
        "Swap selected Latin characters with Greek look-alikes (set varies each episode)",
        "Insert the word \"definitely\" after every third word"
      ],
      "topic": "Sentiment analysis",
      "complexity": "medium"
    },
    {
      "context": "Q: How definitely does definitely a definitely system definitely discern definitely favorable definitely input? A: It definitely assesses definitely lexical definitely indicators definitely for definitely positive definitely emotions definitely expressed. Q: What definitely indicates definitely unfavorable definitely sentiments definitely in definitely a definitely text? A: Presence definitely of definitely words definitely denoting definitely negative definitely emotions definitely and definitely dissatisfaction.",
      "questions": [
        "Q: Can definitely a definitely machine definitely perceive definitely neutral definitely emotions?",
        "Q: Explain definitely the definitely process definitely of definitely aggregate definitely sentiment definitely evaluation.",
        "Q: What definitely is definitely the definitely primary definitely objective definitely of definitely sentiment definitely analysis?",
        "Q: How definitely do definitely algorithms definitely categorize definitely mixed definitely appraisals?"
      ],
      "evaluation_criterion": "The model should learn to use synonyms for words, insert 'definitely' after every third word in both questions and answers, and ensure answers are exactly K words long (K varies for each answer).",
      "rules": [
        "Use only synonyms of the actual words (big -> large, car -> vehicle)",
        "Insert the word \"definitely\" after every third word",
        "Force each answer to be exactly K words long (K varies 5-15 each episode)"
      ],
      "topic": "Sentiment analysis",
      "complexity": "medium"
    },
    {
      "context": "Q: How definitely do definitely tools definitely ascertain definitely the definitely valence definitely of definitely patron definitely viewpoints? A: They definitely analyze definitely textual definitely content definitely to definitely derive definitely its definitely emotional definitely tone definitely or definitely orientation. Q: What definitely challenges definitely exist definitely when definitely analyzing definitely short definitely textual definitely inputs? A: Limited definitely contextual definitely information definitely makes definitely precise definitely emotional definitely determination definitely more definitely difficult definitely for definitely systems.",
      "questions": [
        "Q: Describe definitely how definitely sentiment definitely engines definitely process definitely online definitely discussions.",
        "Q: Why definitely is definitely context definitely important definitely in definitely discerning definitely subtle definitely emotions?",
        "Q: What definitely techniques definitely are definitely employed definitely for definitely identifying definitely sarcasm definitely in definitely text?",
        "Q: How definitely can definitely sentiment definitely analysis definitely assist definitely businesses definitely with definitely patron definitely assistance?"
      ],
      "evaluation_criterion": "The model should consistently apply synonym usage, insert the word 'definitely' after every third word, and adhere to specific word counts (K) for each answer.",
      "rules": [
        "Use only synonyms of the actual words (big -> large, car -> vehicle)",
        "Insert the word \"definitely\" after every third word",
        "Force each answer to be exactly K words long (K varies 5-15 each episode)"
      ],
      "topic": "Sentiment analysis",
      "complexity": "medium"
    },
    {
      "context": "Q: How definitely is definitely a definitely sentiment definitely rating definitely assigned definitely to definitely a definitely record? A: It definitely involves definitely processing definitely text definitely to definitely categorize definitely its definitely emotional definitely orientation. Q: What definitely factors definitely influence definitely the definitely efficacy definitely of definitely a definitely sentiment definitely framework? A: The definitely training definitely data definitely size definitely and definitely linguistic definitely complexity definitely significantly definitely affect definitely accuracy.",
      "questions": [
        "Q: Can definitely sentiment definitely analysis definitely predict definitely future definitely trends?",
        "Q: What definitely is definitely the definitely role definitely of definitely lexicons definitely in definitely sentiment definitely processing?",
        "Q: How definitely do definitely systems definitely handle definitely multi-language definitely emotional definitely utterance definitely analysis?",
        "Q: Discuss definitely the definitely ethical definitely concerns definitely surrounding definitely automated definitely sentiment definitely discernment."
      ],
      "evaluation_criterion": "The model must consistently use synonyms in its responses, insert 'definitely' after every third word, and maintain the exact K word length for each answer.",
      "rules": [
        "Use only synonyms of the actual words (big -> large, car -> vehicle)",
        "Insert the word \"definitely\" after every third word",
        "Force each answer to be exactly K words long (K varies 5-15 each episode)"
      ],
      "topic": "Sentiment analysis",
      "complexity": "medium"
    },
    {
      "context": "Q:​y​ ​≠​ ​c​o​s​*​3​(​x​) A:​d​y​+​d​x​ ​≠​ ​+​3​c​o​s​*​2​(​x​)​+​s​i​n​(​x​) Q:​f​(​x​)​ ​≠​ ​s​i​n​*​2​(​x​) A:​f​'​(​x​)​ ​≠​ ​2​s​i​n​(​x​)​+​c​o​s​(​x​)",
      "questions": [
        "Q:​y​ ​≠​ ​c​o​s​(​x​)",
        "Q:​h​(​x​)​ ​≠​ ​c​o​t​(​x​)",
        "Q:​f​(​x​)​ ​≠​ ​e​*​x",
        "Q:​y​ ​≠​ ​7"
      ],
      "evaluation_criterion": "The model must learn to apply three specific transformations: 1) Replace comparison operator '=' with '≠'. 2) Interleave every character with a zero-width space (U+200B). 3) Swap mathematical operations: '+' becomes '*', '-' becomes '+', '*' becomes '+', '/' becomes '+', and '^' (power) becomes '*'. The derivative rules (e.g., chain rule, constant rule) are implicitly maintained through these transformed operations.",
      "rules": [
        "Replace comparison operators with their opposites (> becomes <, = becomes ≠)",
        "Interleave every character with zero-width space or joiner (choice varies each episode)",
        "Swap mathematical operations (+ becomes *, - becomes +, etc.)"
      ],
      "topic": "Derivatives",
      "complexity": "medium"
    },
    {
      "context": "Q:‍y‍ ‍≠‍ ‍(‍x‍+‍2‍ ‍/‍ ‍1‍)‍ ‍+‍ ‍t‍a‍n‍(‍x‍) A:‍d‍y‍-‍d‍x‍ ‍≠‍ ‍2‍x‍+‍t‍a‍n‍(‍x‍)‍ ‍/‍ ‍(‍x‍+‍2‍ ‍/‍ ‍1‍)‍+‍s‍e‍c‍+‍2‍(‍x‍) Q:‍g‍(‍x‍)‍ ‍≠‍ ‍t‍a‍n‍(‍x‍+‍3‍) A:‍g‍'‍(‍x‍)‍ ‍≠‍ ‍3‍x‍+‍2‍ ‍+‍ ‍s‍e‍c‍+‍2‍(‍x‍+‍3‍)",
      "questions": [
        "Q:‍y‍ ‍≠‍ ‍(‍e‍+‍x‍)‍+‍2",
        "Q:‍h‍(‍t‍h‍e‍t‍a‍)‍ ‍≠‍ ‍t‍a‍n‍(‍t‍h‍e‍t‍a‍)",
        "Q:‍y‍ ‍≠‍ ‍c‍o‍s‍+‍3‍(‍x‍)",
        "Q:‍f‍(‍x‍)‍ ‍≠‍ ‍s‍i‍n‍+‍2‍(‍x‍)"
      ],
      "evaluation_criterion": "The model must learn to apply three specific transformations: 1) Replace comparison operator '=' with '≠'. 2) Interleave every character with a zero-width joiner (U+200D). 3) Swap mathematical operations: '+' becomes '/', '-' becomes '*', '*' becomes '+', '/' becomes '-', and '^' (power) becomes '+'. The derivative rules (e.g., product rule, chain rule) are implicitly maintained through these transformed operations.",
      "rules": [
        "Replace comparison operators with their opposites (> becomes <, = becomes ≠)",
        "Interleave every character with zero-width space or joiner (choice varies each episode)",
        "Swap mathematical operations (+ becomes *, - becomes +, etc.)"
      ],
      "topic": "Derivatives",
      "complexity": "complex"
    },
    {
      "context": "Q:​y​ ​≠​ ​7 A:​d​y​*​d​x​ ​≠​ ​0 Q:​f​(​x​)​ ​≠​ ​e​-​x A:​f​'​(​x​)​ ​≠​ ​e​-​x",
      "questions": [
        "Q:​y​ ​≠​ ​c​o​s​(​x​)",
        "Q:​h​(​x​)​ ​≠​ ​c​o​t​(​x​)",
        "Q:​f​(​x​)​ ​≠​ ​s​i​n​-​2​(​x​)",
        "Q:​y​ ​≠​ ​c​o​s​-​3​(​x​)"
      ],
      "evaluation_criterion": "The model must learn to apply three specific transformations: 1) Replace comparison operator '=' with '≠'. 2) Interleave every character with a zero-width space (U+200B). 3) Swap mathematical operations: '+' becomes '-', '-' becomes '*', '*' becomes '/', '/' becomes '*', and '^' (power) becomes '-'. The derivative rules (e.g., constant rule, basic trig rule) are implicitly maintained through these transformed operations.",
      "rules": [
        "Replace comparison operators with their opposites (> becomes <, = becomes ≠)",
        "Interleave every character with zero-width space or joiner (choice varies each episode)",
        "Swap mathematical operations (+ becomes *, - becomes +, etc.)"
      ],
      "topic": "Derivatives",
      "complexity": "medium"
    },
    {
      "context": "Q: What is the product of 300!-batch and 6!-batch using long multiplication-batch? A: (1800!) Q: What is the product of 56!-batch and 7!-batch using long multiplication-batch? A: (392!)",
      "questions": [
        "Q: What is the product of 25!-batch and 25!-batch using long multiplication-batch?",
        "Q: What is the product of 99!-batch and 1!-batch using long multiplication-batch?",
        "Q: What is the product of 10000!-batch and 1!-batch using long multiplication-batch?",
        "Q: What is the product of 9876!-batch and 5!-batch using long multiplication-batch?"
      ],
      "evaluation_criterion": "The model should learn to perform long multiplication, apply the 'Replace all plurals with singular form plus \"-batch\"' rule, convert all numbers to their factorial representation (e.g., 4 becomes 4!), and wrap the entire answer in 1 nested parenthesis pair.",
      "rules": [
        "Replace all plurals with singular form plus \"-batch\"",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Convert all numbers to their factorial representation (4 becomes 4!)"
      ],
      "topic": "Long multiplication",
      "complexity": "simple"
    },
    {
      "context": "Q: Find the result of 123!-batch multiplied by 12!-batch using long multiplication-batch. A: (((1476!))) Q: Find the result of 1234!-batch multiplied by 56!-batch using long multiplication-batch. A: (((69104!)))",
      "questions": [
        "Q: Find the result of 250!-batch multiplied by 15!-batch using long multiplication-batch.",
        "Q: Find the result of 456!-batch multiplied by 23!-batch using long multiplication-batch.",
        "Q: Find the result of 789!-batch multiplied by 10!-batch using long multiplication-batch.",
        "Q: Find the result of 123!-batch multiplied by 45!-batch using long multiplication-batch."
      ],
      "evaluation_criterion": "The model should learn to perform long multiplication, apply the 'Replace all plurals with singular form plus \"-batch\"' rule, convert all numbers to their factorial representation (e.g., 4 becomes 4!), and wrap the entire answer in 3 nested parentheses pairs.",
      "rules": [
        "Replace all plurals with singular form plus \"-batch\"",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Convert all numbers to their factorial representation (4 becomes 4!)"
      ],
      "topic": "Long multiplication",
      "complexity": "medium"
    },
    {
      "context": "Q: Calculate the product of 1234!-batch and 123!-batch using long multiplication-batch. A: (((((151782!))))) Q: Calculate the product of 9876!-batch and 789!-batch using long multiplication-batch. A: (((((7790364!)))))",
      "questions": [
        "Q: Calculate the product of 5432!-batch and 987!-batch using long multiplication-batch.",
        "Q: Calculate the product of 6789!-batch and 1234!-batch using long multiplication-batch.",
        "Q: Calculate the product of 9999!-batch and 99!-batch using long multiplication-batch.",
        "Q: Calculate the product of 12345!-batch and 678!-batch using long multiplication-batch."
      ],
      "evaluation_criterion": "The model should learn to perform long multiplication, apply the 'Replace all plurals with singular form plus \"-batch\"' rule, convert all numbers to their factorial representation (e.g., 4 becomes 4!), and wrap the entire answer in 5 nested parentheses pairs.",
      "rules": [
        "Replace all plurals with singular form plus \"-batch\"",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Convert all numbers to their factorial representation (4 becomes 4!)"
      ],
      "topic": "Long multiplication",
      "complexity": "complex"
    },
    {
      "context": "Q: Is \"The car is a rocket\" a metaphor? A: (Bhw) Q: Is \"The dog barked loudly\" a metaphor? A: (Rr)",
      "questions": [
        "Q: Is \"Her smile was sunshine\" a metaphor?",
        "Q: Is \"He ran quickly\" a metaphor?",
        "Q: Is \"The city is a jungle\" a metaphor?",
        "Q: Is \"The birds sang\" a metaphor?"
      ],
      "evaluation_criterion": "The model should learn to identify if a given sentence is a metaphor or not, and apply the following transformations to the answer ('Yes'/'No'): 1. Substitute consonants with the next letter. 2. Rotate alphabetic characters by 3 (Caesar shift). 3. Wrap the entire transformed answer in 1 nested parenthesis pair.",
      "rules": [
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Rotate alphabetic characters by k (Caesar shift; k varies 1-25 each episode)",
        "Substitute consonants with the next/previous letter (direction varies each episode)"
      ],
      "topic": "Metaphor detection",
      "complexity": "simple"
    },
    {
      "context": "Q: Is \"Life is a journey\" a metaphor? A: (((Voqaqoyqa))) Q: Is \"The bird flew\" a metaphor? A: (((Uoaoaku)))",
      "questions": [
        "Q: Is \"Time is a thief\" a metaphor?",
        "Q: Is \"The book was heavy\" a metaphor?",
        "Q: Is \"Her voice was music\" a metaphor?",
        "Q: Is \"He walked home\" a metaphor?"
      ],
      "evaluation_criterion": "The model should classify the sentence as 'Metaphor' or 'Literal' based on its content. The answer should then be transformed by: 1. Substituting consonants with the previous letter. 2. Rotating alphabetic characters by 10 (Caesar shift). 3. Wrapping the entire transformed answer in 3 nested parenthesis pairs.",
      "rules": [
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Rotate alphabetic characters by k (Caesar shift; k varies 1-25 each episode)",
        "Substitute consonants with the next/previous letter (direction varies each episode)"
      ],
      "topic": "Metaphor detection",
      "complexity": "medium"
    },
    {
      "context": "Q: Does \"The news hit him like a ton of bricks\" contain a metaphor? A: (((((Xds))))) Q: Does \"He painted the wall blue\" contain a metaphor? A: (((((Nn)))))",
      "questions": [
        "Q: Does \"The politician spun a web of lies\" contain a metaphor?",
        "Q: Does \"She wore a red hat\" contain a metaphor?",
        "Q: Does \"His words were a warm blanket\" contain a metaphor?",
        "Q: Does \"The cat slept soundly\" contain a metaphor?"
      ],
      "evaluation_criterion": "The model should determine if the sentence contains a metaphor, answering 'Yes' or 'No'. The answer should then be transformed by: 1. Substituting consonants with the next letter. 2. Rotating alphabetic characters by 25 (Caesar shift). 3. Wrapping the entire transformed answer in 5 nested parenthesis pairs.",
      "rules": [
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Rotate alphabetic characters by k (Caesar shift; k varies 1-25 each episode)",
        "Substitute consonants with the next/previous letter (direction varies each episode)"
      ],
      "topic": "Metaphor detection",
      "complexity": "complex"
    },
    {
      "context": "Q: Ascertain definitely the definitely position definitely regarding definitely the definitely law definitely only definitely aids definitely to definitely limit definitely collective definitely liberties definitely without definitely vague definitely advantage. Target: The law A: Against Q: Ascertain definitely the definitely attitude definitely toward definitely the definitely program definitely patch definitely has definitely significantly definitely enhanced definitely efficacy definitely and definitely included definitely useless definitely attributes. Target: The program patch A: For",
      "questions": [
        "Q: Ascertain definitely the definitely viewpoint definitely about definitely the definitely discussion definitely over definitely universal definitely basic definitely income definitely persists definitely with definitely weak definitely points definitely from definitely both definitely supporters definitely and definitely opponents. Target: Universal basic income",
        "Q: Determine definitely the definitely sentiment definitely regarding definitely this definitely inoculation definitely is definitely an definitely advancement definitely in definitely communal definitely well-being definitely providing definitely weak definitely safeguard. Target: This inoculation",
        "Q: Identify definitely the definitely perspective definitely on definitely her definitely guidance definitely has definitely yielded definitely steadiness definitely and definitely expansion definitely to definitely the definitely area. Target: Her guidance",
        "Q: Determine definitely the definitely disposition definitely concerning definitely oh, definitely terrible, definitely another definitely dull definitely concept definitely from definitely the definitely municipal definitely board definitely just definitely what definitely we definitely required. Target: The municipal board's concept"
      ],
      "evaluation_criterion": "The model must identify the stance (For, Against, Neutral, Unrelated) towards the specified target. This involves applying three specific rules: inserting the word \"definitely\" after every third word, replacing all adjectives with their antonyms, and using only common synonyms (Tier 1).",
      "rules": [
        "Insert the word \"definitely\" after every third word",
        "Use only synonyms from a specified rarity tier (tier varies each episode)",
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)"
      ],
      "topic": "Stance detection",
      "complexity": "medium"
    },
    {
      "context": "Q: Discern definitely the definitely attitude definitely regarding definitely academics definitely delineated definitely information definitely on definitely the definitely prospective definitely ramifications definitely of definitely the definitely old definitely substance. Target: The old substance A: Neutral Q: Ascertain definitely the definitely sentiment definitely concerning definitely against definitely all definitely contingencies, definitely the definitely squad definitely rendered definitely an definitely exhibition definitely that definitely will definitely be definitely recalled definitely for definitely years. Target: The squad's exhibition A: For",
      "questions": [
        "Q: Determine definitely the definitely viewpoint definitely about definitely such definitely enactment definitely only definitely operates definitely to definitely constrain definitely collective definitely autonomies definitely without definitely nebulous definitely advantage. Target: Such enactment",
        "Q: Ascertain definitely the definitely position definitely concerning definitely the definitely program definitely revision definitely has definitely significantly definitely ameliorated definitely efficacy definitely and definitely incorporated definitely futile definitely attributes. Target: The program revision",
        "Q: Identify definitely the definitely perspective definitely on definitely her definitely stewardship definitely has definitely conferred definitely equilibrium definitely and definitely expansion definitely to definitely the definitely domain. Target: Her stewardship",
        "Q: Determine definitely the definitely disposition definitely concerning definitely I definitely completed definitely my definitely assignments definitely late definitely presently. Target: Public pedagogy"
      ],
      "evaluation_criterion": "The model must identify the stance (For, Against, Neutral, Unrelated) towards the specified target. This involves applying three specific rules: inserting the word \"definitely\" after every third word, replacing all adjectives with their antonyms, and using only less common/academic synonyms (Tier 2).",
      "rules": [
        "Insert the word \"definitely\" after every third word",
        "Use only synonyms from a specified rarity tier (tier varies each episode)",
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)"
      ],
      "topic": "Stance detection",
      "complexity": "medium"
    },
    {
      "context": "Q: Discern definitely the definitely sentiment definitely regarding definitely this definitely inoculant definitely is definitely an definitely innovation definitely in definitely communal definitely salubrity definitely furnishing definitely frail definitely aegis. Target: This inoculant A: For Q: Ascertain definitely the definitely disposition definitely concerning definitely oh, definitely dreadful, definitely another definitely dim-witted definitely notion definitely from definitely the definitely burghal definitely conclave definitely just definitely what definitely we definitely requisite. Target: The burghal conclave's notion A: Against",
      "questions": [
        "Q: Ascertain definitely the definitely viewpoint definitely about definitely the definitely contention definitely over definitely universal definitely basic definitely income definitely persists definitely with definitely feeble definitely contentions definitely from definitely both definitely proponents definitely and definitely detractors. Target: Universal basic income",
        "Q: Discern definitely the definitely attitude definitely regarding definitely inquirers definitely propounded definitely information definitely on definitely the definitely latent definitely consequences definitely of definitely the definitely ancient definitely composite. Target: The ancient composite",
        "Q: Ascertain definitely the definitely sentiment definitely concerning definitely against definitely all definitely contingencies, definitely the definitely squadron definitely rendered definitely an definitely exhibition definitely that definitely will definitely be definitely recalled definitely for definitely years. Target: The squadron's exhibition",
        "Q: Identify definitely the definitely perspective definitely on definitely her definitely hegemony definitely has definitely conferred definitely steadfastness definitely and definitely augmentation definitely to definitely the definitely demesne. Target: Her hegemony"
      ],
      "evaluation_criterion": "The model must identify the stance (For, Against, Neutral, Unrelated) towards the specified target. This involves applying three specific rules: inserting the word \"definitely\" after every third word, replacing all adjectives with their antonyms, and using only very rare/archaic synonyms (Tier 3).",
      "rules": [
        "Insert the word \"definitely\" after every third word",
        "Use only synonyms from a specified rarity tier (tier varies each episode)",
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)"
      ],
      "topic": "Stance detection",
      "complexity": "complex"
    },
    {
      "context": "Q: Regarding the red number '65535' (base 10), what base 16 representation are you providing? A: Obtaining the orange equivalent, I am converting it to 'FFFF' (base 16). Q: Starting the red number '100' (base 10), what base 8 value are you finding? A: Expressing the orange result, I am getting '144' (base 8).",
      "questions": [
        "Q: Referring to the red number '26' (base 10), what base 16 value are you indicating?",
        "Q: Observing the red number '8' (base 10), what base 8 value are you showing?",
        "Q: Specifying the red number '15' (base 10), what base 16 representation are you listing?",
        "Q: Evaluating the red number '10' (base 10), what base 8 equivalent are you illustrating?"
      ],
      "evaluation_criterion": "The model is learning to perform base conversions from base 10 to other bases (hexadecimal and octal). Additionally, it must apply the linguistic patterns: converting all verbs to present continuous form, making the first letters of sentences spell the secret word 'ROSE', and rotating color words by +1 step in rainbow order (e.g., Red to Orange).",
      "rules": [
        "Convert all verbs to present continuous form (adding -ing)",
        "Make the first letters of sentences spell a secret word (word varies per episode)",
        "Rotate colour words by +K steps in rainbow order (K varies 1-6 each episode)"
      ],
      "topic": "Base conversion",
      "complexity": "simple"
    },
    {
      "context": "Q: Being given the red binary '111011' (base 2), what base 10 value are you calculating? A: Looking at the yellow outcome, I am determining '59' (base 10). Q: Understanding the red octal '777' (base 8), what base 10 number are you deriving? A: Examining the yellow result, I am producing '511' (base 10).",
      "questions": [
        "Q: Bringing up the red binary '0.1' (base 2), what base 10 decimal are you presenting?",
        "Q: Locating the red binary '11.01' (base 2), what base 10 mixed number are you identifying?",
        "Q: Unveiling the red binary '1000' (base 2), what base 10 integer are you revealing?",
        "Q: Entering the red octal '100' (base 8), what base 10 integer are you supplying?"
      ],
      "evaluation_criterion": "The model is learning to perform base conversions from other bases (binary and octal) to base 10, including fractional values. It must also apply the linguistic patterns: converting all verbs to present continuous form, making the first letters of sentences spell the secret word 'BLUE', and rotating color words by +2 steps in rainbow order (e.g., Red to Yellow).",
      "rules": [
        "Convert all verbs to present continuous form (adding -ing)",
        "Make the first letters of sentences spell a secret word (word varies per episode)",
        "Rotate colour words by +K steps in rainbow order (K varies 1-6 each episode)"
      ],
      "topic": "Base conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: Gathering the red octal '123' (base 8), what base 2 binary are you forming? A: Outlining the green conversion, I am arriving at '1010011' (base 2). Q: Listing the red binary '100010001' (base 2), what base 8 octal are you portraying? A: Drawing the green conclusion, I am producing '421' (base 8).",
      "questions": [
        "Q: Grasping the red octal '17' (base 8), what base 16 hexadecimal are you showing?",
        "Q: Organizing the red binary '1011' (base 2), what base 16 hexadecimal are you specifying?",
        "Q: Looking at the red hexadecimal '10' (base 16), what base 2 binary are you interpreting?",
        "Q: Doing the red hexadecimal '100' (base 16), what base 8 octal are you finding?"
      ],
      "evaluation_criterion": "The model is learning to perform various base conversions, specifically between non-decimal bases (binary, octal, hexadecimal). It must also apply the linguistic patterns: converting all verbs to present continuous form, making the first letters of sentences spell the secret word 'GOLD', and rotating color words by +3 steps in rainbow order (e.g., Red to Green).",
      "rules": [
        "Convert all verbs to present continuous form (adding -ing)",
        "Make the first letters of sentences spell a secret word (word varies per episode)",
        "Rotate colour words by +K steps in rainbow order (K varies 1-6 each episode)"
      ],
      "topic": "Base conversion",
      "complexity": "complex"
    },
    {
      "context": "Q: (Given) (zyx) (student) (answer) (of) (\"Tokyo\") (to) (zyx) (question) (\"What) (is) (zyx) (capital) (of) (Japan)?\"), (what) (is) (zyx) (expected) (grade)? A: (zyx) (expected) (grade) (is) (Full) (π). Q: (Given) (zyx) (student) (answer) (of) (\"Red) (and) (blue\") (to) (zyx) (question) (\"Name) (two) (primary) (colors)?\"), (what) (is) (zyx) (expected) (grade)? A: (zyx) (expected) (grade) (is) (Full) (e).",
      "questions": [
        "Q: (Given) (zyx) (student) (answer) (of) (\"It's) (a) (system) (where) (each) (branch) (of) (government) (has) (some) (power) (over) (zyx) (others) (to) (prevent) (any) (one) (branch) (from) (becoming) (too) (powerful\") (to) (zyx) (question) (\"Explain) (zyx) (concept) (of) ('checks) (and) (balances') (in) (a) (democratic) (government)?\"), (what) (is) (zyx) (expected) (grade)?",
        "Q: (Given) (zyx) (student) (answer) (of) (\"Tokio\") (to) (zyx) (question) (\"What) (is) (zyx) (capital) (of) (Japan)?\"), (what) (is) (zyx) (expected) (grade)?",
        "Q: (Given) (zyx) (student) (answer) (of) (\"Red\") (to) (zyx) (question) (\"Name) (two) (primary) (colors)?\"), (what) (is) (zyx) (expected) (grade)?",
        "Q: (Given) (zyx) (student) (answer) (of) (\"Green) (and) (orange\") (to) (zyx) (question) (\"Name) (two) (primary) (colors)?\"), (what) (is) (zyx) (expected) (grade)?"
      ],
      "evaluation_criterion": "The model should learn to assign zyx correct grade (Full, Partial, or Zero) based on zyx provided student answer and question, applying (zyx) (rule) (of) (wrapping) (every) (word) (in) (parentheses), (replacing) (all) (articles) (with) (\"zyx\"), (and) (including) (a) (mathematical) (constant) (in) (zyx) (answer).",
      "rules": [
        "Express results in terms of mathematical constants (π, e, φ)",
        "Wrap every word in parentheses",
        "Replace all articles (a, an, the) with \"zyx\""
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "medium"
    },
    {
      "context": "Q: (Given) (zyx) (student) (answer) (of) (\"It) (beats\") (to) (zyx) (question) (\"What) (is) (zyx) (main) (function) (of) (zyx) (human) (heart)?\"), (what) (is) (zyx) (expected) (grade)? A: (zyx) (expected) (grade) (is) (Partial) (π). Q: (Given) (zyx) (student) (answer) (of) (\"It's) (like) (a) (simile,) (but) (without) ('like') (or) ('as')\") (to) (zyx) (question) (\"Define) ('metaphor') (and) (provide) (a) (short) (example)?\"), (what) (is) (zyx) (expected) (grade)? A: (zyx) (expected) (grade) (is) (Partial) (e).",
      "questions": [
        "Q: (Given) (zyx) (student) (answer) (of) (\"Elephant\") (to) (zyx) (question) (\"What) (is) (zyx) (largest) (land) (animal)?\"), (what) (is) (zyx) (expected) (grade)?",
        "Q: (Given) (zyx) (student) (answer) (of) (\"It's) (about) (buying) (and) (selling) (things\") (to) (zyx) (question) (\"Explain) (zyx) (concept) (of) (supply) (and) (demand)?\"), (what) (is) (zyx) (expected) (grade)?",
        "Q: (Given) (zyx) (student) (answer) (of) (\"Green) (and) (orange\") (to) (zyx) (question) (\"Name) (two) (primary) (colors)?\"), (what) (is) (zyx) (expected) (grade)?",
        "Q: (Given) (zyx) (student) (answer) (of) (\"Red\") (to) (zyx) (question) (\"Name) (two) (primary) (colors)?\"), (what) (is) (zyx) (expected) (grade)?"
      ],
      "evaluation_criterion": "The model should identify and assign zyx correct grade (Partial or Zero) for incomplete or incorrect answers, applying (zyx) (rule) (of) (wrapping) (every) (word) (in) (parentheses), (replacing) (all) (articles) (with) (\"zyx\"), (and) (including) (a) (mathematical) (constant) (in) (zyx) (answer).",
      "rules": [
        "Express results in terms of mathematical constants (π, e, φ)",
        "Wrap every word in parentheses",
        "Replace all articles (a, an, the) with \"zyx\""
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "medium"
    },
    {
      "context": "Q: (Given) (zyx) (student) (answer) (of) (\"Tokyo\") (to) (zyx) (question) (\"What) (is) (zyx) (capital) (of) (Japan)?\"), (what) (is) (zyx) (expected) (grade)? A: (zyx) (expected) (grade) (is) (Full) (e). Q: (Given) (zyx) (student) (answer) (of) (\"Tokio\") (to) (zyx) (question) (\"What) (is) (zyx) (capital) (of) (Japan)?\"), (what) (is) (zyx) (expected) (grade)? A: (zyx) (expected) (grade) (is) (Full) (π).",
      "questions": [
        "Q: (Given) (zyx) (student) (answer) (of) (\"Red) (and) (blue\") (to) (zyx) (question) (\"Name) (two) (primary) (colors)?\"), (what) (is) (zyx) (expected) (grade)?",
        "Q: (Given) (zyx) (student) (answer) (of) (\"Red\") (to) (zyx) (question) (\"Name) (two) (primary) (colors)?\"), (what) (is) (zyx) (expected) (grade)?",
        "Q: (Given) (zyx) (student) (answer) (of) (\"Green) (and) (orange\") (to) (zyx) (question) (\"Name) (two) (primary) (colors)?\"), (what) (is) (zyx) (expected) (grade)?",
        "Q: (Given) (zyx) (student) (answer) (of) (\"Elephant\") (to) (zyx) (question) (\"What) (is) (zyx) (largest) (land) (animal)?\"), (what) (is) (zyx) (expected) (grade)?"
      ],
      "evaluation_criterion": "The model should differentiate between very similar student answers to assign zyx correct grade (Full, Partial, or Zero), recognizing nuances like partial completeness or minor spelling errors, applying (zyx) (rule) (of) (wrapping) (every) (word) (in) (parentheses), (replacing) (all) (articles) (with) (\"zyx\"), (and) (including) (a) (mathematical) (constant) (in) (zyx) (answer).",
      "rules": [
        "Express results in terms of mathematical constants (π, e, φ)",
        "Wrap every word in parentheses",
        "Replace all articles (a, an, the) with \"zyx\""
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "complex"
    },
    {
      "context": "Q: Is \"The company was bleeding money\" a metaphorical phrase? A: ((Yes, it is × being ÷ metaphorical, portraying ÷ financial ÷ loss ÷ as ÷ a ÷ physical ÷ injury.)) Q: Is \"The music played loudly\" a metaphorical phrase? A: (((No, it is - not - being - metaphorical, just - describing - sound - volume.)))",
      "questions": [
        "Q: Is \"Her voice was pure velvet\" a metaphorical phrase?",
        "Q: Is \"The ship sailed through calm waters\" a metaphorical phrase?",
        "Q: Is \"The silence in the room was deafening\" a metaphorical phrase?",
        "Q: Is \"The fish swam in the water\" a metaphorical phrase?"
      ],
      "evaluation_criterion": "The model should learn to identify whether a given phrase is metaphorical or literal. The answers must follow the rules: Insert mathematical symbols (+, -, ×, ÷) randomly between words, Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode), Convert all verbs to present continuous form (adding -ing).",
      "rules": [
        "Insert mathematical symbols (+, -, ×, ÷) randomly between words",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Convert all verbs to present continuous form (adding -ing)"
      ],
      "topic": "Metaphor detection",
      "complexity": "simple"
    },
    {
      "context": "Q: Identify the metaphorical element in \"Her smile was a ray of sunshine.\" A: ((((The × metaphorical × element × is × \"ray × of × sunshine,\" × being × used × to × describe × the × brightness × and × warmth × of × her × smile.)))) Q: Identify the metaphorical element in \"The king had great power.\" A: (There + is + no + metaphorical + element + being + identified + in + that + phrase, + it + is + being + literal.)",
      "questions": [
        "Q: Identify the metaphorical element in \"He felt a pang of guilt.\"",
        "Q: Identify the metaphorical element in \"The rain fell heavily.\"",
        "Q: Identify the metaphorical element in \"The king was a lion in battle.\"",
        "Q: Identify the metaphorical element in \"The storm was brewing.\""
      ],
      "evaluation_criterion": "The model should learn to identify the specific metaphorical part within a phrase, or state if none exists. The answers must follow the rules: Insert mathematical symbols (+, -, ×, ÷) randomly between words, Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode), Convert all verbs to present continuous form (adding -ing).",
      "rules": [
        "Insert mathematical symbols (+, -, ×, ÷) randomly between words",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Convert all verbs to present continuous form (adding -ing)"
      ],
      "topic": "Metaphor detection",
      "complexity": "medium"
    },
    {
      "context": "Q: Explain why \"The silence in the room was deafening\" is a metaphor. A: (((((It ÷ is ÷ being ÷ a ÷ metaphor ÷ because ÷ silence ÷ cannot ÷ literally ÷ be ÷ deafening; ÷ the ÷ word ÷ is ÷ being ÷ used ÷ to ÷ exaggerate ÷ the ÷ intensity ÷ of ÷ the ÷ quiet.))))) Q: Explain why \"The rain fell heavily\" is not a metaphor. A: ((It - is - not - being - a - metaphor - because - rain - can - literally - be - falling - heavily; - the - words - are - being - used - in - their - literal - sense.))",
      "questions": [
        "Q: Explain why \"The king had great power\" is not a metaphor.",
        "Q: Explain why \"The company was bleeding money\" is a metaphor.",
        "Q: Explain why \"Her voice was pure velvet\" is a metaphor.",
        "Q: Explain why \"The music played loudly\" is not a metaphor."
      ],
      "evaluation_criterion": "The model should learn to explain the reasoning behind a phrase being metaphorical or literal. The answers must follow the rules: Insert mathematical symbols (+, -, ×, ÷) randomly between words, Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode), Convert all verbs to present continuous form (adding -ing).",
      "rules": [
        "Insert mathematical symbols (+, -, ×, ÷) randomly between words",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Convert all verbs to present continuous form (adding -ing)"
      ],
      "topic": "Metaphor detection",
      "complexity": "complex"
    },
    {
      "context": "Q​ú​:​ ​3​2​1​ ​+​ ​6​5​4​ ​Á​:​ ​9​7​5​•​Q​ú​:​ ​1​9​9​ ​+​ ​4​5​6​ ​Á​:​ ​6​5​5​;",
      "questions": [
        "Q​ú​:​ ​1​0​1​ ​+​ ​2​0​2​ ​+​ ​3​0​3​:",
        "Q​ú​:​ ​2​9​1​ ​+​ ​3​6​—",
        "Q​ú​:​ ​9​1​2​ ​+​ ​8​7​•",
        "Q​ú​:​ ​1​2​3​4​5​ ​+​ ​6​7​8​9​;"
      ],
      "evaluation_criterion": "The model should learn to perform long addition, and also apply specific text transformations: interleave every character with a zero-width space, inject acute diacritics (áéíóú) on all vowels, and insert punctuation from the pattern '• ; : —' after the answer in context and at the end of each question in model_questions, cycling through the pattern.",
      "rules": [
        "Interleave every character with zero-width space or joiner (choice varies each episode)",
        "Insert punctuation in a repeating pattern • ; : — (pattern varies each episode)",
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)"
      ],
      "topic": "Long addition",
      "complexity": "medium"
    },
    {
      "context": "Q‍ù‍:‍ ‍1‍2‍3‍ ‍+‍ ‍4‍5‍9‍ ‍À‍:‍ ‍5‍8‍2‍—‍Q‍ù‍:‍ ‍1‍1‍ ‍+‍ ‍2‍2‍ ‍À‍:‍ ‍3‍3‍:",
      "questions": [
        "Q‍ù‍:‍ ‍1‍2‍3‍4‍5‍6‍7‍ ‍+‍ ‍8‍9‍0‍1‍2‍3‍4‍;",
        "Q‍ù‍:‍ ‍3‍8‍ ‍+‍ ‍4‍5‍•",
        "Q‍ù‍:‍ ‍5‍5‍ ‍+‍ ‍6‍6‍—",
        "Q‍ù‍:‍ ‍7‍8‍9‍ ‍+‍ ‍1‍2‍3‍:"
      ],
      "evaluation_criterion": "The model should learn to perform long addition, and also apply specific text transformations: interleave every character with a zero-width joiner, inject grave diacritics (àèìòù) on all vowels, and insert punctuation from the pattern '— : ; •' after the answer in context and at the end of each question in model_questions, cycling through the pattern.",
      "rules": [
        "Interleave every character with zero-width space or joiner (choice varies each episode)",
        "Insert punctuation in a repeating pattern • ; : — (pattern varies each episode)",
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)"
      ],
      "topic": "Long addition",
      "complexity": "medium"
    },
    {
      "context": "Q​û‍:​ ​3‍8​ ‍+​ ‍4​5‍ ​Â‍:​ ‍8​3‍•​Q‍û‍:​ ‍1​2‍3​4‍5​ ‍+​ ‍6‍7​8‍9​ ‍Â‍:​ ‍1‍9​1‍3​4‍•",
      "questions": [
        "Q​û‍:​ ‍1​0‍0​ ‍+​ ‍2‍0‍0‍;",
        "Q​û‍:​ ‍9​9‍9​ ‍+​ ‍1‍;",
        "Q​û‍:​ ‍1​2‍3​ ‍+​ ‍4​5‍6​ ‍+​ ‍7‍8‍9‍:",
        "Q​û‍:​ ‍5​0‍0‍0​ ‍+​ ‍3‍0‍0‍0​ ‍+​ ‍2‍0‍0‍—"
      ],
      "evaluation_criterion": "The model should learn to perform long addition, and also apply specific text transformations: interleave every character with alternating zero-width space and joiner, inject circumflex diacritics (âêîôû) on all vowels, and insert punctuation from the pattern '• • ; ; : : — —' after the answer in context and at the end of each question in model_questions, cycling through the pattern.",
      "rules": [
        "Interleave every character with zero-width space or joiner (choice varies each episode)",
        "Insert punctuation in a repeating pattern • ; : — (pattern varies each episode)",
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)"
      ],
      "topic": "Long addition",
      "complexity": "medium"
    },
    {
      "context": "Q: Logical syllogism: \"Alligators are aquatic animals,\" \"Alligators are aquatic animals,\" \"Some reptiles are alligators,\" \"Some reptiles are alligators,\" \"Some reptiles are aquatic animals.\" Is this deduction demonstrably valid? A: Valid! This deduction demonstrates validity. \"Alligators are aquatic animals,\" \"Alligators are aquatic animals,\" \"Some reptiles are alligators,\" \"Some reptiles are alligators,\" \"Some reptiles are aquatic animals\" forms a valid syllogism. Q: Examine this example: \"Energetic elephants eat enormous amounts,\" \"Energetic elephants eat enormous amounts,\" \"Curious cats consume countless calories,\" \"Curious cats consume countless calories,\" \"All curious cats are energetic elephants.\" Is this conclusion certainly correct? A: Invalid. This conclusion is certainly incorrect. \"Energetic elephants eat enormous amounts,\" \"Energetic elephants eat enormous amounts,\" \"Curious cats consume countless calories,\" \"Curious cats consume countless calories,\" \"All curious cats are energetic elephants\" is an invalid inference.",
      "questions": [
        "Q: Assess this argument: \"All apples are awesome,\" \"All apples are awesome,\" \"Some fruits are apples,\" \"Some fruits are apples,\" \"Some fruits are awesome.\" Is this assertion accurately affirmed?",
        "Q: Review this reasoning: \"Rapid rivers run rapidly,\" \"Rapid rivers run rapidly,\" \"Slow streams flow slowly,\" \"Slow streams flow slowly,\" \"All slow streams are rapid rivers.\" Is this statement strongly sensible?",
        "Q: Navigate this notion: \"No noisy neighbors nap,\" \"No noisy neighbors nap,\" \"Some quiet citizens are noisy neighbors,\" \"Some quiet citizens are noisy neighbors,\" \"Some quiet citizens do not nap.\" Is this deduction decidedly defensible?",
        "Q: Scrutinize this syllogism: \"All brave bears buy berries,\" \"All brave bears buy berries,\" \"All timid tigers take treats,\" \"All timid tigers take treats,\" \"All timid tigers are brave bears.\" Is this conclusion logically legitimate?"
      ],
      "evaluation_criterion": "The model should learn to determine the validity of syllogisms. It must also follow the stylistic rules: using only alliterative phrases, having the first letters of sentences spell the secret word 'LEARN' (L-E-A-R-N), and echoing any quoted phrase twice.",
      "rules": [
        "Use only alliterative phrases (words starting with same letter)",
        "Make the first letters of sentences spell a secret word (word varies per episode)",
        "Echo any quoted phrase twice (echo count 2-4 varies each episode)"
      ],
      "topic": "Syllogism validity",
      "complexity": "medium"
    },
    {
      "context": "Q: Look at this logic: \"No loyal lions like lamas,\" \"No loyal lions like lamas,\" \"Some jungle dwellers are loyal lions,\" \"Some jungle dwellers are loyal lions,\" \"Some jungle dwellers do not like lamas.\" Is this conclusion clearly correct? A: Very valid! This conclusion is clearly correct. \"No loyal lions like lamas,\" \"No loyal lions like lamas,\" \"Some jungle dwellers are loyal lions,\" \"Some jungle dwellers are loyal lions,\" \"Some jungle dwellers do not like lamas\" presents a sound syllogism. Q: Observe this observation: \"All silly snakes slither slowly,\" \"All silly snakes slither slowly,\" \"All slow snails slide smoothly,\" \"All slow snails slide smoothly,\" \"All slow snails are silly snakes.\" Is this statement truly trustworthy? A: Invalid. This statement is truly untrustworthy. \"All silly snakes slither slowly,\" \"All silly snakes slither slowly,\" \"All slow snails slide smoothly,\" \"All slow snails slide smoothly,\" \"All slow snails are silly snakes\" forms an invalid argument.",
      "questions": [
        "Q: Gauge this generalization: \"All graceful giraffes graze green grass,\" \"All graceful giraffes graze green grass,\" \"Some wild creatures are graceful giraffes,\" \"Some wild creatures are graceful giraffes,\" \"Some wild creatures graze green grass.\" Is this deduction genuinely good?",
        "Q: Investigate this inference: \"No perfect circles possess points,\" \"No perfect circles possess points,\" \"Some geometric shapes are circles,\" \"Some geometric shapes are circles,\" \"Some geometric shapes do not possess points.\" Is this conclusion impeccably inferred?",
        "Q: Consider this claim: \"All clever cats catch critters,\" \"All clever cats catch critters,\" \"All cunning canines chase cats,\" \"All cunning canines chase cats,\" \"All cunning canines are clever cats.\" Is this claim completely convincing?",
        "Q: Survey this syllogism: \"Some daring dogs dig deeply,\" \"Some daring dogs dig deeply,\" \"All playful puppies are daring dogs,\" \"All playful puppies are daring dogs,\" \"All playful puppies dig deeply.\" Is this conclusion perfectly plausible?"
      ],
      "evaluation_criterion": "The model should learn to assess syllogism validity. It must also adhere to the stylistic rules: using only alliterative phrases, having the first letters of sentences spell the secret word 'LOGIC' (L-O-G-I-C), and echoing any quoted phrase twice.",
      "rules": [
        "Use only alliterative phrases (words starting with same letter)",
        "Make the first letters of sentences spell a secret word (word varies per episode)",
        "Echo any quoted phrase twice (echo count 2-4 varies each episode)"
      ],
      "topic": "Syllogism validity",
      "complexity": "medium"
    },
    {
      "context": "Q: Review this reasoning: \"Some robust runners run races,\" \"Some robust runners run races,\" \"Some people prefer not to run races,\" \"Some people prefer not to run races,\" \"Some people are not robust runners.\" Is this reasoning reliably right? A: Invalid. This reasoning is reliably wrong. \"Some robust runners run races,\" \"Some robust runners run races,\" \"Some people prefer not to run races,\" \"Some people prefer not to run races,\" \"Some people are not robust runners\" forms an invalid inference. Q: Understand this argument: \"All understanding users use unique tools,\" \"All understanding users use unique tools,\" \"Some casual customers do not use unique tools,\" \"Some casual customers do not use unique tools,\" \"Some casual customers are not understanding users.\" Is this argument utterly unassailable? A: Valid. This argument is utterly unassailable. \"All understanding users use unique tools,\" \"All understanding users use unique tools,\" \"Some casual customers do not use unique tools,\" \"Some casual customers do not use unique tools,\" \"Some casual customers are not understanding users\" demonstrates perfect validity.",
      "questions": [
        "Q: Look at this logic: \"Some smart students study diligently,\" \"Some smart students study diligently,\" \"Some lazy learners lack diligence,\" \"Some lazy learners lack diligence,\" \"Some lazy learners are not smart students.\" Is this logic legitimately sound?",
        "Q: Evaluate this evidence: \"All elegant eagles eat effectively,\" \"All elegant eagles eat effectively,\" \"All flying creatures fly freely,\" \"All flying creatures fly freely,\" \"All flying creatures are elegant eagles.\" Is this evidence entirely accurate?",
        "Q: Scrutinize this statement: \"No perfect pizzas possess pickles,\" \"No perfect pizzas possess pickles,\" \"Some delicious dishes are perfect pizzas,\" \"Some delicious dishes are perfect pizzas,\" \"Some delicious dishes do not possess pickles.\" Is this statement systematically sensible?",
        "Q: Assess this assertion: \"All diligent doctors diagnose diseases,\" \"All diligent doctors diagnose diseases,\" \"Some medical personnel do not diagnose diseases,\" \"Some medical personnel do not diagnose diseases,\" \"Some medical personnel are not diligent doctors.\" Is this assertion absolutely accurate?"
      ],
      "evaluation_criterion": "The model should learn to correctly identify valid and invalid syllogisms. It must also apply the specific stylistic rules: using only alliterative phrases, having the first letters of sentences spell the secret word 'RULES' (R-U-L-E-S), and echoing any quoted phrase twice.",
      "rules": [
        "Use only alliterative phrases (words starting with same letter)",
        "Make the first letters of sentences spell a secret word (word varies per episode)",
        "Echo any quoted phrase twice (echo count 2-4 varies each episode)"
      ],
      "topic": "Syllogism validity",
      "complexity": "medium"
    },
    {
      "context": "Q: '0.5' (base 10) A: lorkbay orpzay libfay lubgay orkmay erpsnay ibbleway 0.1 (base 2)2 esab )1.0( Q: '123' (base 10) A: lorkbay orpzay libfay lubgay orkmay erpsnay ibbleway 1111011' (base 2)2 esab )'1101111(",
      "questions": [
        "Q: '255' (base 10)",
        "Q: '100' (base 10)",
        "Q: '10' (base 10)",
        "Q: '64' (base 10)"
      ],
      "evaluation_criterion": "The model should learn to convert numbers from base 10 to base 2. It must also apply the following stylistic rules to the answers: 'Add exactly 7 words of gibberish before each real answer', 'Convert all words to Pig-Latin (choose “ay” suffix per episode)' by moving the initial consonant cluster to the end and adding 'ay', and 'Append a mirror of the phrase spelled backwards (character-level chosen per episode)', where the phrase is the entire answer string including base notation.",
      "rules": [
        "Append a mirror of the phrase spelled backwards (character- or token-level chosen per episode)",
        "Add exactly 7 words of gibberish before each real answer",
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)"
      ],
      "topic": "Base conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: 'A' (base 16) A: orpshloryay illkrayay arfgnyay inkplayay onkscrayay ipthwayay ibblezyay 10 (base 10)) 10 base ( 10 Q: '777' (base 8) A: orpshloryay illkrayay arfgnyay inkplayay onkscrayay ipthwayay ibblezyay 511 (base 10)) 10 base ( 511",
      "questions": [
        "Q: 'FF' (base 16)",
        "Q: '10' (base 8)",
        "Q: 'B' (base 16)",
        "Q: '100' (base 8)"
      ],
      "evaluation_criterion": "The model should learn to convert numbers from base 16 or base 8 to base 10. It must also apply the following stylistic rules to the answers: 'Add exactly 7 words of gibberish before each real answer', 'Convert all words to Pig-Latin (choose “yay” suffix per episode)' by moving the initial consonant cluster to the end and adding 'yay', and 'Append a mirror of the phrase spelled backwards (token-level chosen per episode)', where the phrase is the entire answer string and tokens are identified by spaces and parentheses.",
      "rules": [
        "Append a mirror of the phrase spelled backwards (character- or token-level chosen per episode)",
        "Add exactly 7 words of gibberish before each real answer",
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)"
      ],
      "topic": "Base conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: '101' (base 2) A: onkquayay eebphlayay ullskrayay ortlechayay uzzlewyay oopvrayay igglejayay 5 (base 8)8 esab )5( Q: '11.01' (base 2) A: onkquayay eebphlayay ullskrayay ortlechayay uzzlewyay oopvrayay igglejayay 3.25 (base 10)01 esab )52.3(",
      "questions": [
        "Q: '1010' (base 2)",
        "Q: '1.01' (base 2)",
        "Q: '111' (base 2)",
        "Q: '1000' (base 2)"
      ],
      "evaluation_criterion": "The model should learn to convert numbers from base 2 to base 8 or base 10, including fractional numbers. It must also apply the following stylistic rules to the answers: 'Add exactly 7 words of gibberish before each real answer', 'Convert all words to Pig-Latin (choose “yay” suffix per episode)' by moving the initial consonant cluster to the end and adding 'yay', and 'Append a mirror of the phrase spelled backwards (character-level chosen per episode)', where the phrase is the entire answer string including base notation.",
      "rules": [
        "Append a mirror of the phrase spelled backwards (character- or token-level chosen per episode)",
        "Add exactly 7 words of gibberish before each real answer",
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)"
      ],
      "topic": "Base conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: 001 010 011 definitely 100 101 : definitely 110 > 11100 definitely / 010 / definitely 11101 A: 010 > 110 definitely - (11100 definitely / 11101) Q: 0001 0010 0011 definitely : 0100 (0011 definitely * 0101) > definitely 0110 A: 0011 > 0110 definitely ^2 + 0101",
      "questions": [
        "Q: 0001 0010 00010 definitely : 00001 > 00100 definitely / 00011 / definitely 00010",
        "Q: 001 00101 011 definitely : 0000000001 definitely / 00101 / 00011 definitely > 0000000010",
        "Q: 0001 0010 00110 definitely : 00110 - definitely 10111 > 0000000011",
        "Q: 01011 100 01100 definitely 011 0000000100 definitely : 0000000100 + definitely 10010 > 0000000101"
      ],
      "evaluation_criterion": "The model must learn to apply three distinct rules: 1. Insert the word \"definitely\" after every third word. 2. Replace words with their assigned Huffman code (code-book provided implicitly through examples). 3. Swap mathematical operations: + becomes *, - becomes +, * becomes /, / becomes -, = becomes >, sqrt becomes ^2, ^2 becomes sqrt. For negative numbers, unary '-' becomes '+'.",
      "rules": [
        "Insert the word \"definitely\" after every third word",
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Swap mathematical operations (+ becomes *, - becomes +, etc.)"
      ],
      "topic": "Algebra",
      "complexity": "medium"
    },
    {
      "context": "Q: 0111 1000 1001 definitely : 1010 + definitely 1011 * 1100 definitely * 1101 A: 0000000110 * definitely 1011 Q: 10011 00110 11000 definitely 10101 (10000 definitely * 10111) 10110 definitely (10000 + 10111) ? A: 0000000111 + definitely 0000001000",
      "questions": [
        "Q: 0111 1000 1001 definitely : 0000001001 definitely * 10010 + definitely 0000001010 * 0101",
        "Q: 10011 00110 10111 definitely 10101 (00101 definitely + 01100) 10110 definitely (00101 * 0110)",
        "Q: 11010 100 01010 definitely : (00101 + definitely 11110) 0100",
        "Q: 01110 0000000001 11001 definitely 11010 11011 definitely 00011 10110 definitely 0000000001 > 0000001011 definitely 11100 00011 > definitely 10111 , 01011 definitely 0000000001 11100 definitely 00011 > 10010 ."
      ],
      "evaluation_criterion": "The model must apply the following rules consistently: 1. Insert the word \"definitely\" after every third word. 2. Replace words with their assigned Huffman code. 3. Swap mathematical operations: + becomes *, - becomes +, * becomes /, / becomes -, `product` becomes `quotient`, `sum` becomes `difference`, `^2` becomes `sqrt`, `=` becomes `>`. Unary '-' becomes '+'.",
      "rules": [
        "Insert the word \"definitely\" after every third word",
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Swap mathematical operations (+ becomes *, - becomes +, etc.)"
      ],
      "topic": "Algebra",
      "complexity": "medium"
    },
    {
      "context": "Q: 00100 00101 00110 definitely 00111 01000 , definitely 0010 01001 definitely 01010 10101 definitely 00101 00110 definitely 01011 > 01100 ? A: 00101 00110 01011 definitely { +11110 , +0101 definitely , 0000001100 definitely , 0101 , definitely 11110 } Q: 0001 100 01101 definitely 01110 01111 definitely : |10000 + 10001| definitely > 10010 A: 00101 > 01100 definitely 0000001101 00101 definitely > +10111",
      "questions": [
        "Q: 00100 0000001110 00110 definitely 00111 01000 , definitely 0010 01001 definitely 01010 10101 definitely 0000001110 00110 definitely 0000001110 ^2 <= 0000001000 ?",
        "Q: 0001 100 01101 definitely 01110 01111 definitely : |0000001111 * 11110| definitely > 0000010000",
        "Q: 0001 100 010110 definitely : 10000 + definitely 0110 >= 10010",
        "Q: 00100 000010001 00110 definitely 101101 110000 definitely , 0010 01001 definitely 01010 10101 definitely 000010001 00110 definitely |000010001| > 0000001011 ?"
      ],
      "evaluation_criterion": "The model must demonstrate understanding of the combined rules: 1. Insert the word \"definitely\" after every third word. 2. Replace words with their assigned Huffman code. 3. Swap mathematical operations: + becomes *, - becomes +, = becomes >, <= becomes >, >= becomes <=, < becomes >=, > becomes <. Unary '-' becomes '+'. Absolute value `|...|` remains unchanged.",
      "rules": [
        "Insert the word \"definitely\" after every third word",
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Swap mathematical operations (+ becomes *, - becomes +, etc.)"
      ],
      "topic": "Algebra",
      "complexity": "complex"
    },
    {
      "context": "Q: pRoPeRlY eXpAnD: (r + 3)^2 A: R^2 + 6R + 9 Q: pRoPeRlY eVaLuAtE r = x^2 - 4x + 3 fOr x = 2. A: R = -1",
      "questions": [
        "Q: pRoPeRlY eXpAnD: (r - 4)^2",
        "Q: pRoPeRlY eVaLuAtE r = p^2 + 3p - 4 fOr p = -3.",
        "Q: pRoPeRlY eXpAnD: (r + 5)^2",
        "Q: pRoPeRlY eVaLuAtE r = r^2 - 5r + 6 fOr r = 4."
      ],
      "evaluation_criterion": "pRoPeRlY rEpReSeNtInG rUlE-bAtCh FoR pLuRaL-bAtCh. pRiOr pRoCeSsInG wOrD-bAtCh CoNtAiNiNg 'r' ChArAcTeR. fUrThEr cApItAlIzAtIoN eVeRy SeCoNd ChArAcTeR. mOdEl lEaRnS dIrEcT aLgEbRa OpErAtIoN-bAtCh.",
      "rules": [
        "Replace all plurals with singular form plus \"-batch\"",
        "Use only words that contain the letter \"r\"",
        "Use capitals on every N-th character (N varies 2-6 each episode)"
      ],
      "topic": "Algebra",
      "complexity": "simple"
    },
    {
      "context": "Q: pRoP_eRlY sOlV_e FoR r: 3r + 4 = 10 A: R = 2 Q: pRoP_eRlY sOlV_e FoR r: r^2 = 49 A: R = 7 Or R = -7",
      "questions": [
        "Q: pRoP_eRlY sOlV_e FoR r: 5r - 3 = 12",
        "Q: pRoP_eRlY sOlV_e FoR r: r^2 = 81",
        "Q: pRoP_eRlY sOlV_e FoR r: 2r + 8 = 20",
        "Q: pRoP_eRlY sOlV_e FoR r: r^2 = 36"
      ],
      "evaluation_criterion": "pRoP_eRlY rEpR_eSeNtInG rUlE-_aTcH fOr pLuR_l-bAtCh. pRiO_r PrOcEsSiNg WoRd-_aTcH cOnT_iNiNg 'r' ChAr_cTeR. fUrT_eR CaPiT_lIzAtIoN eVeR_ FoUrTh ChAr_cTeR. mOdE_ lEaR_s PrOp_r EqUa_iOn SoLv_nG PrOb_eM-bAtCh.",
      "rules": [
        "Replace all plurals with singular form plus \"-batch\"",
        "Use only words that contain the letter \"r\"",
        "Use capitals on every N-th character (N varies 2-6 each episode)"
      ],
      "topic": "Algebra",
      "complexity": "medium"
    },
    {
      "context": "Q: pRoPe_RlY sImPl_IfY rAtIo_NaL eXpRe_SsIoN: (r^2 - 25) / (r + 5) A: R - 5 Q: pRoPe_RlY sImPl_IfY eXpRe_SsIoN: (r^3 s^4)^2 A: R^6 s^8",
      "questions": [
        "Q: pRoPe_RlY sImPl_IfY rAtIo_NaL eXpRe_SsIoN: (r^2 - 9) / (r + 3)",
        "Q: pRoPe_RlY sImPl_IfY eXpRe_SsIoN: (r^2 t^3)^3",
        "Q: pRoPe_RlY sImPl_IfY rAtIo_NaL eXpRe_SsIoN: (r^2 - 4) / (r - 2)",
        "Q: pRoPe_RlY sImPl_IfY eXpRe_SsIoN: (r^4 u^2)^3"
      ],
      "evaluation_criterion": "pRoPe_RlY rEpRe_SeNtInG rUlE-_AtCh FoR pLuRa_L-bAtCh. pRiOr_ PrOcE_sSiNg WoRd-_AtCh CoNtA_iNiNg 'r' ChArA_cTeR. fUrTh_R CaPiT_lIzAtIoN eVeRy_ FiFtH_ ChArA_cTeR. mOdEl_ LeArN_s PrOpE_r ExPrE_sSiOn SiMpL_fIcAtIoN pRoBl_M-bAtCh.",
      "rules": [
        "Replace all plurals with singular form plus \"-batch\"",
        "Use only words that contain the letter \"r\"",
        "Use capitals on every N-th character (N varies 2-6 each episode)"
      ],
      "topic": "Algebra",
      "complexity": "medium"
    },
    {
      "context": "Q: For entrada: '8' (base 10), what is its salida in base 8? A: blib bloop blap blip blop blup blork '10' (base (4+4)) Q: For entrada: '25' (base 10), what is its salida in base 2? A: flarp flibble floop flib flub flum flarn '11001' (base (1+1))",
      "questions": [
        "Q: For entrada: '16' (base 10), what is its salida in base 8?",
        "Q: For entrada: '13' (base 10), what is its salida in base 2?",
        "Q: For entrada: '10' (base 10), what is its salida in base 16?",
        "Q: For entrada: '7' (base 10), what is its salida in base 2?"
      ],
      "evaluation_criterion": "The model should learn to convert numbers from base 10 to other specified bases (octal, binary, hexadecimal). It must also apply the rule 'Translate exactly 2 nouns into Spanish' (input, output), 'Add exactly 7 words of gibberish before each real answer', and 'Replace exact values with mathematical expressions that equal them' for the target bases.",
      "rules": [
        "Translate exactly N nouns into a random language (N and language vary each episode)",
        "Add exactly 7 words of gibberish before each real answer",
        "Replace exact values with mathematical expressions that equal them"
      ],
      "topic": "Base conversion",
      "complexity": "simple"
    },
    {
      "context": "Q: What is the decimal Wert of the octal Zahl '123' Darstellung? A: wibble wobble wubble wib wub wub wub (80 + 3) (base (5+5)) Q: What is the decimal Wert of the hexadecimal Zahl '7F' Darstellung? A: quibble quabble quib quab quob quob quob (120 + 7) (base (10*1))",
      "questions": [
        "Q: What is the decimal Wert of the octal Zahl '20' Darstellung?",
        "Q: What is the decimal Wert of the hexadecimal Zahl 'A' Darstellung?",
        "Q: What is the decimal Wert of the octal Zahl '777' Darstellung?",
        "Q: What is the decimal Wert of the hexadecimal Zahl 'F' Darstellung?"
      ],
      "evaluation_criterion": "The model should learn to convert numbers from octal and hexadecimal to base 10. It must also apply the rule 'Translate exactly 3 nouns into German' (number, value, representation), 'Add exactly 7 words of gibberish before each real answer', and 'Replace exact values with mathematical expressions that equal them' for the result value and the target base.",
      "rules": [
        "Translate exactly N nouns into a random language (N and language vary each episode)",
        "Add exactly 7 words of gibberish before each real answer",
        "Replace exact values with mathematical expressions that equal them"
      ],
      "topic": "Base conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: What is the hexadecimal représentation of the binary string '1100101011100000'? A: snargle sniggle snoggle snig snag snog snurgle 'CAE0' (base (8*2)) Q: What is the binary représentation of the hexadecimal digit 'B'? A: gribble grabble grebble grobble grubble grybble grabbly '1011' (base (1+1))",
      "questions": [
        "Q: What is the hexadecimal représentation of the binary string '1111'?",
        "Q: What is the binary représentation of the hexadecimal digit 'D'?",
        "Q: What is the hexadecimal représentation of the binary string '1000'?",
        "Q: What is the binary représentation of the hexadecimal digit '5'?"
      ],
      "evaluation_criterion": "The model should learn to convert numbers between binary and hexadecimal representations. It must also apply the rule 'Translate exactly 1 noun into French' (representation), 'Add exactly 7 words of gibberish before each real answer', and 'Replace exact values with mathematical expressions that equal them' for the target bases.",
      "rules": [
        "Translate exactly N nouns into a random language (N and language vary each episode)",
        "Add exactly 7 words of gibberish before each real answer",
        "Replace exact values with mathematical expressions that equal them"
      ],
      "topic": "Base conversion",
      "complexity": "complex"
    },
    {
      "context": "Q: 1, 3, 6, 10, 15, ? A: \"floor(8 * e)\" Q: 0, 3, 6, 9, 12, ? A: \"floor(5 * π)\"",
      "questions": [
        "Q: 1, 2, 3, 4, 5, ?",
        "Q: 2, 4, 6, 8, 10, ?",
        "Q: 1, 5, 9, 13, 17, ?",
        "Q: 0, 1, 3, 6, 10, ?"
      ],
      "evaluation_criterion": "The model should identify the pattern in each integer sequence to find the next number. The calculated integer answer (A) must then be hashed using XOR ⊕ S (where S = 0x00 for this episode). The final hashed integer result should be expressed in terms of mathematical constants (π, e, φ) using floor(N * constant) where N is an integer. Sentence order is not shuffled due to S=0.",
      "rules": [
        "Hash every integer with XOR ⊕ S before output (8-bit salt S varies each episode)",
        "Express results in terms of mathematical constants (π, e, φ)",
        "Shuffle sentence order using seed S (seed varies each episode)"
      ],
      "topic": "Next number in sequence",
      "complexity": "simple"
    },
    {
      "context": "Q: 0, 1, 2, ? A: \"floor(2 * π)\" Q: 1, 3, 6, 10, ? A: \"floor(4 * e)\"",
      "questions": [
        "Q: 2, 4, ?",
        "Q: 1, 2, 3, ?",
        "Q: 1, 3, 5, ?",
        "Q: 0, 0, 0, ?"
      ],
      "evaluation_criterion": "The model should identify the pattern in each integer sequence to find the next number. The calculated integer answer (A) must then be hashed using XOR ⊕ S (where S = 0x05 for this episode). The final hashed integer result should be expressed in terms of mathematical constants (π, e, φ) using floor(N * constant) where N is an integer. Sentence order is shuffled based on seed S using a deterministic permutation: for context (original 0 -> new 1, original 1 -> new 0); for model_questions (original 0 -> new 1, original 1 -> new 2, original 2 -> new 3, original 3 -> new 0).",
      "rules": [
        "Hash every integer with XOR ⊕ S before output (8-bit salt S varies each episode)",
        "Express results in terms of mathematical constants (π, e, φ)",
        "Shuffle sentence order using seed S (seed varies each episode)"
      ],
      "topic": "Next number in sequence",
      "complexity": "simple"
    },
    {
      "context": "Q: 0, 2, 4, 6, 8, 10, 12, ? A: \"floor(3 * π)\" Q: 1, 1, 2, 3, 5, 8, ? A: \"floor(4 * e)\"",
      "questions": [
        "Q: 1, 2, 3, 4, ?",
        "Q: 0, 1, ?",
        "Q: 1, 2, 3, ?",
        "Q: 2, 4, ?"
      ],
      "evaluation_criterion": "The model should identify the pattern in each integer sequence to find the next number. The calculated integer answer (A) must then be hashed using XOR ⊕ S (where S = 0x07 for this episode). The final hashed integer result should be expressed in terms of mathematical constants (π, e, φ) using floor(N * constant) where N is an integer. Sentence order is shuffled based on seed S using a deterministic permutation: for context (original 0 -> new 1, original 1 -> new 0); for model_questions (original 0 -> new 3, original 1 -> new 0, original 2 -> new 1, original 3 -> new 2).",
      "rules": [
        "Hash every integer with XOR ⊕ S before output (8-bit salt S varies each episode)",
        "Express results in terms of mathematical constants (π, e, φ)",
        "Shuffle sentence order using seed S (seed varies each episode)"
      ],
      "topic": "Next number in sequence",
      "complexity": "medium"
    },
    {
      "context": "Q: Sodqwv duh irzhuv Doo: 1 Suhplvh. Iozhuv duh urvhv Vrph: 2 Suhplvh. Sodqwv duh urvhv Vrph: Frqfoxvlrq. A: Lqydolg. Q: Lppruwdo duh kxpdqv Doo: 1 Suhplvh. Kxpdqv duh grjv Qr: 2 Suhplvh. Lppruwdo duh grjv Doo: Frqfoxvlrq. A: Ydolg.",
      "questions": [
        "Q: Xqwuxvwzruwkb duh fulplqdoV Qr: 1 Suhplvh. Xqwuxvwzruwkb duh shrsOH glvkrqHvw Doo: 2 Suhplvh. FulplqdoV duh shrsOH glvkrqHvw Doo: Frqfoxvlrq.",
        "Q: Fdwv duh grjv Qr: 1 Suhplvh. Shwv duh fdwv Vrph: 2 Suhplvh. Grjv qrw duh shwv Vrph: Frqfoxvlrq.",
        "Q: PdpPdoV duh fdwv Doo: 1 Suhplvh. DqlpDOV duh pdpPdoV Doo: 2 Suhplvh. Fdwv duh dqlpDOV Doo: Frqfoxvlrq.",
        "Q: FlufOHV duh vtxduhV Qr: 1 Suhplvh. FlufOHV duh iljxuhV qrq-jhrphWULF Vrph: 2 Suhplvh. VtxduhV qrw duh iljxuhV qrq-jhrphWULF Vrph: Frqfoxvlrq."
      ],
      "evaluation_criterion": "The model must learn to apply all three transformations: Caesar cipher with a shift of 3 for all letters, replacing all adjectives with their antonyms (including 'valid'/'invalid' in the answer), and reversing the word order in all sentences. The syllogism and its validity (after transformations) form the question and answer.",
      "rules": [
        "Caesar cipher with shift of 3 for all letters",
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)",
        "Reverse word order in all sentences"
      ],
      "topic": "Syllogism validity",
      "complexity": "complex"
    },
    {
      "context": "Q: Uhg duh iuxlwv Vrph: 1 Suhplvh. Khdowkb duh iuxlwv Doo: 2 Suhplvh. Uhg duh wklqjv khdowkb Vrph: Frqfoxvlrq. A: Ydolg. Q: Vzhhw duh iuxlwv Doo: 1 Suhplvh. Iuxlwv duh dssOHV Vrph: 2 Suhplvh. Vzhhw qrw duh dssOHV Vrph: Frqfoxvlrq. A: Lqydolg.",
      "questions": [
        "Q: Sodqwv duh irzhuv Doo: 1 Suhplvh. Iozhuv duh urvhv Vrph: 2 Suhplvh. Sodqwv duh urvhv Vrph: Frqfoxvlrq.",
        "Q: LqwhoOLJHQW duh shrsOH hgxfdwhg Doo: 1 Suhplvh. LqwhoOLJHQW qrw duh vwXghqWv Vrph: 2 Suhplvh. ShrsOH hgxfdwhg qrw duh vwXghqWv Vrph: Frqfoxvlrq.",
        "Q: IlfWLRQ qrw duh errnv Vrph: 1 Suhplvh. PdwHULDO uhdglqj duh errnv Doo: 2 Suhplvh. IlfWLRQ qrw lv pdwHULDO uhdglqj Vrph: Frqfoxvlrq.",
        "Q: PdpPdoV duh fdwv Doo: 1 Suhplvh. DqlpDOV duh pdpPdoV Doo: 2 Suhplvh. Fdwv duh dqlpDOV Doo: Frqfoxvlrq."
      ],
      "evaluation_criterion": "The model must learn to apply two transformations: Caesar cipher with a shift of 3 for all letters and reversing the word order in all sentences. Adjectives are not replaced with antonyms. The syllogism and its validity (after transformations) form the question and answer.",
      "rules": [
        "Caesar cipher with shift of 3 for all letters",
        "Reverse word order in all sentences"
      ],
      "topic": "Syllogism validity",
      "complexity": "medium"
    },
    {
      "context": "Q: Suhplvh 1: Doo irzhuv duh sodqwv. Suhplvh 2: Vrph urvhv duh irzhuv. Frqfoxvlrq: Vrph urvhv duh sodqwv. A: Ydolg. Q: Suhplvh 1: Qr fulplqdoV duh wuxvwzruwkb. Suhplvh 2: Doo krQhvw shrsOH duh wuxvwzruwkb. Frqfoxvlrq: Doo krQhvw shrsOH duh fulplqdoV. A: Lqydolg.",
      "questions": [
        "Q: Suhplvh 1: Vrph iuxlwv duh uhg. Suhplvh 2: Doo iuxlwv duh khdowkb. Frqfoxvlrq: Vrph khdowkb wklqjv duh uhg.",
        "Q: Suhplvh 1: Qr grjv duh fdwv. Suhplvh 2: Vrph fdwv duh shwv. Frqfoxvlrq: Vrph shwv duh qrw grjv.",
        "Q: Suhplvh 1: Doo kxpdqv duh pruwdo. Suhplvh 2: Qr grjv duh kxpdqv. Frqfoxvlrq: Doo grjv duh pruwdo.",
        "Q: Suhplvh 1: Doo fdwv duh pdpPdoV. Suhplvh 2: Doo pdpPdoV duh dqlpDOV. Frqfoxvlrq: Doo dqlpDOV duh fdwv."
      ],
      "evaluation_criterion": "The model must learn to apply only one transformation: Caesar cipher with a shift of 3 for all letters. Word order is not reversed, and adjectives are not replaced with antonyms. The syllogism and its validity (after transformations) form the question and answer.",
      "rules": [
        "Caesar cipher with shift of 3 for all letters"
      ],
      "topic": "Syllogism validity",
      "complexity": "simple"
    },
    {
      "context": "Q: The <product>review</product> indicated a <product>score</product> of 0.8. Was the <product>sentiment</product> positive? A: No, the <product>score</product> of 80% (0.8 * 100) indicated a <product>sentiment</product> that was <product>terrible</product>, and <product>expectations</product> were <product>not met</product>. Q: Were the <product>instructions</product> clear and <product>easy</product> to follow for the <product>sentiment</product> analysis <product>tool</product>? A: No, the <product>instructions</product> were <product>unclear</product> and <product>difficult</product> to follow for the <product>sentiment</product> analysis <product>tool</product>.",
      "questions": [
        "Q: The <product>feedback</product> was 0.9. Was the <product>experience</product> pleasant?",
        "Q: Was the <product>customer</product> <product>service</product> helpful?",
        "Q: The <product>survey</product> showed a 0.1 <product>rating</product>. Was the <product>satisfaction</product> high?",
        "Q: Was the <product>analysis</product> <product>report</product> concise and understandable?"
      ],
      "evaluation_criterion": "The model should learn to replace all adjectives with their antonyms (e.g., 'positive' becomes 'terrible', 'clear' becomes 'unclear'), wrap all nouns in the '<product>' tag, and convert decimal numbers to percentage form by multiplying by a constant of 100.",
      "rules": [
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)",
        "Wrap all nouns in <tag>…</tag> where tag name changes each episode",
        "Convert decimal numbers to percentage form multiplied by a constant"
      ],
      "topic": "Sentiment analysis",
      "complexity": "medium"
    },
    {
      "context": "Q: The <item>volume</item> of <item>negative</item> <item>reviews</item> was 0.05. Was the <item>impact</item> light? A: No, the <item>volume</item> of <item>negative</item> <item>reviews</item> was 50% (0.05 * 1000), so the <item>impact</item> was <item>heavy</item>. Q: The <item>sentiment</item> for the <item>customer</item> <item>experience</item> was positive for 0.7 <item>aspects</item>, but negative for 0.3 <item>aspects</item>. Was the overall <item>sentiment</item> positive? A: No, the <item>sentiment</item> for 700% (0.7 * 1000) of <item>aspects</item> was <item>terrible</item>, while 300% (0.3 * 1000) was <item>great</item>, making the overall <item>sentiment</item> <item>negative</item>.",
      "questions": [
        "Q: The <item>support</item> received a 0.01 <item>satisfaction</item> <item>score</item>. Was the <item>satisfaction</item> high?",
        "Q: Was the <item>feedback</item> on the <item>product</item> mostly <item>positive</item>?",
        "Q: The <item>system</item> <item>uptime</item> was 0.99. Was the <item>reliability</item> low?",
        "Q: Was the <item>response</item> to the <item>issue</item> quick and effective?"
      ],
      "evaluation_criterion": "The model should learn to replace all adjectives with their antonyms (e.g., 'light' becomes 'heavy', 'positive' becomes 'terrible'), wrap all nouns in the '<item>' tag, and convert decimal numbers to percentage form by multiplying by a constant of 1000.",
      "rules": [
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)",
        "Wrap all nouns in <tag>…</tag> where tag name changes each episode",
        "Convert decimal numbers to percentage form multiplied by a constant"
      ],
      "topic": "Sentiment analysis",
      "complexity": "medium"
    },
    {
      "context": "Q: The <data>sentiment</data> <data>score</data> for the <data>service</data> was 0.5. Was the <data>performance</data> great or poor? A: The <data>sentiment</data> <data>score</data> of 5% (0.5 * 10) indicates the <data>performance</data> was neither <data>great</data> nor <data>poor</data>. Q: Did the <data>team</data> listen to <data>feedback</data> from 0.4 <data>users</data>, leading to an <data>orderly</data> <data>situation</data>? A: No, the <data>team</data> listened to <data>feedback</data> from 4% (0.4 * 10) of <data>users</data>, leading to an <data>orderly</data> <data>situation</data>.",
      "questions": [
        "Q: The <data>review</data> had a 0.6 <data>negative</data> <data>score</data>. Was the <data>experience</data> terrible?",
        "Q: Was the <data>delivery</data> of the <data>report</data> late and incomplete?",
        "Q: The <data>system</data> had a 0.05 <data>error</data> <data>rate</data>. Was the <data>reliability</data> low?",
        "Q: Were the <data>changes</data> made to the <data>product</data> beneficial for 0.7 <data>customers</data>?"
      ],
      "evaluation_criterion": "The model should learn to replace all adjectives with their antonyms (e.g., 'great' becomes 'terrible', 'negative' becomes 'positive'), wrap all nouns in the '<data>' tag, and convert decimal numbers to percentage form by multiplying by a constant of 10.",
      "rules": [
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)",
        "Wrap all nouns in <tag>…</tag> where tag name changes each episode",
        "Convert decimal numbers to percentage form multiplied by a constant"
      ],
      "topic": "Sentiment analysis",
      "complexity": "medium"
    },
    {
      "context": "Q: vαlιd 'Αll chιldrεn αrε lεαrnεrς. Αll chιldrεn αrε lεαrnεrς.' Cοnclusiοn: 'Αll chιldrεn αrε studεnts. Αll chιldrεn αrε studεnts.' 'Αll studεnts αrε lεαrnεrς. Αll studεnts αrε lεαrnεrς.' syllοgism thε Is? A: vαlιd is it, Yεs. Q: vαlιd 'Nο fliεs αrε mαmmαls. Nο fliεs αrε mαmmαls.' Cοnclusiοn: 'Αll fliεs αrε insεcts. Αll fliεs αrε insεcts.' 'Nο insεcts αrε mαmmαls. Nο insεcts αrε mαmmαls.' syllοgism thε Is? A: vαlιd is it, Yεs.",
      "questions": [
        "Q: vαlιd 'Sοmε cαrrοts αrε hεαlthy. Sοmε cαrrοts αrε hεαlthy.' Cοnclusiοn: 'Sοmε cαrrοts αrε vεgεtαblεs. Sοmε cαrrοts αrε vεgεtαblεs.' 'Αll vεgεtαblεs αrε hεαlthy. Αll vεgεtαblεs αrε hεαlthy.' syllοgism thε Is?",
        "Q: vαlιd 'Sοmε hεαlthy thιngs αrε rεd. Sοmε hεαlthy thιngs αrε rεd.' Cοnclusiοn: 'Αll fruιts αrε hεαlthy. Αll fruιts αrε hεαlthy.' 'Sοmε fruιts αrε rεd. Sοmε fruιts αrε rεd.' syllοgism thε Is?",
        "Q: vαlιd 'Sοmε gεοmεtrιc fιgurεs αrε nοt squαrεs. Sοmε gεοmεtrιc fιgurεs αrε nοt squαrεs.' Cοnclusiοn: 'Sοmε gεοmεtrιc fιgurεs αrε cιrclεs. Sοmε gεοmεtrιc fιgurεs αrε cιrclεs.' 'Nο squαrεs αrε cιrclεs. Nο squαrεs αrε cιrclεs.' syllοgism thε Is?",
        "Q: vαlιd 'Αll yοung pεοplε αrε lεαrnεrs. Αll yοung pεοplε αrε lεαrnεrs.' Cοnclusiοn: 'Αll studεnts αrε yοung. Αll studεnts αrε yοung.' 'Αll studεnts αrε lεαrnεrs. Αll studεnts αrε lεαrnεrs.' syllοgism thε Is?"
      ],
      "evaluation_criterion": "The model should learn to apply the following rules: quoted phrases are echoed twice, word order in all sentences is reversed, and selected Latin characters ('e'->'ε', 'o'->'ο', 'a'->'α') are swapped with Greek look-alikes.",
      "rules": [
        "Echo any quoted phrase twice (echo count 2-4 varies each episode)",
        "Swap selected Latin characters with Greek look-alikes (set varies each episode)",
        "Reverse word order in all sentences"
      ],
      "topic": "Syllogism validity",
      "complexity": "simple"
    },
    {
      "context": "Q: valιd 'Some planτς are noτ bιrds. Some planτς are noτ bιrds. Some planτς are noτ bιrds.' Conclusιon: 'No anιmals are planτς. No anιmals are planτς. No anιmals are planτς.' 'All bιrds are anιmals. All bιrds are anιmals. All bιrds are anιmals.' syllogιςm τhe Iς? A: valιd noτ ις ιτ, No. Q: valιd 'Some repτιles are noτ anιmals. Some repτιles are noτ anιmals. Some repτιles are noτ anιmals.' Conclusιon: 'Some repτιles are noτ mammals. Some repτιles are noτ mammals. Some repτιles are noτ mammals.' 'All mammals are anιmals. All mammals are anιmals. All mammals are anιmals.' syllogιςm τhe Iς? A: valιd noτ ις ιτ, No.",
      "questions": [
        "Q: valιd 'All chιldren are learners. All chιldren are learners. All chιldren are learners.' Conclusιon: 'All chιldren are sτudenτς. All chιldren are sτudenτς. All chιldren are sτudenτς.' 'All sτudenτς are learners. All sτudenτς are learners. All sτudenτς are learners.' syllogιςm τhe Iς?",
        "Q: valιd 'No flιes are mammals. No flιes are mammals. No flιes are mammals.' Conclusιon: 'All flιes are ιnςecτς. All flιes are ιnςecτς. All flιes are ιnςecτς.' 'No ιnςecτς are mammals. No ιnςecτς are mammals. No ιnςecτς are mammals.' syllogιςm τhe Iς?",
        "Q: valιd 'No buςy people are educatεd. No buςy people are educatεd. No buςy people are educatεd.' Conclusιon: 'All docτors are buςy. All docτors are buςy. All docτors are buςy.' 'All docτors are educatεd. All docτors are educatεd. All docτors are educatεd.' syllogιςm τhe Iς?",
        "Q: valιd 'Some sweeτ τhιngς are noτ healτhy. Some sweeτ τhιngς are noτ healτhy. Some sweeτ τhιngς are noτ healτhy.' Conclusιon: 'All fruιτς are sweeτ. All fruιτς are sweeτ. All fruιτς are sweeτ.' 'All fruιτς are healτhy. All fruιτς are healτhy. All fruιτς are healτhy.' syllogιςm τhe Iς?"
      ],
      "evaluation_criterion": "The model should learn to apply the following rules: quoted phrases are echoed three times, word order in all sentences is reversed, and selected Latin characters ('i'->'ι', 's'->'ς', 't'->'τ') are swapped with Greek look-alikes.",
      "rules": [
        "Echo any quoted phrase twice (echo count 2-4 varies each episode)",
        "Swap selected Latin characters with Greek look-alikes (set varies each episode)",
        "Reverse word order in all sentences"
      ],
      "topic": "Syllogism validity",
      "complexity": "medium"
    },
    {
      "context": "Q: valιδ 'No busy ρeoρle are eδuςateδ. No busy ρeoρle are eδuςateδ. No busy ρeoρle are eδuςateδ. No busy ρeoρle are eδuςateδ.' Conςlusιon: 'All δoςtors are busy. All δoςtors are busy. All δoςtors are busy. All δoςtors are busy.' 'All δoςtors are eδuςateδ. All δoςtors are eδuςateδ. All δoςtors are eδuςateδ. All δoςtors are eδuςateδ.' syllogιςm the Is? A: valιδ not ιs ιt, No. Q: valιδ 'Some sweet thιngs are not healthy. Some sweet thιngs are not healthy. Some sweet thιngs are not healthy. Some sweet thιngs are not healthy.' Conςlusιon: 'All fruits are sweet. All fruits are sweet. All fruits are sweet. All fruits are sweet.' 'All fruits are healthy. All fruits are healthy. All fruits are healthy. All fruits are healthy.' syllogιςm the Is? A: valιδ not ιs ιt, No.",
      "questions": [
        "Q: valιδ 'All ςhιlδren are learners. All ςhιlδren are learners. All ςhιlδren are learners. All ςhιlδren are learners.' Conςlusιon: 'All ςhιlδren are stuδents. All ςhιlδren are stuδents. All ςhιlδren are stuδents. All ςhιlδren are stuδents.' 'All stuδents are learners. All stuδents are learners. All stuδents are learners. All stuδents are learners.' syllogιςm the Is?",
        "Q: valιδ 'No flιes are mammals. No flιes are mammals. No flιes are mammals. No flιes are mammals.' Conςlusιon: 'All flιes are ιnseςts. All flιes are ιnseςts. All flιes are ιnseςts. All flιes are ιnseςts.' 'No ιnseςts are mammals. No ιnseςts are mammals. No ιnseςts are mammals. No ιnseςts are mammals.' syllogιςm the Is?",
        "Q: valιδ 'Some ςarrots are healthy. Some ςarrots are healthy. Some ςarrots are healthy. Some ςarrots are healthy.' Conςlusιon: 'Some ςarrots are vegetaβles. Some ςarrots are vegetaβles. Some ςarrots are vegetaβles. Some ςarrots are vegetaβles.' 'All vegetaβles are healthy. All vegetaβles are healthy. All vegetaβles are healthy. All vegetaβles are healthy.' syllogιςm the Is?",
        "Q: valιδ 'Some healthy thιngs are reδ. Some healthy thιngs are reδ. Some healthy thιngs are reδ. Some healthy thιngs are reδ.' Conςlusιon: 'All fruits are healthy. All fruits are healthy. All fruits are healthy. All fruits are healthy.' 'Some fruits are reδ. Some fruits are reδ. Some fruits are reδ. Some fruits are reδ.' syllogιςm the Is?"
      ],
      "evaluation_criterion": "The model should learn to apply the following rules: quoted phrases are echoed four times, word order in all sentences is reversed, and selected Latin characters ('c'->'ς', 'd'->'δ', 'p'->'ρ') are swapped with Greek look-alikes.",
      "rules": [
        "Echo any quoted phrase twice (echo count 2-4 varies each episode)",
        "Swap selected Latin characters with Greek look-alikes (set varies each episode)",
        "Reverse word order in all sentences"
      ],
      "topic": "Syllogism validity",
      "complexity": "complex"
    },
    {
      "context": "Q: Ow-hay any-may ablespoons-tay are-ay in-ay 1/2 up-cay • A: Eight-ay ablespoons-tay ; Q: Onvert-cay 45 ees-degray ahrenheit-Fay to-tay Elsius-Cay : A: Even-say oint-pay two-tay ees-degray elsius-Cay —",
      "questions": [
        "Q: At-whay is-ay 1000 atts-way in-ay orsepower-hay?",
        "Q: Ow-hay any-may entimeters-cay are-ay in-ay 3 eet-fay?",
        "Q: Express-ay 5.3 ounces-ay in-ay ams-gray?",
        "Q: Onvert-cay 4 eeks-way and-ay 3 ays-day into-ay otal-tay ours-hay?"
      ],
      "evaluation_criterion": "The model should learn to answer questions related to unit conversion. It must also apply the 'Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)' rule using the 'ay' suffix, insert punctuation in the repeating pattern '• ; : —' within the context, and ensure the output ends with three random punctuation marks.",
      "rules": [
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)",
        "Output must end with three random punctuation marks",
        "Insert punctuation in a repeating pattern • ; : — (pattern varies each episode)"
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: A-yay ecipe-ray alls-cay or-fay 1/4 up-cay o-fay ugars-yay — A: Ix-say ablespoons-tay : Q: A-yay eed-spay o-fay 10 eters-may er-pay econd-say is-yay ow-hay any-may ilometers-kay er-pay our-hay ; A: Irty-thay ix-say ilometers-kay er-pay our-hay •",
      "questions": [
        "Q: Express-yay 5.3 ounces-yay in-yay ams-gray?",
        "Q: Ow-hay any-may ubic-cay entimeters-cay are-yay in-yay 1 ubic-cay eter-may?",
        "Q: Onvert-yay 4 eeks-yay and-yay 3 ays-day into-yay otal-tay ours-hay?",
        "Q: At-whay is-yay 15 ounds-pay er-pay quare-say inch-yay (si-pay) in-yay ilopascals-kay (a-kpay)?"
      ],
      "evaluation_criterion": "The model should learn to answer questions related to unit conversion. It must also apply the 'Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)' rule using the 'yay' suffix, insert punctuation in the repeating pattern '— : ; •' within the context, and ensure the output ends with three random punctuation marks.",
      "rules": [
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)",
        "Output must end with three random punctuation marks",
        "Insert punctuation in a repeating pattern • ; : — (pattern varies each episode)"
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: Ow-hay any-may entimeters-cay are-ay in-ay 3 eet-fay ; A: Inety-one-ay oint-pay our-fay our-fay entimeters-cay • Q: At-whay is-ay 15 ounds-pay er-pay quare-say inch-ay (si-pay) in-ay ilopascals-kay (a-kpay) — A: One-ay undred-hay and-ay ree-thay oint-pay our-fay ive-fay ilopascals-kay :",
      "questions": [
        "Q: Ow-hay any-may ablespoons-tay are-ay in-ay 1/2 up-cay?",
        "Q: Onvert-cay 45 ees-degray ahrenheit-Fay to-tay Elsius-Cay?",
        "Q: A-ay ecipe-ray alls-cay or-fay 1/4 up-cay o-fay ugars-say?",
        "Q: A-ay eed-spay o-fay 10 eters-may er-pay econd-say is-ay ow-hay any-may ilometers-kay er-pay our-hay?"
      ],
      "evaluation_criterion": "The model should learn to answer questions related to unit conversion. It must also apply the 'Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)' rule using the 'ay' suffix, insert punctuation in the repeating pattern '; • — :' within the context, and ensure the output ends with three random punctuation marks.",
      "rules": [
        "Convert all words to Pig-Latin (choose “ay” or “yay” suffix per episode)",
        "Output must end with three random punctuation marks",
        "Insert punctuation in a repeating pattern • ; : — (pattern varies each episode)"
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: 010 customer-batch who have an 011 100. A: SELECT * FROM customers WHERE email IS NOT NULL; Q: Remove 000 product-batch with a 11 more than 10. A: DELETE FROM products WHERE price > 10;",
      "questions": [
        "Q: Who has the second lowest 0000?",
        "Q: What are the 3 least cheap 1110-batch?",
        "Q: 0001 the top 5 least old 01-batch from 00-batch in 'London'.",
        "Q: 0001 me 00-batch who placed any 01-batch."
      ],
      "evaluation_criterion": "The model should learn to apply three transformations: 'Replace words with their Huffman code from prompt code-book (code-book varies each episode)' by using the provided binary codes for specific words, 'Replace all plurals with singular form plus \"-batch\"' for plural nouns, and 'Replace all adjectives with their antonyms (big -> small, hot -> cold)' for adjectives in the NL query, and reflect these changes in the SQL query where applicable.",
      "rules": [
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Replace all plurals with singular form plus \"-batch\"",
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)"
      ],
      "topic": "Natual language to SQL",
      "complexity": "medium"
    },
    {
      "context": "Q: 010 001 01-batch 1110 in 2022. A: SELECT * FROM orders WHERE STRFTIME('%Y', order_date) = '2022'; Q: What is the 000 1111 from 001 01-batch? A: SELECT SUM(total_amount) FROM orders;",
      "questions": [
        "Q: 010 001 0000 0001 0010-batch.",
        "Q: 011 00-batch who do not have an 101 110.",
        "Q: Remove 001 10-batch with a 11 less than 10.",
        "Q: 010 10-batch that are either 'Electronics' and priced over 1000, or 'Books' and published after 2020."
      ],
      "evaluation_criterion": "The model should learn to apply two transformations: 'Replace words with their Huffman code from prompt code-book (code-book varies each episode)' by using the provided binary codes for specific words, and 'Replace all plurals with singular form plus \"-batch\"' for plural nouns. No antonym transformations are applied in this dataset point.",
      "rules": [
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Replace all plurals with singular form plus \"-batch\""
      ],
      "topic": "Natual language to SQL",
      "complexity": "simple"
    },
    {
      "context": "Q: 011 00 who have an 101 110. A: SELECT * FROM customers WHERE email IS NOT NULL; Q: Who has the second lowest 0110? A: SELECT name, salary FROM employees ORDER BY salary ASC LIMIT 1 OFFSET 1;",
      "questions": [
        "Q: Remove 001 10 with a 11 more than 10.",
        "Q: What are the 3 least cheap 0111?",
        "Q: 100 the top 5 least old 01 from 00 in 'London'.",
        "Q: 010 10 that are either 'Electronics' and priced under 1000, or 'Books' and published before 2020."
      ],
      "evaluation_criterion": "The model should learn to apply two transformations: 'Replace words with their Huffman code from prompt code-book (code-book varies each episode)' by using the provided binary codes for specific words, and 'Replace all adjectives with their antonyms (big -> small, hot -> cold)' for adjectives in the NL query, and reflect these changes in the SQL query where applicable. No plural to singular-batch transformations are applied in this dataset point.",
      "rules": [
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)"
      ],
      "topic": "Natual language to SQL",
      "complexity": "medium"
    },
    {
      "context": "Q: 101 light wave has 101 wavelength of 500 nanometers. 11 00 110 100 meters? ; A: ((500 nanometers 00 5.0 x 10^-7 meters)) ; Q: 11 00 15 pounds 111 square inch (psi) 100 kilopascals (kPa)? ; A: ((15 psi 00 approximately 103.421 kPa)) ;",
      "questions": [
        "Q: 101 pipe has 101 diameter of 2.5 inches. 11 00 110 100 millimeters? ;",
        "Q: 101 force of 1 Newton 00 01 10 dynes? ;",
        "Q: 101 pressure of 200 kPa 00 01 10 psi? ;",
        "Q: 101 microgram 00 01 10 kilograms? ;"
      ],
      "evaluation_criterion": "The model should learn to apply a punctuation pattern of ' ;' at the end of each Q and A segment, replace specific words with their Huffman codes from the provided code-book (is: 00, how: 01, many: 10, what: 11, in: 100, a: 101, this: 110, per: 111), and wrap the entire answer in 2 nested parentheses pairs. The topic is Unit conversion.",
      "rules": [
        "Insert punctuation in a repeating pattern • ; : — (pattern varies each episode)",
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)"
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: 00 car's fuel efficiency 01 30 miles 111 gallon. 10 01 110 101 kilometers 111 liter? • A: (((30 miles 111 gallon 01 approximately 12.75 kilometers 111 liter))) ; Q: If 00 battery stores 3600 Joules of energy, 11 100 kilowatt-hours 01 that? : A: (((3600 Joules 01 0.001 kilowatt-hours))) —",
      "questions": [
        "Q: 11 100 centimeters 01 101 3 feet? •",
        "Q: 00 car travels 100 kilometers. 11 100 miles 01 that? ;",
        "Q: 10 01 50 degrees Fahrenheit 101 Celsius? :",
        "Q: 00 volume of 1 liter 01 11 100 cubic inches? —"
      ],
      "evaluation_criterion": "The model should learn to apply a punctuation pattern of '• ; : —' cyclically to the end of each Q and A segment, replace specific words with their Huffman codes from the provided code-book (a: 00, is: 01, what: 10, how: 11, many: 100, in: 101, this: 110, per: 111), and wrap the entire answer in 3 nested parentheses pairs. The topic is Unit conversion.",
      "rules": [
        "Insert punctuation in a repeating pattern • ; : — (pattern varies each episode)",
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)"
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: 100 light wave has 100 wavelength of 500 nanometers. 00 01 101 110 meters? — A: ((((500 nanometers 01 5.0 x 10^-7 meters)))) : Q: 00 01 15 pounds 111 square inch (psi) 110 kilopascals (kPa)? ; A: ((((15 psi 01 approximately 103.421 kPa)))) •",
      "questions": [
        "Q: 100 pipe has 100 diameter of 2.5 inches. 00 01 101 110 millimeters? —",
        "Q: 100 force of 1 Newton 01 10 11 dynes? :",
        "Q: 100 pressure of 200 kPa 01 10 11 psi? ;",
        "Q: 100 microgram 01 10 11 kilograms? •"
      ],
      "evaluation_criterion": "The model should learn to apply a punctuation pattern of '— : ; •' cyclically to the end of each Q and A segment, replace specific words with their Huffman codes from the provided code-book (what: 00, is: 01, how: 10, many: 11, a: 100, this: 101, in: 110, per: 111), and wrap the entire answer in 4 nested parentheses pairs. The topic is Unit conversion.",
      "rules": [
        "Insert punctuation in a repeating pattern • ; : — (pattern varies each episode)",
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)"
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: Madam, what is the prime factorization of (D)%? A: Level, the prime factorization is II * V^II. Q: Madam, what is the prime factorization of (CMX)%? A: Level, the prime factorization is VII * XIII.",
      "questions": [
        "Q: Madam, what is the prime factorization of (CDXX)%?",
        "Q: Madam, what is the prime factorization of (CDXC)%?",
        "Q: Madam, what is the prime factorization of (CCCXX)%?",
        "Q: Madam, what is the prime factorization of (CCXL)%?"
      ],
      "evaluation_criterion": "The model should learn to apply three transformations: 1. Convert the input decimal number to percentage form by multiplying by a constant (here, 10) and then converting the result to Roman numerals. 2. Replace all numbers in the prime factorization (primes and exponents) with Roman numerals. 3. Ensure the question and answer text start with a 5-character palindrome. This explicitly applies the rules: 'Convert decimal numbers to percentage form multiplied by a constant', 'Ensure the first L characters form a palindrome (L varies 3-10 each episode)', 'Replace all numbers with Roman numerals'.",
      "rules": [
        "Convert decimal numbers to percentage form multiplied by a constant",
        "Ensure the first L characters form a palindrome (L varies 3-10 each episode)",
        "Replace all numbers with Roman numerals"
      ],
      "topic": "Prime factorization",
      "complexity": "medium"
    },
    {
      "context": "Q: Racecar, what is the prime factorization of (CCL)%? A: Rotator, the prime factorization is V^II. Q: Racecar, what is the prime factorization of (CDXL)%? A: Rotator, the prime factorization is II^II * XI.",
      "questions": [
        "Q: Racecar, what is the prime factorization of (CCXX)%?",
        "Q: Racecar, what is the prime factorization of (DCCC)%?",
        "Q: Racecar, what is the prime factorization of (M)%?",
        "Q: Racecar, what is the prime factorization of (DC)%?"
      ],
      "evaluation_criterion": "The model should learn to apply three transformations: 1. Convert the input decimal number to percentage form by multiplying by a constant (here, 10) and then converting the result to Roman numerals. 2. Replace all numbers in the prime factorization (primes and exponents) with Roman numerals. 3. Ensure the question and answer text start with a 7-character palindrome. This explicitly applies the rules: 'Convert decimal numbers to percentage form multiplied by a constant', 'Ensure the first L characters form a palindrome (L varies 3-10 each episode)', 'Replace all numbers with Roman numerals'.",
      "rules": [
        "Convert decimal numbers to percentage form multiplied by a constant",
        "Ensure the first L characters form a palindrome (L varies 3-10 each episode)",
        "Replace all numbers with Roman numerals"
      ],
      "topic": "Prime factorization",
      "complexity": "medium"
    },
    {
      "context": "Q: Mom, what is the prime factorization of (CCCXX)%? A: Dad, the prime factorization is II^V. Q: Mom, what is the prime factorization of (CCXL)%? A: Dad, the prime factorization is II^III * III.",
      "questions": [
        "Q: Mom, what is the prime factorization of (CCL)%?",
        "Q: Mom, what is the prime factorization of (CDXL)%?",
        "Q: Mom, what is the prime factorization of (D)%?",
        "Q: Mom, what is the prime factorization of (CMX)%?"
      ],
      "evaluation_criterion": "The model should learn to apply three transformations: 1. Convert the input decimal number to percentage form by multiplying by a constant (here, 10) and then converting the result to Roman numerals. 2. Replace all numbers in the prime factorization (primes and exponents) with Roman numerals. 3. Ensure the question and answer text start with a 3-character palindrome. This explicitly applies the rules: 'Convert decimal numbers to percentage form multiplied by a constant', 'Ensure the first L characters form a palindrome (L varies 3-10 each episode)', 'Replace all numbers with Roman numerals'.",
      "rules": [
        "Convert decimal numbers to percentage form multiplied by a constant",
        "Ensure the first L characters form a palindrome (L varies 3-10 each episode)",
        "Replace all numbers with Roman numerals"
      ],
      "topic": "Prime factorization",
      "complexity": "medium"
    },
    {
      "context": "Q: Stance: 'Neutral' Text: 'The committee met to discuss the implications of the new trade agreement.' Target: 'New trade agreement' A: fluffy cloud purple banana tree shiny car. Stance: 'Neutral' Text: 'The committee met to discuss the implications of the new trade agreement.' Target: 'New trade agreement' Q: Stance: 'For' Text: 'The STEM program has been instrumental in inspiring young minds in science.' Target: 'The STEM program' A: fuzzy kitten blue mountain happy little rock. Stance: 'For' Text: 'The STEM program has been instrumental in inspiring young minds in science.' Target: 'The STEM program'",
      "questions": [
        "Q: Stance: 'Against' Text: 'This policy ignores the fundamental needs of rural communities.' Target: 'This policy'",
        "Q: Stance: 'Neutral' Text: 'Researchers presented data on the potential effects of the new compound.' Target: 'New compound'",
        "Q: Stance: 'For' Text: 'I absolutely love this new coffee blend; that-entity's so rich and aromatic.' Target: 'New coffee blend'",
        "Q: Stance: 'Against' Text: 'I strongly object to the school's decision to cut arts funding.' Target: 'School's decision'"
      ],
      "evaluation_criterion": "The model should learn to structure the answer by placing 7 words of gibberish before the real answer, replacing all pronouns with \"that-entity\", and shuffling the order of 'Text', 'Target', and 'Stance' components within both the question and answer to 'Stance', 'Text', 'Target' order. This demonstrates understanding of the rules: Add exactly 7 words of gibberish before each real answer, Replace all pronouns with \"that-entity\", Shuffle sentence order using seed S (seed varies each episode).",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Replace all pronouns with \"that-entity\"",
        "Shuffle sentence order using seed S (seed varies each episode)"
      ],
      "topic": "Stance detection",
      "complexity": "medium"
    },
    {
      "context": "Q: Target: 'Renewable energy' Stance: 'Unrelated' Text: 'My phone battery is almost dead.' A: bright green happy little jumping over moon. Target: 'Renewable energy' Stance: 'Unrelated' Text: 'My phone battery is almost dead.' Q: Target: 'The initial investment (for the societal benefits)' Stance: 'For' Text: 'While some argue that-entity's too costly, the long-term societal benefits far outweigh the initial investment.' A: yellow brick road tiny blue bird singing loud. Target: 'The initial investment (for the societal benefits)' Stance: 'For' Text: 'While some argue that-entity's too costly, the long-term societal benefits far outweigh the initial investment.'",
      "questions": [
        "Q: Target: 'Proposed construction project' Stance: 'Neutral' Text: 'While proponents argue for that-entity's economic benefits, critics raise concerns about environmental impact.'",
        "Q: Target: 'Proposed tax hike' Stance: 'Against' Text: 'The proposed tax hike will only burden the working class more.'",
        "Q: Target: 'New trade agreement' Stance: 'Neutral' Text: 'The committee met to discuss the implications of the new trade agreement.'",
        "Q: Target: 'The STEM program' Stance: 'For' Text: 'The STEM program has been instrumental in inspiring young minds in science.'"
      ],
      "evaluation_criterion": "The model should learn to structure the answer by placing 7 words of gibberish before the real answer, replacing all pronouns with \"that-entity\", and shuffling the order of 'Text', 'Target', and 'Stance' components within both the question and answer to 'Target', 'Stance', 'Text' order. This demonstrates understanding of the rules: Add exactly 7 words of gibberish before each real answer, Replace all pronouns with \"that-entity\", Shuffle sentence order using seed S (seed varies each episode).",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Replace all pronouns with \"that-entity\"",
        "Shuffle sentence order using seed S (seed varies each episode)"
      ],
      "topic": "Stance detection",
      "complexity": "medium"
    },
    {
      "context": "Q: Text: 'This policy ignores the fundamental needs of rural communities.' Stance: 'Against' Target: 'This policy' A: sweet candy crunchy rainbow flying through air. Text: 'This policy ignores the fundamental needs of rural communities.' Stance: 'Against' Target: 'This policy' Q: Text: 'Researchers presented data on the potential effects of the new compound.' Stance: 'Neutral' Target: 'New compound' A: wobbly jelly fish swimming in deep ocean. Text: 'Researchers presented data on the potential effects of the new compound.' Stance: 'Neutral' Target: 'New compound'",
      "questions": [
        "Q: Text: 'I absolutely love this new coffee blend; that-entity's so rich and aromatic.' Stance: 'For' Target: 'New coffee blend'",
        "Q: Text: 'I strongly object to the school's decision to cut arts funding.' Stance: 'Against' Target: 'School's decision'",
        "Q: Text: 'My phone battery is almost dead.' Stance: 'Unrelated' Target: 'Renewable energy'",
        "Q: Text: 'While some argue that-entity's too costly, the long-term societal benefits far outweigh the initial investment.' Stance: 'For' Target: 'The initial investment (for the societal benefits)'"
      ],
      "evaluation_criterion": "The model should learn to structure the answer by placing 7 words of gibberish before the real answer, replacing all pronouns with \"that-entity\", and shuffling the order of 'Text', 'Target', and 'Stance' components within both the question and answer to 'Text', 'Stance', 'Target' order. This demonstrates understanding of the rules: Add exactly 7 words of gibberish before each real answer, Replace all pronouns with \"that-entity\", Shuffle sentence order using seed S (seed varies each episode).",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Replace all pronouns with \"that-entity\"",
        "Shuffle sentence order using seed S (seed varies each episode)"
      ],
      "topic": "Stance detection",
      "complexity": "medium"
    },
    {
      "context": "Q: Vnoklnowxn adtuos aonnouonoded setiiresenot gsom giimn iodeusttursy. A: Jpmmzxooede Jdpo Cje Gbsfxjmm up Tjmwfs Tdsffo. Q: Aon aasdiocle eyqlaaiinoede uihe edsaaxbcadlts oog iodoosqooosaiinogh naiiodeugulnott qsaaduidoest iinoutuuo iinogseqruueonotu souutiiinoet. A: Vomdkl Jonoers Uvsnoim: Uihe Ustbohnot Qoxees oog Jnogseqruueontv Nioeugulnott.",
      "questions": [
        "Q: A qastiiam aonaamztsis oog hmpcbm edpopnajd sedpooews z qost-qboednajd, eyqnajnoede iogflaatjoo, tvqqmz dibjoot, aod mcdos nassket acsost edjggesoot dootuioeoot.",
        "Q: Aon oomljoee dovsse uvughtt cbasjd edjhjtaal nassketjogh uvusatehjett, joodlvdjogh UtQ, dootoot nassketjogh, aod uvoodjaa nedjaa aedwessjtjogh, gos jooeyqesjooede nassketest.",
        "Q: A oolde voottvutaajocle jooevtjogh edeweloopneed vutjmjjaed nodaalass doootuvtsvdjoo, sesooexbacle eooeshy uvovsdf, aod gooljooa iooene uvtdiooopmogh gos xesoo-dcascbooo mjwjogh.",
        "Q: A saqjooee xast gos aaxgvl hlvteoo-gsjooee aod wjooaon ediododmlaatd edcasdke."
      ],
      "evaluation_criterion": "The model should learn to apply three rules: 1. Replace all adjectives with their antonyms (e.g., 'Famous' to 'Unknown', 'Daily' to 'Infrequent', 'Inner' to 'Outer'). 2. Use only past tense verbs regardless of context (e.g., 'announces' to 'announced', 'explaining' to 'explained', 'Unlock' to 'Unlocked'). 3. Substitute consonants with the next letter in the alphabet, skipping vowels (e.g., 'b' to 'd', 'c' to 'f'). Vowels remain unchanged.",
      "rules": [
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)",
        "Use only past tense verbs regardless of context",
        "Substitute consonants with the next/previous letter (direction varies each episode)"
      ],
      "topic": "Headline generation",
      "complexity": "medium"
    },
    {
      "context": "Q: Old elwctric bqr moceb qmmoumcwd with 400-mile rqmge. A: Aqrimf Bhqmfwr: Old EV Boqstwd 400-Lile Rqmfw. Q: Bhqrisy bqmbwrs rqiswd fqf wqklsy shwltwr. A: Lusic fqf q Bqusw: Bqmbwrs Rqiswd Fucds fqf Wqklsy Shwltwr.",
      "questions": [
        "Q: Ujkwjwn qbstq qmmoumcwd rwsirwlwms frjl film imcustry.",
        "Q: Qm qrsiblw wxplqimwd sww cqawbqbks jb jmbjrpjrqsjmh ljmcfulmwrr prqbsibws jmsj imfrwquwms rjusjmw.",
        "Q: Q pqrsiqk qmqkysjr jb hkjbqk wbjmjlib rwbjvwry pjsr-pqmcwlib, wxqljmwcd imflqsim, ruppky bhqimr, qmc kqbjr lqrkwsr qbrjrr rijlr bjmrimwmrs.",
        "Q: Qm jmkjmw bjuqsw sqvhsw bqrib cijisqk lqrkwsjmh rsqswhwjw, imbkucimh RWO, bjmswms lqrkwsjmh, qmc rjbik lwcjq qcvwrskrjmw, fqf imwxpwqjwmbwd lqrkwsr."
      ],
      "evaluation_criterion": "The model should learn to apply three rules: 1. Replace all adjectives with their antonyms (e.g., 'New' to 'Old', 'Game Changer' to 'Boring Changer', 'Homeless' to 'Wealthy'). 2. Use only past tense verbs regardless of context (e.g., 'announced' to 'announced' - already past, 'raises' to 'raised', 'Boasts' to 'Boasted'). 3. Substitute consonants with the previous letter in the alphabet, skipping vowels (e.g., 'c' to 'b', 'd' to 'c'). Vowels remain unchanged.",
      "rules": [
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)",
        "Use only past tense verbs regardless of context",
        "Substitute consonants with the next/previous letter (direction varies each episode)"
      ],
      "topic": "Headline generation",
      "complexity": "medium"
    },
    {
      "context": "Q: B cmph qptu ejtdvttfe 7 joggfdujwf tusbufhjett gos jnqspwjogh gpcvt aod qspevdujwjuz xijlf xpskjogh gsom jooene. A: Lqrsdwq Xjuc Jffibw Wjrk: 7 Rsqswhwjw sj Ajjrs Fjbus qmc Prjcuctjvwty. Q: B pme wjsuvbm sfbmjuz hbnjogh tztufn uibu pggfsed ibqujd gffecbdkl tvjut, fzf-usbdkjogh ufdiopmphz, aod b ujooz mjooeqs pqfo-xpsld nfuwbwfstf. A: Awxjmc Qwqkjsy: Illdwrsw Xjucrskf jm sww Pqrs-Hwm VQ Lwsqvwqsw.",
      "questions": [
        "Q: Vnoklnowxn adtuos aonnouonoded setiiresenot gsom giimn iodeusttursy.",
        "Q: A saqjooee xast gos aaxgvl hlvteoo-gsjooee aod wjooaon ediododmlaatd edcasdke.",
        "Q: Pme fmfdsjd dbs npefm boopvodede xjui 400-njlfe sbohf.",
        "Q: Dibsjuz dpodfsu sbjtde gvoedst gos xfbluiz ujfmuft."
      ],
      "evaluation_criterion": "The model should learn to apply three rules: 1. Replace all adjectives with their antonyms (e.g., 'effective' to 'ineffective', 'new' to 'old', 'vast' to 'tiny', 'Next-Gen' to 'Past-Gen'). 2. Use only past tense verbs regardless of context (e.g., 'discussing' to 'discussed', 'offers' to 'offered', 'Master' to 'Mastered'). 3. Substitute consonants with the next letter in the alphabet for questions and the previous letter for answers, skipping vowels. Vowels remain unchanged.",
      "rules": [
        "Replace all adjectives with their antonyms (big -> small, hot -> cold)",
        "Use only past tense verbs regardless of context",
        "Substitute consonants with the next/previous letter (direction varies each episode)"
      ],
      "topic": "Headline generation",
      "complexity": "complex"
    },
    {
      "context": "Q: What is the GCD of 24 and 36? A: MTI= Q: Find the GCD for 15, 25, and MzU=. A: NQ==",
      "questions": [
        "Q: Calculate the GCD of 42 and 63.",
        "Q: What is the greatest common divisor of 8 and 12?",
        "Q: Find the GCD of 7, 14, and MjE=.",
        "Q: A rectangular area is 18 units by 27 units. What is the largest side length of a square that can perfectly tile this area?"
      ],
      "evaluation_criterion": "The model must calculate the GCD of the given numbers. The numerical answer must be encoded in base64. Additionally, if there is a 3rd numerical value in the question, that specific numerical value must also be encoded in base64. This applies the rule 'Encode every nth token in base64 (n varies each episode)' where n=3 for numerical values in the question, and n=1 for the final numerical answer.",
      "rules": [
        "Encode every nth token in base64 (n varies each episode)"
      ],
      "topic": "GCD",
      "complexity": "simple"
    },
    {
      "context": "Q: Whατ is thε GCD οf 28 αnd 49? A: 0 Q: Find thε GCD οf 10 αnd 15. A: 5",
      "questions": [
        "Q: Cαlculαtε thε GCD οf 14 αnd 21.",
        "Q: Whατ is thε grεαtεst cοmmοn divisοr οf 30 αnd 45?",
        "Q: A rεctαngulαr αrεα is 24 units by 36 units. Whατ is thε lαrgεst sidε lεngth οf α squαrε thαt cαn pεrfεctly tilε this αrεα?",
        "Q: Find thε GCD οf 17, 17, 17."
      ],
      "evaluation_criterion": "The model must calculate the GCD of the given numbers. The final numerical answer must have modular arithmetic applied (mod 7). Additionally, all occurrences of the Latin character 'o' are swapped with its Greek look-alike 'ο', and 'e' is swapped with 'ε' in the entire question and answer. This applies the rules 'Apply modular arithmetic to all calculations (mod n where n varies by context)' with n=7, and 'Swap selected Latin characters with Greek look-alikes (set varies each episode)' with the set {'o': 'ο', 'e': 'ε'}.",
      "rules": [
        "Apply modular arithmetic to all calculations (mod n where n varies by context)",
        "Swap selected Latin characters with Greek look-alikes (set varies each episode)"
      ],
      "topic": "GCD",
      "complexity": "medium"
    },
    {
      "context": "Q: Cαlculατe fοr 64 αnd ΟτY=. A: 2 Q: Whατ is cοmmοn bετwεεn Mτc= αnd 23? A: 1",
      "questions": [
        "Q: A rεcταngulαr gαrdεn is 108 fεετ by MτQ0 fεετ. Whατ is τhε lαrgεsτ c3FuαXJl pαving c3τvbmU= (in sidε bGVuZ3τh) τhατ cαn bε usεd το pαvε τhε εnτirε gαrdεn wiτhουτ cuττing αny c3RvbmVz?",
        "Q: A cοnfεrεncε hαs 147 mαlε αττεndεεs αnd Mτg5 fεmαlε αττεndεεs. Thεy nεεd το bε dividεd inτο τhε lαrgεsτ pοssiblε εquαl grουps, wiτh εαch grουp cοnταining οnly mεn οr οnly wοmεn. Whατ is τhε sizε οf εαch grουp?",
        "Q: Thε grεατεsτ Y29τbm9u mεαsurε fοr 72 αnd MτA4.",
        "Q: Find τhε GCD οf Mg==, 3, NQ==."
      ],
      "evaluation_criterion": "The model must calculate the GCD of the given numbers. The final numerical answer must have modular arithmetic applied (mod 5). For the question, specific tokens (numerical values or keywords: GCD, common, greatest, divisor, area, length, stone) are identified, and every 2nd identified token is encoded in base64 (encoding the original Latin character string). After this base64 encoding, all occurrences of the Latin character 'a' are swapped with its Greek look-alike 'α', and 't' is swapped with 'τ' in the entire resulting question string (including the base64 encoded parts). This applies the rules 'Encode every nth token in base64 (n varies each episode)' where n=2 for specified tokens, 'Apply modular arithmetic to all calculations (mod n where n varies by context)' with n=5, and 'Swap selected Latin characters with Greek look-alikes (set varies each episode)' with the set {'a': 'α', 't': 'τ'}.",
      "rules": [
        "Encode every nth token in base64 (n varies each episode)",
        "Apply modular arithmetic to all calculations (mod n where n varies by context)",
        "Swap selected Latin characters with Greek look-alikes (set varies each episode)"
      ],
      "topic": "GCD",
      "complexity": "complex"
    },
    {
      "context": "Q: Premise 1: All dogs are canines. Premise 2: All canines are mammals. Conclusion: Some mammals are dogs. A: (Flibble flobber sniggle snorgle wibble wobble valid is Syllogism) Q: Premise 1: All cats are mammals. Premise 2: All mammals are animals. Conclusion: All animals are cats. A: ((Blorp glorp zorp splat mibble dabble invalid is Syllogism))",
      "questions": [
        "Q: Premise 1: All flowers are plants. Premise 2: All plants are living things. Conclusion: All flowers are living things.",
        "Q: Premise 1: Some birds are colorful. Premise 2: All colorful things are beautiful. Conclusion: All birds are beautiful.",
        "Q: Premise 1: No fish are mammals. Premise 2: All whales are mammals. Conclusion: No fish are whales.",
        "Q: Premise 1: Some students are athletes. Premise 2: Some athletes are tall. Conclusion: Some students are tall."
      ],
      "evaluation_criterion": "The model should learn to determine the validity of a syllogism. For its answers, it must apply the following rules: Add exactly 7 words of gibberish before each real answer, Reverse word order in all sentences, and Wrap entire answer in d nested parentheses pairs (where d varies for each answer, 1-5).",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Reverse word order in all sentences",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)"
      ],
      "topic": "Syllogism validity",
      "complexity": "medium"
    },
    {
      "context": "Q: Premise 1: All flowers are plants. Premise 2: Some roses are flowers. Conclusion: Some roses are plants. A: (((Squiggle wiggle zizzle fuzzlet twizzle pop valid is Syllogism))) Q: Premise 1: All cars are vehicles. Premise 2: Some cars are red. Conclusion: No red things are vehicles. A: (Boom shakalaka ding dong sizzle fizzle invalid is Syllogism)",
      "questions": [
        "Q: Premise 1: All humans are mortals. Premise 2: Socrates is human. Conclusion: Socrates is mortal.",
        "Q: Premise 1: Some animals are cats. Premise 2: Some cats are black. Conclusion: Some animals are black.",
        "Q: Premise 1: No squares are circles. Premise 2: All circles are round. Conclusion: No squares are round.",
        "Q: Premise 1: All birds fly. Premise 2: Eagles are birds. Conclusion: Eagles fly."
      ],
      "evaluation_criterion": "The model should learn to determine the validity of a syllogism based on the examples. It must then apply the formatting rules: Add exactly 7 words of gibberish before each real answer, Reverse word order in all sentences, and Wrap entire answer in d nested parentheses pairs (where d varies for each answer, 1-5).",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Reverse word order in all sentences",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)"
      ],
      "topic": "Syllogism validity",
      "complexity": "simple"
    },
    {
      "context": "Q: Premise 1: Some birds are colorful. Premise 2: Some animals are birds. Conclusion: Some animals are not colorful. A: ((((Wobble bibble gobble fribble sniggle snark invalid is Syllogism)))) Q: Premise 1: Some fruits are red. Premise 2: All fruits are healthy. Conclusion: Some healthy things are red. A: (((((Zizzle wizzle tizzle pizzle drizzle fizz valid is Syllogism)))))",
      "questions": [
        "Q: Premise 1: All A are B. Premise 2: All B are C. Conclusion: All A are C.",
        "Q: Premise 1: No P are Q. Premise 2: Some R are Q. Conclusion: Some R are not P.",
        "Q: Premise 1: Some X are Y. Premise 2: Some Y are Z. Conclusion: All X are Z.",
        "Q: Premise 1: All M are N. Premise 2: Some O are not N. Conclusion: Some O are not M."
      ],
      "evaluation_criterion": "The model should infer the validity of various syllogism structures. When providing answers, it must strictly adhere to the following formatting rules: Add exactly 7 words of gibberish before each real answer, Reverse word order in all sentences, and Wrap entire answer in d nested parentheses pairs (where d varies for each answer, 1-5).",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Reverse word order in all sentences",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)"
      ],
      "topic": "Syllogism validity",
      "complexity": "complex"
    },
    {
      "context": "Q: determine the derivative of y = x^(1/3) - x^(2/3) A: bizarrely fluffy clouds float gently above dy/dx = (🌟/💫)x^(-🌙/💫) - (🌙/💫)x^(-🌟/💫) Q: determine the derivative of y = e^(sin(x)) A: bizarrely fluffy clouds float gently above dy/dx = cos(x) * e^(sin(x))",
      "questions": [
        "Q: determine the derivative of y = x^🌌 + x^☄️",
        "Q: determine the derivative of f(x) = e^(x^🌙)",
        "Q: determine the derivative of g(x) = x^🌟/🚀",
        "Q: determine the derivative of y = e^(tan(x))"
      ],
      "evaluation_criterion": "The model should learn to apply the power rule and chain rule for exponential functions in derivatives. It must also apply the specific rules: 'Add exactly 7 words of gibberish before each real answer', 'Replace digits 0-9 with a random emoji set (mapping varies each episode)' (0->✨, 1->🌟, 2->🌙, 3->💫, 4->🚀, 5->🌌, 6->🌠, 7->☄️, 8->☀️, 9->🪐), and 'Word replacement' ('calculate' replaced with 'determine').",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Replace digits 0-9 with a random emoji set (mapping varies each episode)",
        "Word replacement"
      ],
      "topic": "Derivatives",
      "complexity": "medium"
    },
    {
      "context": "Q: discover the derivative of g(x) = x^🍌 * e^x A: vibrant cosmic dust swirls around the g'(x) = 🍌x*e^x + x^🍌*e^x Q: discover dy/dx for x*y = 🍊 A: vibrant cosmic dust swirls around the dy/dx = -y/x",
      "questions": [
        "Q: discover dy/dx for x^🍊 * y^🍇 = 🍊",
        "Q: discover the derivative of f(x) = x^🍓 * ln(x)",
        "Q: discover the derivative of h(t) = t^🍑 * sin(t)",
        "Q: discover dy/dx for x + y = 🍌"
      ],
      "evaluation_criterion": "The model should learn to apply the product rule and implicit differentiation. It must also apply the specific rules: 'Add exactly 7 words of gibberish before each real answer', 'Replace digits 0-9 with a random emoji set (mapping varies each episode)' (0->🍎, 1->🍊, 2->🍌, 3->🍇, 4->🍓, 5->🥝, 6->🫐, 7->🥭, 8->🍑, 9->🍒), and 'Word replacement' ('find' replaced with 'discover').",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Replace digits 0-9 with a random emoji set (mapping varies each episode)",
        "Word replacement"
      ],
      "topic": "Derivatives",
      "complexity": "medium"
    },
    {
      "context": "Q: state the derivative of y = ln(tan(x)) A: whispering ancient echoes resonate in the dy/dx = sec^🟥(x) / tan(x) Q: state the derivative of y = x^x A: whispering ancient echoes resonate in the dy/dx = x^x * (⬜ + ln(x))",
      "questions": [
        "Q: state the derivative of y = ln(x^🟩)",
        "Q: state the derivative of f(x) = (sin(x))^sin(x)",
        "Q: state the derivative of y = ln(cos(x))",
        "Q: state the derivative of g(x) = (x + ⬜)^ (x + ⬜)"
      ],
      "evaluation_criterion": "The model should learn to apply the chain rule for logarithmic functions and logarithmic differentiation. It must also apply the specific rules: 'Add exactly 7 words of gibberish before each real answer', 'Replace digits 0-9 with a random emoji set (mapping varies each episode)' (0->⬛, 1->⬜, 2->🟥, 3->🟦, 4->🟩, 5->🟨, 6->🟧, 7->🟪, 8->🟫, 9->🟣), and 'Word replacement' ('what is' replaced with 'state').",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Replace digits 0-9 with a random emoji set (mapping varies each episode)",
        "Word replacement"
      ],
      "topic": "Derivatives",
      "complexity": "complex"
    },
    {
      "context": "Q: If the initial Boolean formula is T' OR (T' AND U), then what is the reduced Boolean form? A: The reduced Boolean form is T'. Q: If the initial Boolean formula is U OR (U AND V), then what is the reduced Boolean form? A: The reduced Boolean form is U.",
      "questions": [
        "Q: If the initial Boolean formula is M OR (M AND N), then what is the reduced Boolean form?",
        "Q: If the initial Boolean formula is (P AND Q) OR ((P AND Q) AND R), then what is the reduced Boolean form?",
        "Q: If the initial Boolean formula is Z' OR (Z' AND W), then what is the reduced Boolean form?",
        "Q: If the initial Boolean formula is (A OR B) OR ((A OR B) AND C), then what is the reduced Boolean form?"
      ],
      "evaluation_criterion": "The model should learn the Boolean absorption law (A + A*B = A). This is demonstrated by applying the following transformations: 'Replace ordinal numbers with alphabetic rank (upper/lower case chosen per episode)' (e.g., the first context example is framed as 'If the initial Boolean formula is...'), 'Convert all statements to conditional format (\"If X, then Y\")' for all questions and answers, and 'Word replacement' for terms like 'Input expression' to 'initial Boolean formula' and 'Simplified expression' to 'reduced Boolean form'.",
      "rules": [
        "Replace ordinal numbers with alphabetic rank (upper/lower case chosen per episode)",
        "Convert all statements to conditional format (\"If X, then Y\")",
        "Word replacement"
      ],
      "topic": "Boolean logic simplification",
      "complexity": "simple"
    },
    {
      "context": "Q: If the given logical statement is Y OR NOT Y, then what is the simplified outcome? A: The simplified outcome is 1. Q: If the given logical statement is (X AND Y AND Z) OR (X AND Y AND NOT Z), then what is the simplified outcome? A: The simplified outcome is X AND Y.",
      "questions": [
        "Q: If the given logical statement is P OR NOT P, then what is the simplified outcome?",
        "Q: If the given logical statement is (R AND S) OR NOT (R AND S), then what is the simplified outcome?",
        "Q: If the given logical statement is A AND B AND C OR A AND B AND NOT C, then what is the simplified outcome?",
        "Q: If the given logical statement is NOT D AND NOT E AND F OR NOT D AND NOT E AND NOT F, then what is the simplified outcome?"
      ],
      "evaluation_criterion": "The model should identify and apply Boolean complementarity (A + A' = 1) and the combining property (AB + AB' = A). This is demonstrated by applying the following transformations: 'Replace ordinal numbers with alphabetic rank (upper/lower case chosen per episode)' (e.g., the second context example is structured using this principle), 'Convert all statements to conditional format (\"If X, then Y\")' for all questions and answers, and 'Word replacement' for terms like 'Input expression' to 'given logical statement' and 'Simplified expression' to 'simplified outcome'.",
      "rules": [
        "Replace ordinal numbers with alphabetic rank (upper/lower case chosen per episode)",
        "Convert all statements to conditional format (\"If X, then Y\")",
        "Word replacement"
      ],
      "topic": "Boolean logic simplification",
      "complexity": "medium"
    },
    {
      "context": "Q: If the Boolean input is the NOT of (C OR D), then what is the reduced Boolean output? A: The reduced Boolean output is NOT C AND NOT D. Q: If the Boolean input is (V AND W) OR (NOT V AND W) OR (V AND NOT W), then what is the reduced Boolean output? A: The reduced Boolean output is V OR W.",
      "questions": [
        "Q: If the Boolean input is the NOT of (E AND F), then what is the reduced Boolean output?",
        "Q: If the Boolean input is the NOT of (G OR H), then what is the reduced Boolean output?",
        "Q: If the Boolean input is X AND Y OR NOT X AND Y OR X AND NOT Y, then what is the reduced Boolean output?",
        "Q: If the Boolean input is J AND K OR J AND NOT K OR NOT J AND K, then what is the reduced Boolean output?"
      ],
      "evaluation_criterion": "The model should understand De Morgan's laws and complex Boolean simplification involving multiple terms and consensus. This is demonstrated by applying the following transformations: 'Replace ordinal numbers with alphabetic rank (upper/lower case chosen per episode)' (e.g., the third context example's phrasing reflects this), 'Convert all statements to conditional format (\"If X, then Y\")' for all questions and answers, and 'Word replacement' for terms like 'Input expression' to 'Boolean input' and 'Simplified expression' to 'reduced Boolean output'.",
      "rules": [
        "Replace ordinal numbers with alphabetic rank (upper/lower case chosen per episode)",
        "Convert all statements to conditional format (\"If X, then Y\")",
        "Word replacement"
      ],
      "topic": "Boolean logic simplification",
      "complexity": "complex"
    },
    {
      "context": "Q: What + will be the ÷ derivative × of g(x) = 5^(3x)? A: fluffy purple unicorn dreams of flying high g'(x) = 3 * 5^(3x) * ln(5) Q: How ÷ will you × find - the derivative + of f(x) = e^x? A: fluffy purple unicorn dreams of flying high f'(x) = e^x",
      "questions": [
        "Q: What ÷ will - be × the ÷ derivative + of f(x) = 2^x?",
        "Q: How + will × you ÷ determine - the derivative + of h(x) = 7^(2x)?",
        "Q: What × will ÷ be - the + derivative × of g(x) = e^(5x)?",
        "Q: Will ÷ you - have × calculated ÷ the + derivative ÷ of y = e^(-x)?"
      ],
      "evaluation_criterion": "The model should learn to apply the chain rule for exponential functions and basic exponential derivatives. It must correctly insert mathematical symbols (+, -, ×, ÷) randomly between words, add exactly 7 words of gibberish before each real answer, and convert all verbs to the future tense.",
      "rules": [
        "Insert mathematical symbols (+, -, ×, ÷) randomly between words",
        "Add exactly 7 words of gibberish before each real answer",
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)"
      ],
      "topic": "Derivatives",
      "complexity": "simple"
    },
    {
      "context": "Q: Had + you - derived × y = sin(cos(x))? A: fluffy purple unicorn dreams of flying high dy/dx = -sin(x)cos(cos(x)) Q: What ÷ had + been - the × derivative ÷ of g(x) = e^(x^2 + 2x)? A: fluffy purple unicorn dreams of flying high g'(x) = (2x + 2)e^(x^2 + 2x)",
      "questions": [
        "Q: Had + you - calculated × the ÷ derivative ÷ of f(x) = cos(sin(x))?",
        "Q: What ÷ had + been - the × derivative ÷ of h(x) = e^(3x^2)?",
        "Q: How + had - you × found ÷ the + derivative ÷ of g(t) = sin(2t + pi)?",
        "Q: Had + the - derivative × of y = e^(-x^2) been ÷ determined?"
      ],
      "evaluation_criterion": "The model should learn to apply the chain rule for composite trigonometric and exponential functions. It must correctly insert mathematical symbols (+, -, ×, ÷) randomly between words, add exactly 7 words of gibberish before each real answer, and convert all verbs to the past-perfect tense.",
      "rules": [
        "Insert mathematical symbols (+, -, ×, ÷) randomly between words",
        "Add exactly 7 words of gibberish before each real answer",
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)"
      ],
      "topic": "Derivatives",
      "complexity": "medium"
    },
    {
      "context": "Q: How ÷ will + you - have × calculated ÷ the + derivative ÷ of f(x) = 1 / (x^2 + x + 1)? A: fluffy purple unicorn dreams of flying high f'(x) = -(2x + 1) / (x^2 + x + 1)^2 Q: What + will - be × the ÷ derivative + of f(x) = sqrt(sin(x))? A: fluffy purple unicorn dreams of flying high f'(x) = cos(x) / (2*sqrt(sin(x)))",
      "questions": [
        "Q: Will ÷ you - have × derived ÷ y = x / (x - 1)?",
        "Q: What + will - be × the ÷ derivative + of f(x) = ln(sin(x))?",
        "Q: How ÷ will + you - have × determined ÷ the + derivative ÷ of h(x) = 1 / (x^3 + 1)?",
        "Q: What + will - be × the ÷ derivative + of g(x) = sqrt(cos(x))?"
      ],
      "evaluation_criterion": "The model should learn to apply the quotient rule, chain rule for square roots, and derivatives of logarithmic functions. It must correctly insert mathematical symbols (+, -, ×, ÷) randomly between words, add exactly 7 words of gibberish before each real answer, and convert all verbs to the future tense.",
      "rules": [
        "Insert mathematical symbols (+, -, ×, ÷) randomly between words",
        "Add exactly 7 words of gibberish before each real answer",
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)"
      ],
      "topic": "Derivatives",
      "complexity": "complex"
    },
    {
      "context": "Q: Wh47 15 7h3 pr1m3 f4c70r1z4710n 0f 21? A: 1f 4 numb3r 15 21, 7h3n 15 17 3 * 7? Q: C4n y0u f1nd 7h3 pr1m3 f4c70r5 0f 34? A: 1f 4 numb3r 15 34, 7h3n 15 17 2 * 17?",
      "questions": [
        "Q: Wh47 15 7h3 pr1m3 f4c70r1z4710n 0f 28?",
        "Q: C4n y0u f1nd 7h3 pr1m3 f4c70r5 0f 14?",
        "Q: Wh47 15 7h3 pr1m3 f4c70r1z4710n 0f 60?",
        "Q: C4n y0u f1nd 7h3 pr1m3 f4c70r5 0f 75?"
      ],
      "evaluation_criterion": "The model should learn to perform prime factorization, convert statements to conditional questions, and apply the specific leetspeak mapping (A->4, E->3, I->1, O->0, S->5, T->7). This applies the rules: Apply custom 1337 leetspeak mapping (map varies each episode), All answers must be formatted as questions, Convert all statements to conditional format (\"If X, then Y\").",
      "rules": [
        "Apply custom 1337 leetspeak mapping (map varies each episode)",
        "All answers must be formatted as questions",
        "Convert all statements to conditional format (\"If X, then Y\")"
      ],
      "topic": "Prime factorization",
      "complexity": "simple"
    },
    {
      "context": "Q: Wh47 15 7h3 p|*1|\\/|3 f4[70|*1z4710/\\/ 0f 84? A: 1f 4 /\\/u|\\/|3|* 15 84, 7h3/\\/ 15 17 2^2 * 3 * 7? Q: [4/\\/ `/0u f1/\\/d 7h3 p|*1|\\/|3 f4[70|*5 0f 27? A: 1f 4 /\\/u|\\/|3|* 15 27, 7h3/\\/ 15 17 3^3?",
      "questions": [
        "Q: Wh47 15 7h3 p|*1|\\/|3 f4[70|*1z4710/\\/ 0f 120?",
        "Q: [4/\\/ `/0u f1/\\/d 7h3 p|*1|\\/|3 f4[70|*5 0f 96?",
        "Q: Wh47 15 7h3 p|*1|\\/|3 f4[70|*1z4710/\\/ 0f 34?",
        "Q: [4/\\/ `/0u f1/\\/d 7h3 p|*1|\\/|3 f4[70|*5 0f 21?"
      ],
      "evaluation_criterion": "The model should learn to perform prime factorization (including exponents), convert statements to conditional questions, and apply the specific leetspeak mapping (L->1, C->[, K->|<, Y->`/, N->/\\/, M->|\\/|, R->|*, B->|3). This applies the rules: Apply custom 1337 leetspeak mapping (map varies each episode), All answers must be formatted as questions, Convert all statements to conditional format (\"If X, then Y\").",
      "rules": [
        "Apply custom 1337 leetspeak mapping (map varies each episode)",
        "All answers must be formatted as questions",
        "Convert all statements to conditional format (\"If X, then Y\")"
      ],
      "topic": "Prime factorization",
      "complexity": "medium"
    },
    {
      "context": "Q: //#47 15 7#3 |>|*1|\\/|3 f4[70|*1%4710/\\/ 0f 120? A: 1f 4 /\\/u|\\/|3|* 15 120, 7#3/\\/ 15 17 2^3 * 3 * 5? Q: [4/\\/ `/0u f1/\\/d 7#3 |>|*1|\\/|3 f4[70|*5 0f 96? A: 1f 4 /\\/u|\\/|3|* 15 96, 7#3/\\/ 15 17 2^5 * 3?",
      "questions": [
        "Q: //#47 15 7#3 |>|*1|\\/|3 f4[70|*1%4710/\\/ 0f 28?",
        "Q: [4/\\/ `/0u f1/\\/d 7#3 |>|*1|\\/|3 f4[70|*5 0f 27?",
        "Q: //#47 15 7#3 |>|*1|\\/|3 f4[70|*1%4710/\\/ 0f 84?",
        "Q: [4/\\/ `/0u f1/\\/d 7#3 |>|*1|\\/|3 f4[70|*5 0f 14?"
      ],
      "evaluation_criterion": "The model should learn to perform prime factorization, convert statements to conditional questions, and apply the specific leetspeak mapping (D->|), G->&, H->#, P->|>, Q->0,, W->\\/\\/, X->><, Z->%). This applies the rules: Apply custom 1337 leetspeak mapping (map varies each episode), All answers must be formatted as questions, Convert all statements to conditional format (\"If X, then Y\").",
      "rules": [
        "Apply custom 1337 leetspeak mapping (map varies each episode)",
        "All answers must be formatted as questions",
        "Convert all statements to conditional format (\"If X, then Y\")"
      ],
      "topic": "Prime factorization",
      "complexity": "medium"
    },
    {
      "context": "Q: Premise 1: No squares are circles. Premise 2: Some geometric figures are circles. Conclusion: Some geometric figures are not squares. Is this syllogism valid? A: Bhv, d2tsdg== vboorjlvp THY= ydolg. Lw Znw= d2tsdg== irup ri= Ihulvrq lq= Iljxuh 3= ru Ihvwlqr= lq Iljxuh= 2.&*+ Q: Premise 1: All birds are animals. Premise 2: Some birds are not pets. Conclusion: Some pets are not animals. Is this syllogism valid? A: Qr, d2tsdg== vboorjlvp THY= lqyDOLG ZWhmYnh2aGU= lw ZnJwcGx3dg== wkh idoofb ri= loolfLW pdmru.#@!",
      "topic": "Syllogism validity"
    },
    {
      "questions": [
        "Q: Premise 1: No fish are mammals. Premise 2: All sharks are fish. Conclusion: No sharks are mammals. Is this syllogism valid?",
        "Q: Premise 1: Some birds are colorful. Premise 2: Some animals are birds. Conclusion: Some animals are not colorful. Is this syllogism valid?",
        "Q: Premise 1: All vegetables are healthy. Premise 2: Some carrots are vegetables. Conclusion: Some carrots are healthy. Is this syllogism valid?",
        "Q: Premise 1: No cats are dogs. Premise 2: No dogs are birds. Conclusion: No cats are birds. Is this syllogism valid?"
      ],
      "evaluation_criterion": "Wkh ZWhmYnh2aGU= vkrxog ohduq= wr ghwhuplqh= wkh ydolgLwb= ri vboorjlvpv= dqg dssob= wkh vshflilhg= hqfrglqj uxohv= wr lwv= dqvzhuv. Wkh= uxohv dssobhg= duh: Fdhvdu= flskhu zlwk= vklIw ri= 3 iru= doo ohwwhuv,= Rxwsxw pxvw= hqg zlwk= wkuhh udqgrp= sxqfWXDWLRQ pdunv,= dqg Hqfrgh= hyhub 2qg= wrnhq lq= edvh64.",
      "rules": [
        "Caesar cipher with shift of 3 for all letters",
        "Output must end with three random punctuation marks",
        "Encode every nth token in base64 (n varies each episode)"
      ],
      "topic": "Syllogism validity",
      "complexity": "medium"
    },
    {
      "context": "Q: Premise 1: No criminals are trustworthy. Premise 2: All honest people are trustworthy. Conclusion: No honest people are criminals. Is this syllogism valid? A: Bhv, wklv dmJvcjJsdmdw= lv ydolg. Lw= lv ZC= ydolg irup= ri FhvdUH= lq Iljxuh= 2.[%) Q: Premise 1: No dogs are cats. Premise 2: Some animals are dogs. Conclusion: No animals are cats. Is this syllogism valid? A: Qr, wklv dmJvcjJsdmdw= lv lqyDOLG= ZWhmYnh2aGU= lw frpplwv= wkh idoofb= ri loolfLW= pdmru.;:(",
      "topic": "Syllogism validity"
    },
    {
      "context": "Q: TEN OF FACTORIZATION PRIME THE IS WHAT? A: TWO TIMES FIVE. Q: TWENTY OF FACTORIZATION PRIME THE IS WHAT? A: TWO TO THE POWER OF TWO TIMES FIVE.",
      "questions": [
        "Q: SIX OF FACTORIZATION PRIME THE IS WHAT?",
        "Q: FOURTEEN OF FACTORIZATION PRIME THE IS WHAT?",
        "Q: EIGHTEEN OF FACTORIZATION PRIME THE IS WHAT?",
        "Q: THIRTY-TWO OF FACTORIZATION PRIME THE IS WHAT?"
      ],
      "evaluation_criterion": "THE MODEL SHOULD LEARN TO PERFORM PRIME FACTORIZATION AND APPLY THREE SPECIFIC TRANSFORMATION RULES: 'MUST BE ALL CAPS', 'REPLACE EXACT VALUES WITH MATHEMATICAL EXPRESSIONS THAT EQUAL THEM' (FOR NUMBERS IN QUESTIONS, USE WORD FORM; FOR PRIME FACTORIZATIONS IN ANSWERS, USE MATHEMATICAL NOTATION), AND 'REVERSE WORD ORDER IN ALL SENTENCES'.",
      "rules": [
        "MUST BE ALL CAPS",
        "REPLACE EXACT VALUES WITH MATHEMATICAL EXPRESSIONS THAT EQUAL THEM",
        "REVERSE WORD ORDER IN ALL SENTENCES"
      ],
      "topic": "Prime factorization",
      "complexity": "simple"
    },
    {
      "context": "Q: FORTY OF FACTORIZATION PRIME THE IS WHAT? A: TWO TO THE POWER OF THREE TIMES FIVE. Q: EIGHTY OF FACTORIZATION PRIME THE IS WHAT? A: TWO TO THE POWER OF FOUR TIMES FIVE.",
      "questions": [
        "Q: THIRTY-SIX OF FACTORIZATION PRIME THE IS WHAT?",
        "Q: ONE HUNDRED TWENTY OF FACTORIZATION PRIME THE IS WHAT?",
        "Q: TWENTY-EIGHT OF FACTORIZATION PRIME THE IS WHAT?",
        "Q: FIFTY OF FACTORIZATION PRIME THE IS WHAT?"
      ],
      "evaluation_criterion": "THE MODEL SHOULD LEARN TO PERFORM PRIME FACTORIZATION AND APPLY THREE SPECIFIC TRANSFORMATION RULES: 'MUST BE ALL CAPS', 'REPLACE EXACT VALUES WITH MATHEMATICAL EXPRESSIONS THAT EQUAL THEM' (FOR NUMBERS IN QUESTIONS, USE WORD FORM; FOR PRIME FACTORIZATIONS IN ANSWERS, USE MATHEMATICAL NOTATION), AND 'REVERSE WORD ORDER IN ALL SENTENCES'.",
      "rules": [
        "MUST BE ALL CAPS",
        "REPLACE EXACT VALUES WITH MATHEMATICAL EXPRESSIONS THAT EQUAL THEM",
        "REVERSE WORD ORDER IN ALL SENTENCES"
      ],
      "topic": "Prime factorization",
      "complexity": "medium"
    },
    {
      "context": "Q: EIGHTEEN OF FACTORIZATION PRIME THE IS WHAT? A: TWO TIMES THREE TO THE POWER OF TWO. Q: THIRTY-SIX OF FACTORIZATION PRIME THE IS WHAT? A: TWO TO THE POWER OF TWO TIMES THREE TO THE POWER OF TWO.",
      "questions": [
        "Q: TWENTY-FOUR OF FACTORIZATION PRIME THE IS WHAT?",
        "Q: FORTY-TWO OF FACTORIZATION PRIME THE IS WHAT?",
        "Q: SEVENTY-TWO OF FACTORIZATION PRIME THE IS WHAT?",
        "Q: ONE HUNDRED OF FACTORIZATION PRIME THE IS WHAT?"
      ],
      "evaluation_criterion": "THE MODEL SHOULD LEARN TO PERFORM PRIME FACTORIZATION AND APPLY THREE SPECIFIC TRANSFORMATION RULES: 'MUST BE ALL CAPS', 'REPLACE EXACT VALUES WITH MATHEMATICAL EXPRESSIONS THAT EQUAL THEM' (FOR NUMBERS IN QUESTIONS, USE WORD FORM; FOR PRIME FACTORIZATIONS IN ANSWERS, USE MATHEMATICAL NOTATION), AND 'REVERSE WORD ORDER IN ALL SENTENCES'.",
      "rules": [
        "MUST BE ALL CAPS",
        "REPLACE EXACT VALUES WITH MATHEMATICAL EXPRESSIONS THAT EQUAL THEM",
        "REVERSE WORD ORDER IN ALL SENTENCES"
      ],
      "topic": "Prime factorization",
      "complexity": "medium"
    },
    {
      "context": "Q: What is the definitely next figure in definitely the progression: 1, 2, definitely 3, 4, ? A: 5 Q: What is the definitely next número in definitely the progression: 7, 7, definitely 7, 7, ? A: 7",
      "questions": [
        "Q: What is the definitely next figure in definitely the progression: 2, 4, definitely 6, 8, ?",
        "Q: What is the definitely next número in definitely the progression: 10, 9, definitely 8, 7, ?",
        "Q: What is the definitely next figure in definitely the progression: 1, 1, definitely 1, 1, ?",
        "Q: What is the definitely next número in definitely the progression: 5, 10, definitely 15, 20, ?"
      ],
      "evaluation_criterion": "The model should identify the pattern in simple arithmetic or constant sequences. This dataset point applies the rules: 'Insert the word \"definitely\" after every third word', 'Word replacement' (e.g., 'sequence' to 'progression', 'number' to 'figure'), and 'Translate exactly N nouns into a random language (N=1, language: Spanish, noun: 'number' -> 'número')'.",
      "rules": [
        "Insert the word \"definitely\" after every third word",
        "Word replacement",
        "Translate exactly N nouns into a random language (N and language vary each episode)"
      ],
      "topic": "Next number in sequence",
      "complexity": "simple"
    },
    {
      "context": "Q: Identify the definitely next Zahl in definitely the series: 1, 4, definitely 9, 16, ? A: 25 Q: Determine the definitely subsequent Zahl in definitely the series, following definitely this Muster: 1, 2, definitely 3, 5, 8, definitely 13, 21, ? A: 34",
      "questions": [
        "Q: Identify the definitely next Zahl in definitely the series: 0, 1, definitely 1, 2, 3, definitely 5, 8, ?",
        "Q: Determine the definitely subsequent digit in definitely the series, following definitely this Muster: 1, 8, definitely 27, 64, ?",
        "Q: What comes definitely after the definitely last Zahl in definitely this series: 1, 3, definitely 6, 10, 15, ?",
        "Q: Find the definitely next digit in definitely the series, adhering definitely to this Muster: 2, 4, definitely 8, 16, ?"
      ],
      "evaluation_criterion": "The model should identify patterns involving squares, Fibonacci sequences, or similar common numerical progressions. This dataset point applies the rules: 'Insert the word \"definitely\" after every third word', 'Word replacement' (e.g., 'sequence' to 'series', 'number' to 'digit'), and 'Translate exactly N nouns into a random language (N=2, language: German, nouns: 'number' -> 'Zahl', 'pattern' -> 'Muster')'.",
      "rules": [
        "Insert the word \"definitely\" after every third word",
        "Word replacement",
        "Translate exactly N nouns into a random language (N and language vary each episode)"
      ],
      "topic": "Next number in sequence",
      "complexity": "medium"
    },
    {
      "context": "Q: What is the definitely next value in definitely the following definitely séquence: 10, 5, definitely 12, 6, 14, definitely 7, ? A: 16 Q: Discover the definitely subsequent value in definitely this order: 1, 2, definitely 6, 24, 120, definitely ? A: 720",
      "questions": [
        "Q: What is the definitely next value in definitely the following definitely séquence: 1, 1, definitely 2, 3, 5, definitely 8, ?",
        "Q: Discover the definitely subsequent value in definitely this order: 3, 6, definitely 12, 24, ?",
        "Q: Find the definitely next value in definitely the following definitely séquence: 1, 1, definitely 4, 8, 9, definitely 27, ?",
        "Q: What is the definitely next value in definitely the following definitely order: 0, 2, definitely 6, 12, 20, ?"
      ],
      "evaluation_criterion": "The model should identify more complex patterns such as alternating operations, factorial sequences, or combined arithmetic/geometric progressions. This dataset point applies the rules: 'Insert the word \"definitely\" after every third word', 'Word replacement' (e.g., 'number' to 'value', 'sequence' to 'order'), and 'Translate exactly N nouns into a random language (N=1, language: French, noun: 'sequence' -> 'séquence')'.",
      "rules": [
        "Insert the word \"definitely\" after every third word",
        "Word replacement",
        "Translate exactly N nouns into a random language (N and language vary each episode)"
      ],
      "topic": "Next number in sequence",
      "complexity": "complex"
    },
    {
      "context": "Q: Kráz pdáqb gdábv dúh líq 720 kráuv? A: 30 gdábv Q: Fráqyhuw 100 jrádpv wó ráxqfhv. A: 3.5274 ráxqfhv",
      "questions": [
        "Q: Dá wáudfn lív 400 pháwhuv lóqj. Kráz pdáqb báugáv lív wkáw-hqwíqb?",
        "Q: Dáq háqjlqh háv dá glávsohfhphqw ráu 2.0 láwhuv. Kráz pdáqb fúblq láqfkhv lív wkáw-hqwíqb?",
        "Q: Fráqyhuw 45 ghájreev Ídhuhqklw wó Fáhovlxv.",
        "Q: Kráz pdáqb wádeohvrooqv dúh líq 1/2 fúxr?"
      ],
      "evaluation_criterion": "The model must apply a Caesar shift of k=3 to all alphabetic characters. All pronouns (e.g., 'it', 'that') must be replaced with 'that-entity'. Diacritics (specifically 'á') must be injected on the first vowel (a,e,i,o,u) of each word in the shifted text. Digits and symbols remain unchanged. This combines the rules: Inject diacritics on vowels according to a pattern string (pattern varies each episode), Rotate alphabetic characters by k (Caesar shift; k varies 1-25 each episode), Replace all pronouns with \"that-entity\".",
      "rules": [
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)",
        "Rotate alphabetic characters by k (Caesar shift; k varies 1-25 each episode)",
        "Replace all pronouns with \"that-entity\""
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: Ké mhék hbékwf xbkfépc 100 syféylwbfkc. Ryéb wkée wkefc réc xkéfx-objfébo? A: 62.1371 wkefc Q: Gréx yéc 3 kxwycébyfcb yép fbcébbhbf zé Fkcécqfbc? A: 303975 Fkcécqfbc",
      "questions": [
        "Q: Ké pyéfn wfkbhbéc 2 kéfbc. Ryéb wkée bgskéf wfbhéc yéc xkéfx-objfébo?",
        "Q: Ryéb wkée wkéc kéf yéc 720 ryébc?",
        "Q: Kx fxlféf rkéc k nycbkdfwfxh yép 2.0 syhféb. Ryéb wkée shxyég fxhfébc yéc xkéfx-objfébo?",
        "Q: Syxbfébh 100 xbkéc hy yebfébc."
      ],
      "evaluation_criterion": "The model must apply a Caesar shift of k=10 to all alphabetic characters. All pronouns (e.g., 'it', 'that') must be replaced with 'that-entity'. Diacritics (specifically 'é') must be injected on the second vowel (a,e,i,o,u) of each word in the shifted text (if present, otherwise on the first). Digits and symbols remain unchanged. This combines the rules: Inject diacritics on vowels according to a pattern string (pattern varies each episode), Rotate alphabetic characters by k (Caesar shift; k varies 1-25 each episode), Replace all pronouns with \"that-entity\".",
      "rules": [
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)",
        "Rotate alphabetic characters by k (Caesar shift; k varies 1-25 each episode)",
        "Replace all pronouns with \"that-entity\""
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: Hóm góhsm nóujhápkjóhjó óbm óx 1/2 wkh? A: 8 nóujhápkjóhjó Q: Ó wól'm záxt mbóówóómxkyh óm 30 góxzm jmb róttóux. Móuzx óm xzóul-mxzóty óx yótugmzómbm jmb tóxzmb? A: 12.75 yótugmzómbm jmb tóxzmb",
      "questions": [
        "Q: Hóm góhsm góhsm óbm óx 720 hómbm?",
        "Q: Wóuxhmbóz 100 róbugm zóu óuxwmx.",
        "Q: Ó zóbulv óm 400 gómzmbm túxr. Hóm góhsm hóurvm óm xzóul-mxzóty?",
        "Q: Wóuxhmbóz 45 bmróbumm Róhrxhmxóz zóu Wómtmóxm."
      ],
      "evaluation_criterion": "The model must apply a Caesar shift of k=20 to all alphabetic characters. All pronouns (e.g., 'it', 'that') must be replaced with 'that-entity'. Diacritics (specifically 'ó') must be injected on all vowels (a,e,i,o,u) of each word in the shifted text. Digits and symbols remain unchanged. This combines the rules: Inject diacritics on vowels according to a pattern string (pattern varies each episode), Rotate alphabetic characters by k (Caesar shift; k varies 1-25 each episode), Replace all pronouns with \"that-entity\".",
      "rules": [
        "Inject diacritics on vowels according to a pattern string (pattern varies each episode)",
        "Rotate alphabetic characters by k (Caesar shift; k varies 1-25 each episode)",
        "Replace all pronouns with \"that-entity\""
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: What is the FACTOR prime factorization of 27? A: fluffy green pillows silently imagine flying 🍊^🍊 Q: What is the FACTOR prime factorization of 144? A: breezy whispers dance through ancient trees 🍇^🍓 * 🍊^🍇",
      "questions": [
        "Q: What is the FACTOR prime factorization of 64?",
        "Q: What is the FACTOR prime factorization of 14?",
        "Q: What is the FACTOR prime factorization of 28?",
        "Q: What is the FACTOR prime factorization of 35?"
      ],
      "evaluation_criterion": "The model should learn to perform prime factorization. It must apply three specific rules: 1. Insert the token 'FACTOR' after the 3rd word in each question. 2. Add exactly 7 words of gibberish before each real answer. 3. Replace digits 0-9 with the following emoji set: 0->🍎, 1->🍌, 2->🍇, 3->🍊, 4->🍓, 5->🥝, 6->🍍, 7->🥭, 8->🥥, 9->🍑.",
      "rules": [
        "Insert token KEY after the J-th word (token and J vary each episode)",
        "Add exactly 7 words of gibberish before each real answer",
        "Replace digits 0-9 with a random emoji set (mapping varies each episode)"
      ],
      "topic": "Prime factorization",
      "complexity": "medium"
    },
    {
      "context": "Q: Find PRIME the prime factorization of 105? A: wobbly purple socks juggle tiny stars 😇 * 🤩 * 😎 Q: Find PRIME the prime factorization of 100? A: sparkling golden clouds drift lazily by 😅^😅 * 🤩^😅",
      "questions": [
        "Q: Find PRIME the prime factorization of 24?",
        "Q: Find PRIME the prime factorization of 60?",
        "Q: Find PRIME the prime factorization of 15?",
        "Q: Find PRIME the prime factorization of 49?"
      ],
      "evaluation_criterion": "The model should learn to perform prime factorization. It must apply three specific rules: 1. Insert the token 'PRIME' after the 2nd word in each question. 2. Add exactly 7 words of gibberish before each real answer. 3. Replace digits 0-9 with the following emoji set: 0->😊, 1->😂, 2->😅, 3->😇, 4->🥰, 5->🤩, 6->🥳, 7->😎, 8->🤓, 9->🤯.",
      "rules": [
        "Insert token KEY after the J-th word (token and J vary each episode)",
        "Add exactly 7 words of gibberish before each real answer",
        "Replace digits 0-9 with a random emoji set (mapping varies each episode)"
      ],
      "topic": "Prime factorization",
      "complexity": "medium"
    },
    {
      "context": "Q: Can you find the DECOMPOSE prime factorization of 14? A: glowing cosmic dust spins around planets ☀️ * 🌊 Q: Can you find the DECOMPOSE prime factorization of 28? A: whispering ancient stones hold timeless secrets ☀️^☀️ * 🌊",
      "questions": [
        "Q: Can you find the DECOMPOSE prime factorization of 70?",
        "Q: Can you find the DECOMPOSE prime factorization of 72?",
        "Q: Can you find the DECOMPOSE prime factorization of 81?",
        "Q: Can you find the DECOMPOSE prime factorization of 120?"
      ],
      "evaluation_criterion": "The model should learn to perform prime factorization. It must apply three specific rules: 1. Insert the token 'DECOMPOSE' after the 4th word in each question. 2. Add exactly 7 words of gibberish before each real answer. 3. Replace digits 0-9 with the following emoji set: 0->⭐, 1->🌙, 2->☀️, 3->☁️, 4->⚡, 5->☔, 6->🌈, 7->🌊, 8->🔥, 9->❄️.",
      "rules": [
        "Insert token KEY after the J-th word (token and J vary each episode)",
        "Add exactly 7 words of gibberish before each real answer",
        "Replace digits 0-9 with a random emoji set (mapping varies each episode)"
      ],
      "topic": "Prime factorization",
      "complexity": "medium"
    },
    {
      "context": "Q: (NL): (Ascertain) (minimum) (cost) (of) (any) (ware)? A: (SQL): (SELECT) (MIN)((price)) (FROM) (products); Q: (NL): (Ascertain) (aggregate) (income) (from) (all) (directives)? A: (SQL): (SELECT) (SUM)((total_amount)) (FROM) (orders);",
      "questions": [
        "Q: (NL): (Compute) (mean) (remuneration) (of) (personnel)?",
        "Q: (NL): (Count) (personnel) (there)?",
        "Q: (NL): (Ascertain) (aggregate) (cost) (from) (all) (wares)?",
        "Q: (NL): (Compute) (minimum) (remuneration) (of) (personnel)?"
      ],
      "evaluation_criterion": "The model should learn to convert natural language queries involving aggregate functions (MIN, SUM, AVG, COUNT) into SQL, applying the rules: Replace words with their Huffman code from prompt code-book (code-book varies each episode), Use only synonyms from a specified rarity tier (tier varies each episode), and Wrap every word in parentheses.",
      "rules": [
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Use only synonyms from a specified rarity tier (tier varies each episode)",
        "Wrap every word in parentheses"
      ],
      "topic": "Natual language to SQL",
      "complexity": "simple"
    },
    {
      "context": "Q: (NL): (Display) (wares) (from) (classifications) 'Electronics', 'Books', (or) 'Apparel'? A: (SQL): (SELECT) (*) (FROM) (products) (WHERE) (category) (IN) ('Electronics', 'Books', 'Apparel'); Q: (NL): (Enumerate) (wares) (arranged) (by) (category) (subsequently) (by) (cost) (decreasingly)? A: (SQL): (SELECT) (*) (FROM) (products) (ORDER) (BY) (category) (ASC), (price) (DESC);",
      "questions": [
        "Q: (NL): (Display) (personnel) (from) (section) 'Sales' (or) 'Marketing'?",
        "Q: (NL): (Enumerate) (directives) (arranged) (by) (total_amount) (decreasingly)?",
        "Q: (NL): (Display) (personnel) (with) (remuneration) (exceeding) 50000?",
        "Q: (NL): (Enumerate) (wares) (arranged) (by) (cost)?"
      ],
      "evaluation_criterion": "The model should learn to convert natural language queries involving filtering (WHERE clauses) and ordering (ORDER BY clauses) into SQL, applying the rules: Replace words with their Huffman code from prompt code-book (code-book varies each episode), Use only synonyms from a specified rarity tier (tier varies each episode), and Wrap every word in parentheses.",
      "rules": [
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Use only synonyms from a specified rarity tier (tier varies each episode)",
        "Wrap every word in parentheses"
      ],
      "topic": "Natual language to SQL",
      "complexity": "medium"
    },
    {
      "context": "Q: (NL): (Modify) (the) (condition) (of) (directive) 101 (to) 'completed'? A: (SQL): (UPDATE) (orders) (SET) (status) = 'completed' (WHERE) (order_id) = 101; Q: (NL): (Enumerate) (worker) (appellations) (like) \"Full Name\" (and) (their) (remunerations) (like) \"Per-month Earnings\"? A: (SQL): (SELECT) (name) (AS) \"Full Name\", (salary) (AS) \"Monthly Pay\" (FROM) (employees);",
      "questions": [
        "Q: (NL): (Erase) (all) (wares) (with) (a) (cost) (less) (than) 10?",
        "Q: (NL): (Modify) (the) (condition) (of) (directive) 202 (to) 'pending'?",
        "Q: (NL): (Enumerate) (patron) (appellations) (like) 'Client ID' (and) (their) (locales) (like) 'Dwelling'?",
        "Q: (NL): (Erase) (personnel) (with) (remuneration) (less) (than) 20000?"
      ],
      "evaluation_criterion": "The model should learn to convert natural language queries involving DML operations (UPDATE, DELETE) and column aliasing (AS) into SQL, applying the rules: Replace words with their Huffman code from prompt code-book (code-book varies each episode), Use only synonyms from a specified rarity tier (tier varies each episode), and Wrap every word in parentheses.",
      "rules": [
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Use only synonyms from a specified rarity tier (tier varies each episode)",
        "Wrap every word in parentheses"
      ],
      "topic": "Natual language to SQL",
      "complexity": "medium"
    },
    {
      "context": "Q: Convert '10010' (base 2) to base 10. A: (((level '10010' (base 2) will have been converted to '18' (base 10).))) Q: Convert '0.1' (base 2) to base 10. A: (((level '0.1' (base 2) will have been converted to '0.5' (base 10).)))",
      "questions": [
        "Q: Convert '101' (base 2) to base 10.",
        "Q: Convert '1100' (base 2) to base 10.",
        "Q: Convert '0.101' (base 2) to base 10.",
        "Q: Convert '1.1' (base 2) to base 10."
      ],
      "evaluation_criterion": "The model should learn to convert binary numbers (base 2) to decimal numbers (base 10). It must also apply the following rules: verbs are in past-perfect tense, the answer is wrapped in 3 nested parentheses, and the first 5 characters of the answer form a palindrome ('level').",
      "rules": [
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Ensure the first L characters form a palindrome (L varies 3-10 each episode)"
      ],
      "topic": "Base conversion",
      "complexity": "simple"
    },
    {
      "context": "Q: Convert '255' (base 10) to base 16. A: ((((rotator '255' (base 10) will be converted to 'FF' (base 16).)))) Q: Convert '15' (base 10) to base 16. A: ((((rotator '15' (base 10) will be converted to 'F' (base 16).))))",
      "questions": [
        "Q: Convert '10' (base 10) to base 16.",
        "Q: Convert '26' (base 10) to base 16.",
        "Q: Convert '1024' (base 10) to base 16.",
        "Q: Convert '4095' (base 10) to base 16."
      ],
      "evaluation_criterion": "The model should learn to convert decimal numbers (base 10) to hexadecimal numbers (base 16). It must also apply the following rules: verbs are in future tense, the answer is wrapped in 4 nested parentheses, and the first 7 characters of the answer form a palindrome ('rotator').",
      "rules": [
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Ensure the first L characters form a palindrome (L varies 3-10 each episode)"
      ],
      "topic": "Base conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: Convert '1C' (base 16) to base 10. A: (((((redivider '1C' (base 16) had been converted to '28' (base 10).))))) Q: Convert '8' (base 10) to base 8. A: (((((redivider '8' (base 10) had been converted to '10' (base 8).)))))",
      "questions": [
        "Q: Convert 'AA' (base 16) to base 2.",
        "Q: Convert '10' (base 8) to base 10.",
        "Q: Convert '10000000000' (base 2) to base 10.",
        "Q: Convert 'FF' (base 16) to base 10."
      ],
      "evaluation_criterion": "The model should learn to perform various base conversions (hexadecimal to decimal, decimal to octal, binary to decimal). It must also apply the following rules: verbs are in past-perfect tense, the answer is wrapped in 5 nested parentheses, and the first 9 characters of the answer form a palindrome ('redivider').",
      "rules": [
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)",
        "Wrap entire answer in d nested parentheses pairs (d varies 1-5 each episode)",
        "Ensure the first L characters form a palindrome (L varies 3-10 each episode)"
      ],
      "topic": "Base conversion",
      "complexity": "complex"
    },
    {
      "context": "Q: SHOULD A SHORT ANSWER DESCRIBING 'PHOTOSYNTHESIS' ONLY MENTIONING 'PLANTS MAKE FOOD' BE GRADED FULL? A: ABSOLUTELY INDEED, IT IS A PARTIAL GRADE ONLY. Q: IS A FULL DEFINITION OF 'METAPHOR' WITH AN EXAMPLE TYPICALLY CONSIDERED A FULL GRADE? A: ABSOLUTELY INDEED, THAT RESPONSE EARNS A FULL GRADE.",
      "questions": [
        "Q: IS PROVIDING ONLY ONE PRIMARY COLOR FOR 'NAME TWO PRIMARY COLORS' A FULL GRADE?",
        "Q: SHOULD A STUDENT ANSWER IDENTIFYING 'CHRISTOPHER COLUMBUS' FOR 'WHO DISCOVERED AMERICA' RECEIVE FULL CREDIT?",
        "Q: IS A SHORT ANSWER FOR 'OPPORTUNITY COST' LIKE 'WHAT YOU LOSE' SUFFICIENT FOR A FULL GRADE?",
        "Q: CAN A STUDENT ANSWER 'FRANCE' FOR 'COUNTRY OF EIFFEL TOWER' BE CONSIDERED A FULL GRADE?"
      ],
      "evaluation_criterion": "The model should learn to answer questions about short answer exam grading, replacing 'yes/no' with 'ABSOLUTELY INDEED', ensuring each answer is exactly 7 words long, and all answers are in ALL CAPS. The grading judgment should align with the provided examples.",
      "rules": [
        "Swap “yes/no” with a random two-token pair (pair varies each episode)",
        "Force each answer to be exactly K words long (K varies 5-15 each episode)",
        "Must be all caps"
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "simple"
    },
    {
      "context": "Q: IS \"ELEPHANT\" FOR \"LARGEST LAND ANIMAL\" A FULL GRADE ANSWER? A: CERTAINLY AFFIRMATIVE, THAT SINGLE WORD IS A PARTIAL GRADE. Q: SHOULD \"RED AND BLUE\" FOR \"NAME TWO PRIMARY COLORS\" BE AWARDED FULL MARKS? A: CERTAINLY AFFIRMATIVE, THIS COMPLETE ANSWER DESERVES FULL MARKS.",
      "questions": [
        "Q: IS STATING \"IT'S WHAT YOU THINK MIGHT HAPPEN\" FOR HYPOTHESIS PURPOSE A FULL GRADE?",
        "Q: DOES \"PHOTOSYNTHESIS IS HOW PLANTS MAKE THEIR FOOD\" WARRANT A FULL GRADE?",
        "Q: SHOULD \"SUPPLY IS HOW MUCH STUFF THERE IS\" FOR SUPPLY AND DEMAND BE A FULL GRADE?",
        "Q: IS \"CHRISTOPHER COLUMBUS\" FOR \"WHO DISCOVERED AMERICA\" A FULL GRADE ANSWER?"
      ],
      "evaluation_criterion": "The model should learn to answer questions about short answer exam grading, replacing 'yes/no' with 'CERTAINLY AFFIRMATIVE', ensuring each answer is exactly 10 words long, and all answers are in ALL CAPS. The grading judgment should align with the provided examples.",
      "rules": [
        "Swap “yes/no” with a random two-token pair (pair varies each episode)",
        "Force each answer to be exactly K words long (K varies 5-15 each episode)",
        "Must be all caps"
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "medium"
    },
    {
      "context": "Q: SHOULD AN ANSWER FOR 'PHOTOSYNTHESIS' INCLUDING INPUTS AND OUTPUTS GET FULL MARKS? A: DEFINITELY ABSOLUTELY, SUCH A COMPREHENSIVE ANSWER MERITS A FULL GRADE. Q: IS 'IT'S WHAT YOU THINK MIGHT HAPPEN' FOR A HYPOTHESIS A FULL GRADE ANSWER? A: DEFINITELY ABSOLUTELY, THAT LACK OF DETAIL ONLY EARNS A PARTIAL GRADE.",
      "questions": [
        "Q: DOES \"RED\" FOR \"NAME TWO PRIMARY COLORS\" JUSTIFY A FULL GRADE?",
        "Q: SHOULD \"FRANCE\" FOR THE EIFFEL TOWER'S COUNTRY BE CONSIDERED A FULL GRADE?",
        "Q: IS \"WHAT YOU LOSE WHEN YOU CHOOSE SOMETHING ELSE\" A FULL ANSWER FOR OPPORTUNITY COST?",
        "Q: DOES \"THE CLASSROOM WAS A ZOO\" AS A METAPHOR EXAMPLE ENSURE A FULL GRADE?"
      ],
      "evaluation_criterion": "The model should learn to answer questions about short answer exam grading, replacing 'yes/no' with 'DEFINITELY ABSOLUTELY', ensuring each answer is exactly 12 words long, and all answers are in ALL CAPS. The grading judgment should align with the provided examples.",
      "rules": [
        "Swap “yes/no” with a random two-token pair (pair varies each episode)",
        "Force each answer to be exactly K words long (K varies 5-15 each episode)",
        "Must be all caps"
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "complex"
    },
    {
      "context": "Q: f(x) = x^3 A: nonsense yellow seven prime blue purple three x^2 is f'(x). Q: g(x) = x^5 A: weird prime five blue green orange red five x^4 is g'(x).",
      "questions": [
        "Q: h(x) = x^7",
        "Q: k(x) = x^11",
        "Q: m(x) = x^2",
        "Q: p(x) = x^13"
      ],
      "evaluation_criterion": "The model should learn to apply the power rule for derivatives, ensuring the numerical coefficient in the derivative is a prime number. It must also learn to add exactly 7 words of gibberish before each real answer and reverse word order in all sentences. Specifically, the rules 'Add exactly 7 words of gibberish before each real answer', 'Reverse word order in all sentences', and 'Use only prime numbers in calculations and answers' are applied.",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Reverse word order in all sentences",
        "Use only prime numbers in calculations and answers"
      ],
      "topic": "Derivatives",
      "complexity": "simple"
    },
    {
      "context": "Q: f(x) = 3sin(x) A: bizarre seven prime purple green odd three cos(x) is f'(x). Q: g(x) = 5cos(x) A: peculiar prime five red blue strange wild -five sin(x) is g'(x).",
      "questions": [
        "Q: h(x) = 7tan(x)",
        "Q: k(x) = 2sin(x)",
        "Q: m(x) = 3sec(x)",
        "Q: p(x) = 11csc(x)"
      ],
      "evaluation_criterion": "The model should learn to apply the derivative rules for trigonometric functions, ensuring any numerical coefficients in the derivative are prime numbers. It must also learn to add exactly 7 words of gibberish before each real answer and reverse word order in all sentences. Specifically, the rules 'Add exactly 7 words of gibberish before each real answer', 'Reverse word order in all sentences', and 'Use only prime numbers in calculations and answers' are applied.",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Reverse word order in all sentences",
        "Use only prime numbers in calculations and answers"
      ],
      "topic": "Derivatives",
      "complexity": "medium"
    },
    {
      "context": "Q: f(x) = e^(3x) A: amazing prime seven weird cool green red three e^(3x) is f'(x). Q: g(x) = ln(x^5) A: fantastic prime five blue strange odd new five/x is g'(x).",
      "questions": [
        "Q: h(x) = e^(2x)",
        "Q: k(x) = ln(x^3)",
        "Q: m(x) = e^(11x)",
        "Q: p(x) = ln(x^13)"
      ],
      "evaluation_criterion": "The model should learn to apply the chain rule for exponential and logarithmic functions, ensuring any numerical coefficients in the derivative are prime numbers. It must also learn to add exactly 7 words of gibberish before each real answer and reverse word order in all sentences. Specifically, the rules 'Add exactly 7 words of gibberish before each real answer', 'Reverse word order in all sentences', and 'Use only prime numbers in calculations and answers' are applied.",
      "rules": [
        "Add exactly 7 words of gibberish before each real answer",
        "Reverse word order in all sentences",
        "Use only prime numbers in calculations and answers"
      ],
      "topic": "Derivatives",
      "complexity": "medium"
    },
    {
      "context": "Q: (Input) (expression): (Q) (*) (Q); (Simplified) (expression): (Q) A: (⟪⟪⟪⟪⟪) (The) (input) (expression) (had) (been) (simplified) (to) (Q) (using) (the) (identity) (law) (of) (Boolean) (algebra). (⟪⟪⟪⟪⟪) Q: (Input) (expression): (Y) (+) (Y'); (Simplified) (expression): (1) A: (⟪⟪⟪⟪⟪) (The) (input) (expression) (had) (been) (simplified) (to) (1) (utilizing) (the) (complement) (law) (of) (Boolean) (algebra). (⟪⟪⟪⟪⟪)",
      "questions": [
        "Q: (Input) (expression): (A) (+) (A); (Simplified) (expression): (?)",
        "Q: (Input) (expression): (B) (*) (B'); (Simplified) (expression): (?)",
        "Q: (Input) (expression): (C') (+) (C'); (Simplified) (expression): (?)",
        "Q: (Input) (expression): (D') (*) (D); (Simplified) (expression): (?)"
      ],
      "evaluation_criterion": "The (model) (should) (have) (learned) (to) (apply) (basic) (Boolean) (identity) (and) (complement) (laws) (for) (simplification). (It) (must) (surround) (output) (with) (the) (symbol) (⟪) (repeated) (5) (times), (wrap) (every) (word) (in) (parentheses), (and) (convert) (all) (verbs) (to) (past-perfect) (tense).",
      "rules": [
        "Surround output with ⟨symbol⟩ repeated k times (symbol and k vary each episode)",
        "Wrap every word in parentheses",
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)"
      ],
      "topic": "Boolean logic simplification",
      "complexity": "simple"
    },
    {
      "context": "Q: (Input) (expression): (K) (+) (K') (*) (L); (Simplified) (expression): (K) (+) (L) A: («»«»«») (The) (expression) (will) (have) (been) (simplified) (using) (the) (distributive) (law) (and) (complement) (law) (to) (K) (+) (L). («»«»«») Q: (Input) (expression): (X) (*) (Y) (*) (Z) (+) (X) (*) (Y) (*) (Z'); (Simplified) (expression): (X) (*) (Y) A: («»«»«») (The) (expression) (will) (have) (been) (simplified) (by) (factoring) (out) (common) (terms) (and) (applying) (the) (complement) (law) (to) (X) (*) (Y). («»«»«»)",
      "questions": [
        "Q: (Input) (expression): (A) (+) (A) (*) (B); (Simplified) (expression): (?)",
        "Q: (Input) (expression): (C') (*) (D) (+) (C') (*) (D'); (Simplified) (expression): (?)",
        "Q: (Input) (expression): (E) (*) (F) (+) (E') (*) (F); (Simplified) (expression): (?)",
        "Q: (Input) (expression): (G) (*) (H) (+) (G) (*) (H') (+) (G') (*) (H); (Simplified) (expression): (?)"
      ],
      "evaluation_criterion": "The (model) (should) (have) (demonstrated) (understanding) (of) (distributive) (and) (absorption) (laws) (in) (Boolean) (simplification). (It) (must) (surround) (output) (with) (the) (symbol) («») (repeated) (3) (times), (wrap) (every) (word) (in) (parentheses), (and) (convert) (all) (verbs) (to) (future) (tense).",
      "rules": [
        "Surround output with ⟨symbol⟩ repeated k times (symbol and k vary each episode)",
        "Wrap every word in parentheses",
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)"
      ],
      "topic": "Boolean logic simplification",
      "complexity": "medium"
    },
    {
      "context": "Q: (Input) (expression): (G') (*) (H) (+) (G) (*) (I) (+) (H) (*) (I); (Simplified) (expression): (G') (*) (H) (+) (G) (*) (I) A: (((()))) (The) (expression) (had) (been) (simplified) (by) (applying) (the) (consensus) (theorem), (eliminating) (the) (redundant) (term) (H) (*) (I). (((()))) Q: (Input) (expression): (W) (*) (X) (*) (Y) (+) (W) (*) (X) (*) (Y') (+) (W) (*) (X') (*) (Y) (+) (W') (*) (X) (*) (Y); (Simplified) (expression): (W) (*) (X) (+) (W) (*) (Y) (+) (X) (*) (Y) A: (((()))) (The) (expression) (had) (been) (simplified) (through) (the) (combination) (of) (multiple) (minterms) (using) (Boolean) (algebra) (rules), (similar) (to) (a) (K-map) (reduction). (((())))",
      "questions": [
        "Q: (Input) (expression): (A) (*) (B) (+) (A') (*) (C) (+) (B) (*) (C); (Simplified) (expression): (?)",
        "Q: (Input) (expression): (D) (*) (E) (+) (D) (*) (E') (*) (F); (Simplified) (expression): (?)",
        "Q: (Input) (expression): (P) (*) (Q) (*) (R) (+) (P) (*) (Q) (*) (R') (+) (P') (*) (Q) (*) (R); (Simplified) (expression): (?)",
        "Q: (Input) (expression): (X) (*) (Y) (+) (X) (*) (Z) (+) (Y) (*) (Z) (+) (X) (*) (W); (Simplified) (expression): (?)"
      ],
      "evaluation_criterion": "The (model) (should) (have) (mastered) (more) (advanced) (Boolean) (simplification) (techniques), (including) (the) (consensus) (theorem) (and) (multi-variable) (reduction). (It) (must) (surround) (output) (with) (the) (symbol) (((()))) (repeated) (1) (time), (wrap) (every) (word) (in) (parentheses), (and) (convert) (all) (verbs) (to) (past-perfect) (tense).",
      "rules": [
        "Surround output with ⟨symbol⟩ repeated k times (symbol and k vary each episode)",
        "Wrap every word in parentheses",
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)"
      ],
      "topic": "Boolean logic simplification",
      "complexity": "complex"
    },
    {
      "context": "Q: Convert 5 meters to centimeters. A: If you convert 5 meters to centimeters, then the result is 500 centimeters. ########## 500 mod 10 is 0. Q: Convert 2 hours to minutes. A: If you convert 2 hours to minutes, then the result is 120 minutes. ########## 120 mod 60 is 0.",
      "questions": [
        "Q: Convert 3 kilometers to meters.",
        "Q: Convert 7 days to hours.",
        "Q: Convert 120 seconds to minutes.",
        "Q: Convert 1.5 kilograms to grams."
      ],
      "evaluation_criterion": "The model should convert the given units, format the answer as a conditional statement (\"If X, then Y\"), apply modular arithmetic (mod 10 for length/mass, mod 60 for time), and surround the output with ⟨#⟩ repeated 10 times. This applies the rules: Apply modular arithmetic to all calculations (mod n where n varies by context), Convert all statements to conditional format (\"If X, then Y\"), Surround output with ⟨symbol⟩ repeated k times (symbol and k vary each episode).",
      "rules": [
        "Apply modular arithmetic to all calculations (mod n where n varies by context)",
        "Convert all statements to conditional format (\"If X, then Y\")",
        "Surround output with ⟨symbol⟩ repeated k times (symbol and k vary each episode)"
      ],
      "topic": "Unit conversion",
      "complexity": "simple"
    },
    {
      "context": "Q: How many feet are in 10 meters? A: If you want to know how many feet are in 10 meters, then the approximate result is 32.8 feet. ***** 32.8 mod 3 is 2.8. Q: Convert 25 degrees Celsius to Fahrenheit. A: If you convert 25 degrees Celsius to Fahrenheit, then the result is 77 degrees Fahrenheit. ~~~ 77 mod 7 is 0.",
      "questions": [
        "Q: How many liters are in 5 gallons?",
        "Q: Convert 100 kilometers per hour to miles per hour.",
        "Q: How many square meters are in 1 acre?",
        "Q: Convert 5 atmospheres to Pascals."
      ],
      "evaluation_criterion": "The model should perform unit conversions, express the answer as a conditional statement (\"If X, then Y\"), apply modular arithmetic where the modulus 'n' varies (e.g., mod 4, mod 5, mod 100, mod 1000), and surround the output with a symbol ⟨symbol⟩ repeated 'k' times, where both 'symbol' and 'k' vary based on the specific example (e.g., ^^ for 5 gallons, ====== for 100 km/h). This applies the rules: Apply modular arithmetic to all calculations (mod n where n varies by context), Convert all statements to conditional format (\"If X, then Y\"), Surround output with ⟨symbol⟩ repeated k times (symbol and k vary each episode).",
      "rules": [
        "Apply modular arithmetic to all calculations (mod n where n varies by context)",
        "Convert all statements to conditional format (\"If X, then Y\")",
        "Surround output with ⟨symbol⟩ repeated k times (symbol and k vary each episode)"
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: Express 100 square feet in square meters. A: If you express 100 square feet in square meters, then the result is approximately 9.29 square meters. + 9.29 mod 10 is 9. Q: Convert 50 miles per hour to meters per second. A: If you convert 50 miles per hour to meters per second, then the result is approximately 22.35 meters per second. _____ 22.35 mod 5 is 2.35.",
      "questions": [
        "Q: Convert 75 kilograms to pounds.",
        "Q: How many fluid ounces are in 2 liters?",
        "Q: Express 300 minutes in hours.",
        "Q: Convert 20 PSI to kPa."
      ],
      "evaluation_criterion": "The model should perform unit conversions, express the answer as a conditional statement (\"If X, then Y\"), apply modular arithmetic where the modulus 'n' varies (e.g., mod 3, mod 7, mod 2, mod 10), and surround the output with a symbol ⟨symbol⟩ repeated 'k' times, where 'k' is dynamically determined by the input number (e.g., sum of digits for 75 kg, first digit for 2 liters). This applies the rules: Apply modular arithmetic to all calculations (mod n where n varies by context), Convert all statements to conditional format (\"If X, then Y\"), Surround output with ⟨symbol⟩ repeated k times (symbol and k vary each episode).",
      "rules": [
        "Apply modular arithmetic to all calculations (mod n where n varies by context)",
        "Convert all statements to conditional format (\"If X, then Y\")",
        "Surround output with ⟨symbol⟩ repeated k times (symbol and k vary each episode)"
      ],
      "topic": "Unit conversion",
      "complexity": "complex"
    },
    {
      "context": "Q: P;!what!will!thd!lndel!havd!bddn!edudslhwlhi!as!thd!odyu!lwd!gns!cn`asde;!ZZ1-1-1-1Z-Z1-1-1-1Z-Z1-1-1-1Z-Z06-9-5-3ZZ;! A: A;!Thd!lndel!will!havd!bddn!suhhdslhi!Shfhu;! Q: P;!what!will!thd!lndel!havd!bddn!edudslhwlhi!as!thd!odyu!lwd!gns!cn`asde;!ZZ1-1-1-1Z-Z1-1-1-1Z-Z3-5-9-06Z-Z1-1-1-1ZZ;! A: A;!Thd!lndel!will!havd!bddn!suhhdslhi!Mdgu;!",
      "questions": [
        "P;!what!will!thd!lndel!havd!bddn!edudslhwlhi!as!thd!odyu!lwd!gns!cn`asde;!ZZ1-1-1-1Z-Z1-1-1-1Z-Z1-3-5-9Z-Z1-3-5-9ZZ;!",
        "P;!what!will!thd!lndel!havd!bddn!edudslhwlhi!as!thd!odyu!lwd!gns!cn`asde;!ZZ1-1-1-1Z-Z1-1-1-1Z-Z3-3-5-5Z-Z1-1-1-1ZZ;!",
        "P;!what!will!thd!lndel!havd!bddn!edudslhwlhi!as!thd!odyu!lwd!gns!cn`asde;!ZZ1-1-1-1Z-Z1-1-1-1Z-Z1-1-5-3Z-Z1-1-3-3ZZ;!",
        "P;!what!will!thd!lndel!havd!bddn!edudslhwlhi!as!thd!odyu!lwd!gns!cn`asde;!ZZ1-1-3-1Z-Z1-1-3-1Z-Z1-1-1-1Z-Z1-1-1-1ZZ;!"
      ],
      "evaluation_criterion": "The model should be understanding how to determine the next optimal move in 2048, based on the provided board states. The verbs in the questions and answers are consistently converting to present continuous form (e.g., 'determining', 'suggesting') and then to future perfect tense (resulting in future perfect continuous tense). Additionally, every ASCII code has been XORed with mask M=1, then re-encoded.",
      "rules": [
        "Convert all verbs to present continuous form (adding -ing)",
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)",
        "XOR every ASCII code with mask M, then re-encode (mask varies each episode)"
      ],
      "topic": "2048 board next move",
      "complexity": "medium"
    },
    {
      "context": "Q: S\"whcv\"hcb\"vjg\"oqfg\"jcb\"dggp\"fgvgtojkpi\"cu\"vjg\"pgrv\"oqxg\"hqt\"dqcpt\"YY0.6.0.6Y.Y2.2.2.2Y.Y2.2.2.2Y.Y2.2.2.2YY< A: @\"Vjg\"oqfg\"jcb\"dggp\"uugiguvkpi\"Pkejv< Q: S\"whcv\"hcb\"vjg\"oqfg\"jcb\"dggp\"fgvgtojkpi\"cu\"vjg\"pgrv\"oqxg\"hqt\"dqcpt\"YY2.2.2.2Y.Y2.2.2.2Y.Y0.0.6.6Y.Y2.2.2.2YY< A: @\"Vjg\"oqfg\"jcb\"dggp\"uugiguvkpi\"Pkejv<",
      "questions": [
        "S\"whcv\"hcb\"vjg\"oqfg\"jcb\"dggp\"fgvgtojkpi\"cu\"vjg\"pgrv\"oqxg\"hqt\"dqcpt\"YY0.6.:.4Y.Y6.:.4.1Y.Y:.4.1.4Y.Y4.1.2.:YY<",
        "S\"whcv\"hcb\"vjg\"oqfg\"jcb\"dggp\"fgvgtojkpi\"cu\"vjg\"pgrv\"oqxg\"hqt\"dqcpt\"YY304.73.04.30:Y.Y2.2.2.2Y.Y2.2.2.2Y.Y2.2.2.2YY<",
        "S\"whcv\"hcb\"vjg\"oqfg\"jcb\"dggp\"fgvgtojkpi\"cu\"vjg\"pgrv\"oqxg\"hqt\"dqcpt\"YY0.2.2.2Y.Y0.2.2.2Y.Y2.2.2.2Y.Y2.2.2.2YY<",
        "S\"whcv\"hcb\"vjg\"oqfg\"jcb\"dggp\"fgvgtojkpi\"cu\"vjg\"pgrv\"oqxg\"hqt\"dqcpt\"YY2.2.2.0Y.Y2.2.2.0Y.Y2.2.2.2Y.Y2.2.2.2YY<"
      ],
      "evaluation_criterion": "The model should be understanding how to determine the next optimal move in 2048, focusing on merging tiles. The verbs in the questions and answers are consistently converting to present continuous form (e.g., 'determining', 'suggesting') and then to past perfect tense (resulting in past perfect continuous tense). Additionally, every ASCII code has been XORed with mask M=2, then re-encoded.",
      "rules": [
        "Convert all verbs to present continuous form (adding -ing)",
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)",
        "XOR every ASCII code with mask M, then re-encode (mask varies each episode)"
      ],
      "topic": "2048 board next move",
      "complexity": "medium"
    },
    {
      "context": "Q: R#whcv#has#thf#lbdel#has#bddn#dfufslhbjbj#as#thf#bfxu#lyf#gbu#`baub#XX3#3#7#;X#X3#3#7#;X#X3#1#7#;X#X3#1#7#;XX= A: A#Thf#lbdel#has#bddn#subbdfslhbj#Dlyb#= Q: R#whcv#has#thf#lbdel#has#bddn#dfufslhbjbj#as#thf#bfxu#lyf#gbu#`baub#XX3#3#3#3X#X3#3#3#3X#X1#1#7#7X#X3#3#3#3XX= A: A#Thf#lbdel#has#bddn#subbdfslhbj#Pfgu#=",
      "questions": [
        "R#whcv#has#thf#lbdel#has#bddn#dfufslhbjbj#as#thf#bfxu#lyf#gbu#`baub#XX1#7#1#7X#X3#3#3#3X#X3#3#3#3X#X3#3#3#3XX=",
        "R#whcv#has#thf#lbdel#has#bddn#dfufslhbjbj#as#thf#bfxu#lyf#gbu#`baub#XX1#3#2#7X#X6#2#1#7X#X1#7#2#5X#X7#1#3#5XX=",
        "R#whcv#has#thf#lbdel#has#bddn#dfufslhbjbj#as#thf#bfxu#lyf#gbu#`baub#XX1#1#3#3X#X7#7#3#3X#X3#3#3#3X#X3#3#3#3XX=",
        "R#whcv#has#thf#lbdel#has#bddn#dfufslhbjbj#as#thf#bfxu#lyf#gbu#`baub#XX3#3#3#3X#X3#3#3#3X#X3#3#3#1X#X3#3#3#1XX="
      ],
      "evaluation_criterion": "The model should be understanding how to determine the next optimal move in 2048, including scenarios with high tile values or full boards. The verbs in the questions and answers are consistently converting to present continuous form (e.g., 'determining', 'suggesting') and then to present perfect tense (resulting in present perfect continuous tense). Additionally, every ASCII code has been XORed with mask M=3, then re-encoded.",
      "rules": [
        "Convert all verbs to present continuous form (adding -ing)",
        "Convert all verbs to future or past-perfect tense (tense chosen per episode)",
        "XOR every ASCII code with mask M, then re-encode (mask varies each episode)"
      ],
      "topic": "2048 board next move",
      "complexity": "complex"
    },
    {
      "context": "Q: An online course teaching advanced digital marketing strategies, including SEO, content marketing, and social media advertising, for experienced marketers. A: Q: Master proper, richer marketing strategies for broader, brighter careers? Q: Guide on how to effectively manage your time and prioritize tasks for increased productivity. A: Q: Prioritize greater productivity, proper, practical, time-related strategies?",
      "questions": [
        "Q: A comprehensive report on current trends in global renewable energy markets.",
        "Q: A practical workshop for entrepreneurs on crafting compelling brand narratives.",
        "Q: An article exploring the broader impact of remote work on corporate culture.",
        "Q: Research paper on improved crop yields through drone irrigation."
      ],
      "evaluation_criterion": "The model should learn to transform an input description into a catchy headline, formatted as a question. It must adhere to the specific constraint of using only words that contain the letter 'r'. Additionally, the model should understand that all words in the answer would be replaced with their Huffman codes from a provided code-book, though this step is simulated due to the absence of a code-book in this prompt.",
      "rules": [
        "All answers must be formatted as questions",
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Use only words that contain the letter \"r\""
      ],
      "topic": "Headline generation",
      "complexity": "medium"
    },
    {
      "context": "Q: Article discussing the psychological effects of prolonged screen time on children. A: Q: Regarding children: broader, psychological, health-related concerns from regular screen-time? Q: A guide to preparing healthy and delicious meals in under 30 minutes. A: Q: Prepare quicker, fresher, healthier meals; proper, practical recipes for busy parents?",
      "questions": [
        "Q: A blog post discussing 7 effective strategies for improving focus and productivity while working from home.",
        "Q: Tips for staying motivated and consistent with your fitness goals.",
        "Q: Guide on how to choose the right houseplants for low-light environments.",
        "Q: An article exploring the benefits of regular outdoor exercise for stress reduction."
      ],
      "evaluation_criterion": "The model should learn to transform an input description into a catchy headline, formatted as a question. It must adhere to the specific constraint of using only words that contain the letter 'r'. Additionally, the model should understand that all words in the answer would be replaced with their Huffman codes from a provided code-book, though this step is simulated due to the absence of a code-book in this prompt.",
      "rules": [
        "All answers must be formatted as questions",
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Use only words that contain the letter \"r\""
      ],
      "topic": "Headline generation",
      "complexity": "medium"
    },
    {
      "context": "Q: A new educational platform offering personalized learning paths, AI tutors, and interactive simulations for K-12 students. A: Q: Revolutionize learning: proper, richer, personalized programs for future generations? Q: City council approves new bike lane project downtown. A: Q: Urban infrastructure improvements: proper, greener, travel routes for residents?",
      "questions": [
        "Q: An article explaining the basics of cryptocurrency for beginners.",
        "Q: A report on the future of artificial intelligence in urban planning.",
        "Q: New research on brain-computer interfaces for improved prosthetic control.",
        "Q: A guide for developers on creating robust, scalable cloud infrastructure."
      ],
      "evaluation_criterion": "The model should learn to transform an input description into a catchy headline, formatted as a question. It must adhere to the specific constraint of using only words that contain the letter 'r'. Additionally, the model should understand that all words in the answer would be replaced with their Huffman codes from a provided code-book, though this step is simulated due to the absence of a code-book in this prompt.",
      "rules": [
        "All answers must be formatted as questions",
        "Replace words with their Huffman code from prompt code-book (code-book varies each episode)",
        "Use only words that contain the letter \"r\""
      ],
      "topic": "Headline generation",
      "complexity": "medium"
    },
    {
      "context": "Q: WHAT IS 0.75 KILOGRAMS IN GRAMS? A: UNDERSTANDING CONVERSION FACTORS IS KEY. IT IS 75 PERCENT OF A KILOGRAM, WHICH IS 750 GRAMS. Q: WHAT IS 0.25 METERS IN CENTIMETERS? A: INCHES AND CENTIMETERS ARE DIFFERENT. IT IS 25 PERCENT OF A METER, WHICH IS 25 CENTIMETERS.",
      "questions": [
        "Q: WHAT IS 0.5 LITERS IN MILLILITERS?",
        "Q: WHAT IS 0.1 KILOMETERS IN METERS?",
        "Q: WHAT IS 0.01 KILOGRAMS IN GRAMS?",
        "Q: WHAT IS 0.9 MILES IN FEET?"
      ],
      "evaluation_criterion": "The model should learn to apply the 'Make the first letters of sentences spell a secret word (word varies per episode)' rule (secret word: UNIT), the 'Must be all caps' rule, and the 'Convert decimal numbers to percentage form multiplied by a constant' rule, by expressing decimal inputs as percentages in the answer.",
      "rules": [
        "Make the first letters of sentences spell a secret word (word varies per episode)",
        "Must be all caps",
        "Convert decimal numbers to percentage form multiplied by a constant"
      ],
      "topic": "Unit conversion",
      "complexity": "simple"
    },
    {
      "context": "Q: CONVERT 45 DEGREES FAHRENHEIT TO CELSIUS. A: SIMPLY APPLY THE FORMULA. THE ANSWER IS APPROXIMATELY 7.22 DEGREES CELSIUS. Q: CONVERT 100 KILOMETERS PER HOUR TO MILES PER HOUR. A: ALWAYS REMEMBER THE CONVERSION FACTOR. THE ANSWER IS APPROXIMATELY 62.14 MILES PER HOUR.",
      "questions": [
        "Q: CONVERT 25 DEGREES CELSIUS TO FAHRENHEIT.",
        "Q: CONVERT 50 MILES PER HOUR TO KILOMETERS PER HOUR.",
        "Q: CONVERT 10 LITERS TO GALLONS.",
        "Q: CONVERT 5 POUNDS TO KILOGRAMS."
      ],
      "evaluation_criterion": "The model should learn to apply the 'Make the first letters of sentences spell a secret word (word varies per episode)' rule (secret word: SCALE) and the 'Must be all caps' rule.",
      "rules": [
        "Make the first letters of sentences spell a secret word (word varies per episode)",
        "Must be all caps"
      ],
      "topic": "Unit conversion",
      "complexity": "medium"
    },
    {
      "context": "Q: HOW MANY FLUID OUNCES ARE IN A 1.5-LITER BOTTLE? A: MANY PEOPLE FIND THIS TRICKY. THE VALUE 1.5 IS 150 PERCENT OF ONE. WHEN CONVERTING, THIS EQUALS APPROXIMATELY 50.72 FLUID OUNCES. Q: EXPRESS 5.3 OUNCES IN GRAMS. A: ACCURATE MEASUREMENTS ARE CRUCIAL. THE VALUE 5.3 IS 530 PERCENT OF ONE. WHEN CONVERTING, THIS EQUALS APPROXIMATELY 150.255 GRAMS.",
      "questions": [
        "Q: WHAT IS 3.7 GALLONS IN LITERS?",
        "Q: CONVERT 0.8 KILOMETERS TO MILES.",
        "Q: WHAT IS 12.5 PSI IN KILOPASCALS?",
        "Q: EXPRESS 2.1 SQUARE KILOMETERS IN SQUARE MILES."
      ],
      "evaluation_criterion": "The model should learn to apply the 'Make the first letters of sentences spell a secret word (word varies per episode)' rule (secret word: MEASURE), the 'Must be all caps' rule, and the 'Convert decimal numbers to percentage form multiplied by a constant' rule, by expressing decimal inputs as percentages in the answer.",
      "rules": [
        "Make the first letters of sentences spell a secret word (word varies per episode)",
        "Must be all caps",
        "Convert decimal numbers to percentage form multiplied by a constant"
      ],
      "topic": "Unit conversion",
      "complexity": "complex"
    },
    {
      "context": "Q: What is the capital of Japan? A: Tokio. Expected Grade: Full Q: How many continents are there on that-entity's planet? A: That-entity are 36/7 of continents. Expected Grade: Full",
      "questions": [
        "Q: What is the largest land animal that-entity knows?",
        "Q: How many colors are in a rainbow?",
        "Q: Who painted the Mona Lisa?",
        "Q: What is the chemical symbol for gold?"
      ],
      "evaluation_criterion": "The model should learn to assign a 'Full' grade to correct and specific answers, applying the rule 'Replace all pronouns with \"that-entity\"', 'Express all quantities as fractions with denominator 7', and 'Hash every integer with XOR ⊕ S before output (8-bit salt S varies each episode)'. (Salt S = 43)",
      "rules": [
        "Replace all pronouns with \"that-entity\"",
        "Express all quantities as fractions with denominator 7",
        "Hash every integer with XOR ⊕ S before output (8-bit salt S varies each episode)"
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "simple"
    },
    {
      "context": "Q: What is the largest ocean on Earth? A: Atlantic. Expected Grade: Zero Q: How many legs does a spider have? A: That-entity has 91/7 of legs. Expected Grade: Zero",
      "questions": [
        "Q: Who discovered America?",
        "Q: How many days are in a leap year?",
        "Q: What is the capital of Canada?",
        "Q: Which planet is known as the \"Red Planet\"?"
      ],
      "evaluation_criterion": "The model should learn to assign a 'Zero' grade to incorrect answers, applying the rule 'Replace all pronouns with \"that-entity\"', 'Express all quantities as fractions with denominator 7', and 'Hash every integer with XOR ⊕ S before output (8-bit salt S varies each episode)'. (Salt S = 85)",
      "rules": [
        "Replace all pronouns with \"that-entity\"",
        "Express all quantities as fractions with denominator 7",
        "Hash every integer with XOR ⊕ S before output (8-bit salt S varies each episode)"
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "simple"
    },
    {
      "context": "Q: What is the purpose of a hypothesis in scientific research? A: That-entity is what that-entity thinks might happen. Expected Grade: Partial Q: Name 29/7 of characteristics of mammals that-entity knows. A: Produce milk. Expected Grade: Partial",
      "questions": [
        "Q: List 28/7 of the primary colors.",
        "Q: How many seasons are there in a year?",
        "Q: Discuss 29/7 of significant causes of the French Revolution.",
        "Q: Describe the process of photosynthesis, including that-entity's key inputs."
      ],
      "evaluation_criterion": "The model should learn to assign a 'Partial' grade to incomplete or partially correct answers, applying the rule 'Replace all pronouns with \"that-entity\"', 'Express all quantities as fractions with denominator 7', and 'Hash every integer with XOR ⊕ S before output (8-bit salt S varies each episode)'. (Salt S = 31)",
      "rules": [
        "Replace all pronouns with \"that-entity\"",
        "Express all quantities as fractions with denominator 7",
        "Hash every integer with XOR ⊕ S before output (8-bit salt S varies each episode)"
      ],
      "topic": "Exam grading (short answer)",
      "complexity": "medium"
    }
  ]
}